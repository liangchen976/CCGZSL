F:\anaconda3_new\python.exe "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py"
Namespace(attSize=102, batch_size=512, beta=60, beta1=0.5, beta_1=10, class_embedding='att', classifier_lr=0.001, critic_iter=5, cuda=True, dataroot='data', dataset='SUN', dir='models\\SUN', distill_proj_hidden_dim=4096, embedSize=2048, epochs=100, image_embedding='res101', ins_temp=0.1, ins_weight=0.001, lambda1=10, lr=0.0001, matdataset=True, nclass_all=717, nclass_seen=645, ndh=4096, neh=4096, nepoch=3001, ngh=4096, nz=102, outzSize=64, preprocessing=True, pretrain_class_number=130, pretrain_gan=True, recons_weight=0.001, resSize=2048, shuffer_class=True, standardization=False, syn_num=256, syn_num_rp=50, syn_num_s=60, syn_num_u=100, task_num=4, validation=False)
data/SUN/res101.mat E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3
att: torch.Size([717, 102])
splited_seen_class is [tensor([ 77, 227, 395, 332, 326, 523, 371, 431, 313,  86, 388, 403, 492,  51,
        344,  52, 359, 470, 384, 335,   4, 374, 268, 627,  43,  31, 123, 653,
        266, 411, 410, 219, 188, 412,  71, 573, 643,  93, 604, 149, 530, 586,
        224,  35, 506, 127, 398, 347, 283,  37, 449, 488, 629, 623,  13, 633,
         22,   1, 185, 370, 414, 713, 524,  36, 308, 310, 181,  73, 553, 460,
        314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687,  97, 104,
        160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284,  40, 276, 437,
         48, 587,  17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257,
        507, 456, 191, 265, 251,  49, 182, 190,  68, 594, 476, 405, 408, 325,
        218,  96, 583]), tensor([ 63, 663, 548, 105, 453, 551, 555, 198, 143, 559,  50, 229, 209,   6,
        199,  34,  76, 317, 501, 521, 102, 113,  25, 341, 331, 382, 288, 281,
         64, 572, 665, 179,  21, 421, 320, 147, 166, 693, 700, 454, 376, 155,
        480, 111,  83,  92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169,
        642, 337, 106, 552,  55, 567, 478, 161, 139, 396, 544, 386, 195, 531,
        661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483,
        175, 383, 464,  28, 211,  14, 354, 702, 568, 119,  87, 611, 214, 706,
        689, 512,  62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285,  27,
        685, 391, 419, 539, 468,  42, 418, 614, 126,  33, 295, 450, 458, 293,
        167, 620, 659]), tensor([662, 271, 287, 549, 278, 240, 499, 682, 703, 545,  41, 136, 462, 193,
        697, 290,  60, 445,  78, 510, 404, 571, 416, 607, 362, 461, 368,  46,
         59, 194, 670, 333, 442, 357, 554, 270,   9,  30, 678, 108,  69, 311,
        100,  81, 356,  11, 497, 599, 667, 503, 236,  20, 542, 117, 231,  66,
        202, 307, 532, 115, 472, 537, 213, 319, 256, 380, 600, 180, 588, 200,
        447, 277, 429, 171, 360,  88, 422, 173, 690, 655, 303, 439, 189, 694,
        489, 709, 205, 348, 613, 641, 413, 595, 519, 339, 176, 178,  19, 672,
        511, 225, 121, 364, 321, 615, 165,  91, 400, 621, 579, 172, 616, 564,
        255, 118, 232, 533, 128,  16, 490, 491, 186, 589, 441, 292, 120, 430,
        585, 646, 469]), tensor([540, 640, 296, 485, 638, 424,  90, 432, 275, 649, 628, 206, 253, 406,
        125, 639,  56, 297, 291, 159, 541, 428, 373, 327, 443,  26, 228,   7,
        684, 465, 345,   0, 407, 207, 624,  70, 330, 340, 487, 338, 538, 598,
        168, 505,  44, 457, 504,  29, 241, 137, 329, 603, 514, 547,  67,  80,
        592,  15, 516, 343, 151, 390, 351, 634, 239, 575, 561, 135, 280, 273,
        261, 134, 546, 467, 584, 355, 578, 535, 630, 692, 498, 107, 582, 433,
        140, 563, 704, 215, 691, 515, 156, 605, 234, 494, 597, 701, 581, 495,
        146,  45, 177, 708, 526, 619, 131, 463, 565, 377, 707, 263, 304, 673,
        163, 452,  84, 427, 426, 401, 608, 238, 475, 502, 669, 133, 534, 378,
        361,  89])]
splited_unseen_class is [tensor([ 24, 681, 286,  38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695,
        112, 259, 336, 635]), tensor([493, 196,  72,  74,  57,  99, 558, 674, 342,  95, 482,  53, 381, 560,
        508,  10, 656, 712]), tensor([124, 328,  85, 420, 103, 517, 471, 237, 315, 379, 246, 423, 645, 425,
        710, 262, 448, 715]), tensor([158,  75, 245, 138, 254, 298, 622, 152, 509, 679, 145, 184,   3,  32,
        658, 650, 353,  23])]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
pre_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289]
curr_seen_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289]
**************************************************
pretraining cgan model ···
[0/3001] Loss_D: 3.1250 Loss_G: -1.0567, Wasserstein_dist: 1.4045, real_ins_contras_loss: 4.4260,fake_ins_contras_loss : 7.3603
[50/3001] Loss_D: -0.0338 Loss_G: -0.0742, Wasserstein_dist: 1.4407, real_ins_contras_loss: 1.3050,fake_ins_contras_loss : 7.4875
[100/3001] Loss_D: -0.7924 Loss_G: 1.0426, Wasserstein_dist: 2.2110, real_ins_contras_loss: 1.2679,fake_ins_contras_loss : 7.3842
[150/3001] Loss_D: 0.3877 Loss_G: 0.0654, Wasserstein_dist: 0.8895, real_ins_contras_loss: 1.2378,fake_ins_contras_loss : 7.4098
model 0 is saved
cls traing
Training classifier loss= 3.5923
seen=0.0385
[200/3001] Loss_D: 0.6583 Loss_G: -0.5535, Wasserstein_dist: 0.5690, real_ins_contras_loss: 1.2168,fake_ins_contras_loss : 7.3938
[250/3001] Loss_D: 0.6032 Loss_G: -0.6047, Wasserstein_dist: 0.6516, real_ins_contras_loss: 1.2379,fake_ins_contras_loss : 7.2650
[300/3001] Loss_D: 0.6795 Loss_G: -0.6824, Wasserstein_dist: 0.5845, real_ins_contras_loss: 1.2432,fake_ins_contras_loss : 7.1237
[350/3001] Loss_D: 0.5469 Loss_G: -0.8102, Wasserstein_dist: 0.6805, real_ins_contras_loss: 1.1989,fake_ins_contras_loss : 6.9696
model 0 is saved
cls traing
Training classifier loss= 2.6081
seen=0.1677
[400/3001] Loss_D: 0.5224 Loss_G: -0.7246, Wasserstein_dist: 0.7212, real_ins_contras_loss: 1.2083,fake_ins_contras_loss : 6.7867
[450/3001] Loss_D: 0.5458 Loss_G: -0.9268, Wasserstein_dist: 0.6799, real_ins_contras_loss: 1.1915,fake_ins_contras_loss : 6.2935
[500/3001] Loss_D: 0.5481 Loss_G: -0.8947, Wasserstein_dist: 0.7103, real_ins_contras_loss: 1.2321,fake_ins_contras_loss : 6.1338
[550/3001] Loss_D: 0.5370 Loss_G: -1.0187, Wasserstein_dist: 0.6994, real_ins_contras_loss: 1.2111,fake_ins_contras_loss : 6.0088
model 0 is saved
cls traing
Training classifier loss= 1.6625
seen=0.2938
[600/3001] Loss_D: 0.5617 Loss_G: -0.9523, Wasserstein_dist: 0.7092, real_ins_contras_loss: 1.2442,fake_ins_contras_loss : 5.8553
[650/3001] Loss_D: 0.5098 Loss_G: -0.8123, Wasserstein_dist: 0.7295, real_ins_contras_loss: 1.2172,fake_ins_contras_loss : 5.7800
[700/3001] Loss_D: 0.4711 Loss_G: -0.8602, Wasserstein_dist: 0.7815, real_ins_contras_loss: 1.2224,fake_ins_contras_loss : 5.5629
[750/3001] Loss_D: 0.4223 Loss_G: -0.8878, Wasserstein_dist: 0.8246, real_ins_contras_loss: 1.2181,fake_ins_contras_loss : 5.5652
model 0 is saved
cls traing
Training classifier loss= 1.3165
seen=0.3877
[800/3001] Loss_D: 0.4804 Loss_G: -0.7492, Wasserstein_dist: 0.8344, real_ins_contras_loss: 1.2830,fake_ins_contras_loss : 5.3739
[850/3001] Loss_D: 0.4270 Loss_G: -0.7717, Wasserstein_dist: 0.8584, real_ins_contras_loss: 1.2488,fake_ins_contras_loss : 5.2033
[900/3001] Loss_D: 0.3089 Loss_G: -0.7517, Wasserstein_dist: 0.9379, real_ins_contras_loss: 1.1997,fake_ins_contras_loss : 5.0443
[950/3001] Loss_D: 0.2666 Loss_G: -0.7663, Wasserstein_dist: 0.9812, real_ins_contras_loss: 1.2136,fake_ins_contras_loss : 4.8770
model 0 is saved
cls traing
Training classifier loss= 1.0182
seen=0.4692
[1000/3001] Loss_D: 0.2132 Loss_G: -0.7729, Wasserstein_dist: 1.0253, real_ins_contras_loss: 1.1993,fake_ins_contras_loss : 4.7042
[1050/3001] Loss_D: 0.1974 Loss_G: -0.7766, Wasserstein_dist: 1.0479, real_ins_contras_loss: 1.2132,fake_ins_contras_loss : 4.5945
[1100/3001] Loss_D: 0.1380 Loss_G: -0.6566, Wasserstein_dist: 1.1271, real_ins_contras_loss: 1.2213,fake_ins_contras_loss : 4.5009
[1150/3001] Loss_D: 0.1528 Loss_G: -0.5946, Wasserstein_dist: 1.1454, real_ins_contras_loss: 1.2454,fake_ins_contras_loss : 3.6115
model 0 is saved
cls traing
Training classifier loss= 1.0255
seen=0.4831
[1200/3001] Loss_D: 0.1314 Loss_G: -0.6504, Wasserstein_dist: 1.2040, real_ins_contras_loss: 1.2737,fake_ins_contras_loss : 3.7968
[1250/3001] Loss_D: 0.0310 Loss_G: -0.7004, Wasserstein_dist: 1.1975, real_ins_contras_loss: 1.1818,fake_ins_contras_loss : 3.7338
[1300/3001] Loss_D: 0.0820 Loss_G: -0.5166, Wasserstein_dist: 1.2139, real_ins_contras_loss: 1.2460,fake_ins_contras_loss : 3.6390
[1350/3001] Loss_D: 0.0679 Loss_G: -0.5521, Wasserstein_dist: 1.2154, real_ins_contras_loss: 1.2207,fake_ins_contras_loss : 3.6421
model 0 is saved
cls traing
Training classifier loss= 0.8992
seen=0.4846
[1400/3001] Loss_D: 0.0347 Loss_G: -0.4685, Wasserstein_dist: 1.2605, real_ins_contras_loss: 1.2368,fake_ins_contras_loss : 3.4828
[1450/3001] Loss_D: 0.0717 Loss_G: -0.4653, Wasserstein_dist: 1.1993, real_ins_contras_loss: 1.2276,fake_ins_contras_loss : 3.5667
[1500/3001] Loss_D: 0.0381 Loss_G: -0.4749, Wasserstein_dist: 1.2675, real_ins_contras_loss: 1.2452,fake_ins_contras_loss : 3.4290
[1550/3001] Loss_D: 0.0151 Loss_G: -0.4050, Wasserstein_dist: 1.2822, real_ins_contras_loss: 1.2255,fake_ins_contras_loss : 3.3790
model 0 is saved
cls traing
Training classifier loss= 0.7812
seen=0.5077
[1600/3001] Loss_D: -0.0066 Loss_G: -0.4061, Wasserstein_dist: 1.3008, real_ins_contras_loss: 1.2184,fake_ins_contras_loss : 3.2716
[1650/3001] Loss_D: -0.0133 Loss_G: -0.4251, Wasserstein_dist: 1.2824, real_ins_contras_loss: 1.2083,fake_ins_contras_loss : 3.2563
[1700/3001] Loss_D: -0.0059 Loss_G: -0.3515, Wasserstein_dist: 1.2931, real_ins_contras_loss: 1.2272,fake_ins_contras_loss : 3.2059
[1750/3001] Loss_D: 0.0175 Loss_G: -0.2298, Wasserstein_dist: 1.2928, real_ins_contras_loss: 1.2319,fake_ins_contras_loss : 3.1692
model 0 is saved
cls traing
Training classifier loss= 0.7637
seen=0.5000
[1800/3001] Loss_D: -0.6657 Loss_G: -2.1424, Wasserstein_dist: 2.4318, real_ins_contras_loss: 1.2788,fake_ins_contras_loss : 3.5829
[1850/3001] Loss_D: 0.1250 Loss_G: -1.6768, Wasserstein_dist: 1.1725, real_ins_contras_loss: 1.2393,fake_ins_contras_loss : 3.2394
[1900/3001] Loss_D: 0.0584 Loss_G: -1.5606, Wasserstein_dist: 1.2148, real_ins_contras_loss: 1.2062,fake_ins_contras_loss : 3.1700
[1950/3001] Loss_D: 0.0665 Loss_G: -1.4287, Wasserstein_dist: 1.2448, real_ins_contras_loss: 1.2358,fake_ins_contras_loss : 3.1248
model 0 is saved
cls traing
Training classifier loss= 0.6011
seen=0.5246
[2000/3001] Loss_D: 0.0521 Loss_G: -1.4151, Wasserstein_dist: 1.2299, real_ins_contras_loss: 1.2007,fake_ins_contras_loss : 3.0018
[2050/3001] Loss_D: 0.0777 Loss_G: -1.2803, Wasserstein_dist: 1.2703, real_ins_contras_loss: 1.2293,fake_ins_contras_loss : 2.9984
[2100/3001] Loss_D: 0.0212 Loss_G: -1.2771, Wasserstein_dist: 1.2896, real_ins_contras_loss: 1.2071,fake_ins_contras_loss : 2.9024
[2150/3001] Loss_D: -0.0141 Loss_G: -1.1794, Wasserstein_dist: 1.3057, real_ins_contras_loss: 1.1810,fake_ins_contras_loss : 2.8385
model 0 is saved
cls traing
Training classifier loss= 0.5235
seen=0.5354
[2200/3001] Loss_D: 0.0109 Loss_G: -1.1907, Wasserstein_dist: 1.3444, real_ins_contras_loss: 1.2453,fake_ins_contras_loss : 2.8456
[2250/3001] Loss_D: -0.0024 Loss_G: -1.0679, Wasserstein_dist: 1.3528, real_ins_contras_loss: 1.2509,fake_ins_contras_loss : 2.7272
[2300/3001] Loss_D: -0.0812 Loss_G: -1.0337, Wasserstein_dist: 1.3820, real_ins_contras_loss: 1.1883,fake_ins_contras_loss : 2.7691
[2350/3001] Loss_D: -0.0276 Loss_G: -1.0780, Wasserstein_dist: 1.3683, real_ins_contras_loss: 1.2383,fake_ins_contras_loss : 2.6011
model 0 is saved
cls traing
Training classifier loss= 0.5332
seen=0.5462
[2400/3001] Loss_D: -0.0814 Loss_G: -0.9461, Wasserstein_dist: 1.3633, real_ins_contras_loss: 1.1901,fake_ins_contras_loss : 2.6385
[2450/3001] Loss_D: -0.0779 Loss_G: -0.9519, Wasserstein_dist: 1.3816, real_ins_contras_loss: 1.2011,fake_ins_contras_loss : 2.6250
[2500/3001] Loss_D: -0.0990 Loss_G: -0.8716, Wasserstein_dist: 1.3939, real_ins_contras_loss: 1.1974,fake_ins_contras_loss : 2.5278
[2550/3001] Loss_D: -0.1529 Loss_G: -0.7937, Wasserstein_dist: 1.4372, real_ins_contras_loss: 1.1807,fake_ins_contras_loss : 2.5647
model 0 is saved
cls traing
Training classifier loss= 0.4661
seen=0.5554
[2600/3001] Loss_D: -0.0586 Loss_G: -0.7965, Wasserstein_dist: 1.4403, real_ins_contras_loss: 1.2745,fake_ins_contras_loss : 2.6026
[2650/3001] Loss_D: -0.1529 Loss_G: -1.0119, Wasserstein_dist: 1.4421, real_ins_contras_loss: 1.1948,fake_ins_contras_loss : 2.5372
[2700/3001] Loss_D: -0.1368 Loss_G: -0.9558, Wasserstein_dist: 1.4531, real_ins_contras_loss: 1.2218,fake_ins_contras_loss : 2.5067
[2750/3001] Loss_D: -0.0886 Loss_G: -0.8941, Wasserstein_dist: 1.4169, real_ins_contras_loss: 1.2391,fake_ins_contras_loss : 2.4528
model 0 is saved
cls traing
Training classifier loss= 0.4921
seen=0.5585
[2800/3001] Loss_D: -0.1568 Loss_G: -0.7892, Wasserstein_dist: 1.4956, real_ins_contras_loss: 1.2182,fake_ins_contras_loss : 2.4724
[2850/3001] Loss_D: -0.1727 Loss_G: -0.8764, Wasserstein_dist: 1.5073, real_ins_contras_loss: 1.2167,fake_ins_contras_loss : 2.4340
[2900/3001] Loss_D: -0.1664 Loss_G: -0.8111, Wasserstein_dist: 1.4998, real_ins_contras_loss: 1.2190,fake_ins_contras_loss : 2.4401
[2950/3001] Loss_D: -0.1118 Loss_G: -0.9430, Wasserstein_dist: 1.4452, real_ins_contras_loss: 1.2221,fake_ins_contras_loss : 2.4537
model 0 is saved
cls traing
Training classifier loss= 0.4826
seen=0.5569
[3000/3001] Loss_D: -0.2179 Loss_G: -0.6780, Wasserstein_dist: 1.5663, real_ins_contras_loss: 1.2362,fake_ins_contras_loss : 2.4243
average_acc is 0.46934065934065927
----------------------------------------
loading task 1th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258]
current seen_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583]
curr_unseen_label is [24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635]
pre_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276]
finish pretraining cgan model ···
incremental training task 1···
loading task 1 data and the pre model
model 0 is loaded
kl_loss G is 0.008682179264724255
kl_loss G is 0.005529093090444803
kl_loss G is 0.003660428337752819
kl_loss G is 0.002684791339561343
[0/3001] Loss_D: 9.4750 Loss_G: -1.0771, Wasserstein_dist: 0.1381, real_ins_contras_loss: 4.6357,fake_ins_contras_loss : 8.1974, CL_contra_loss : 0.4508
model 1 is saved
cls traing
unseen=0.5000, seen=0.2950, h=0.3711
[100/3001] Loss_D: 0.4641 Loss_G: 0.3956, Wasserstein_dist: 1.3920, real_ins_contras_loss: 1.2992,fake_ins_contras_loss : 8.0302, CL_contra_loss : 0.0431
[200/3001] Loss_D: 0.1888 Loss_G: 0.6675, Wasserstein_dist: 1.6363, real_ins_contras_loss: 1.3060,fake_ins_contras_loss : 8.1200, CL_contra_loss : 0.0375
model 1 is saved
cls traing
unseen=0.5926, seen=0.3027, h=0.4007
[300/3001] Loss_D: -0.0040 Loss_G: 0.8121, Wasserstein_dist: 1.7207, real_ins_contras_loss: 1.2439,fake_ins_contras_loss : 8.0732, CL_contra_loss : 0.0343
[400/3001] Loss_D: -0.1168 Loss_G: 0.8233, Wasserstein_dist: 1.8687, real_ins_contras_loss: 1.2570,fake_ins_contras_loss : 8.1489, CL_contra_loss : 0.0331
model 1 is saved
cls traing
unseen=0.5529, seen=0.3236, h=0.4082
kl_loss G is 0.005556237883865833
kl_loss G is 0.005646309815347195
kl_loss G is 0.005625630728900433
kl_loss G is 0.0055158426985144615
[500/3001] Loss_D: -0.2239 Loss_G: 0.8592, Wasserstein_dist: 1.9687, real_ins_contras_loss: 1.2473,fake_ins_contras_loss : 8.1590, CL_contra_loss : 0.0337
[600/3001] Loss_D: -0.2341 Loss_G: 0.7834, Wasserstein_dist: 2.0214, real_ins_contras_loss: 1.2536,fake_ins_contras_loss : 8.0899, CL_contra_loss : 0.0346
model 1 is saved
cls traing
unseen=0.5503, seen=0.3413, h=0.4213
[700/3001] Loss_D: -0.2578 Loss_G: 0.9082, Wasserstein_dist: 2.0187, real_ins_contras_loss: 1.2685,fake_ins_contras_loss : 8.2811, CL_contra_loss : 0.0339
[800/3001] Loss_D: -0.3586 Loss_G: 0.9091, Wasserstein_dist: 2.1227, real_ins_contras_loss: 1.2687,fake_ins_contras_loss : 8.2458, CL_contra_loss : 0.0321
model 1 is saved
cls traing
unseen=0.5397, seen=0.3459, h=0.4216
[900/3001] Loss_D: -0.4026 Loss_G: 0.8836, Wasserstein_dist: 2.1387, real_ins_contras_loss: 1.2569,fake_ins_contras_loss : 8.2465, CL_contra_loss : 0.0304
kl_loss G is 0.006230579689145088
kl_loss G is 0.006058746017515659
kl_loss G is 0.006157143972814083
kl_loss G is 0.0062100524082779884
[1000/3001] Loss_D: -0.4367 Loss_G: 0.9952, Wasserstein_dist: 2.1277, real_ins_contras_loss: 1.2232,fake_ins_contras_loss : 8.1569, CL_contra_loss : 0.0310
model 1 is saved
cls traing
unseen=0.5476, seen=0.3514, h=0.4281
[1100/3001] Loss_D: -0.3264 Loss_G: 0.9964, Wasserstein_dist: 2.0887, real_ins_contras_loss: 1.2732,fake_ins_contras_loss : 8.2893, CL_contra_loss : 0.0318
[1200/3001] Loss_D: -0.3247 Loss_G: 1.1800, Wasserstein_dist: 2.1325, real_ins_contras_loss: 1.2926,fake_ins_contras_loss : 8.2190, CL_contra_loss : 0.0317
model 1 is saved
cls traing
unseen=0.5503, seen=0.3506, h=0.4283
[1300/3001] Loss_D: -0.4792 Loss_G: 1.1436, Wasserstein_dist: 2.1723, real_ins_contras_loss: 1.2337,fake_ins_contras_loss : 8.2491, CL_contra_loss : 0.0303
[1400/3001] Loss_D: -0.5107 Loss_G: 1.1629, Wasserstein_dist: 2.1851, real_ins_contras_loss: 1.1980,fake_ins_contras_loss : 8.3289, CL_contra_loss : 0.0322
model 1 is saved
cls traing
unseen=0.5476, seen=0.3560, h=0.4315
kl_loss G is 0.006438147742301226
kl_loss G is 0.006250110454857349
kl_loss G is 0.006424645893275738
kl_loss G is 0.006386242341250181
[1500/3001] Loss_D: -0.4244 Loss_G: 1.3521, Wasserstein_dist: 2.1650, real_ins_contras_loss: 1.2493,fake_ins_contras_loss : 8.3181, CL_contra_loss : 0.0318
[1600/3001] Loss_D: -0.5343 Loss_G: 1.2851, Wasserstein_dist: 2.1956, real_ins_contras_loss: 1.1999,fake_ins_contras_loss : 8.1793, CL_contra_loss : 0.0317
model 1 is saved
cls traing
unseen=0.5397, seen=0.3622, h=0.4335
[1700/3001] Loss_D: -0.4438 Loss_G: 1.3915, Wasserstein_dist: 2.1576, real_ins_contras_loss: 1.2525,fake_ins_contras_loss : 8.2869, CL_contra_loss : 0.0305
[1800/3001] Loss_D: -0.4679 Loss_G: 1.4193, Wasserstein_dist: 2.1662, real_ins_contras_loss: 1.2345,fake_ins_contras_loss : 8.2480, CL_contra_loss : 0.0319
model 1 is saved
cls traing
unseen=0.5317, seen=0.3822, h=0.4448
[1900/3001] Loss_D: -0.4919 Loss_G: 1.4935, Wasserstein_dist: 2.1871, real_ins_contras_loss: 1.2520,fake_ins_contras_loss : 8.2087, CL_contra_loss : 0.0298
kl_loss G is 0.006551393773406744
kl_loss G is 0.006487630307674408
kl_loss G is 0.006495760753750801
kl_loss G is 0.006589037831872702
[2000/3001] Loss_D: -0.5183 Loss_G: 1.5378, Wasserstein_dist: 2.2046, real_ins_contras_loss: 1.2205,fake_ins_contras_loss : 8.2290, CL_contra_loss : 0.0328
model 1 is saved
cls traing
unseen=0.5423, seen=0.3815, h=0.4479
[2100/3001] Loss_D: -0.4562 Loss_G: 1.4979, Wasserstein_dist: 2.1878, real_ins_contras_loss: 1.2627,fake_ins_contras_loss : 8.2748, CL_contra_loss : 0.0300
[2200/3001] Loss_D: -0.4757 Loss_G: 1.5669, Wasserstein_dist: 2.1908, real_ins_contras_loss: 1.2708,fake_ins_contras_loss : 8.3212, CL_contra_loss : 0.0303
model 1 is saved
cls traing
unseen=0.5873, seen=0.3730, h=0.4562
[2300/3001] Loss_D: -0.5033 Loss_G: 1.5393, Wasserstein_dist: 2.1753, real_ins_contras_loss: 1.2264,fake_ins_contras_loss : 8.2792, CL_contra_loss : 0.0301
[2400/3001] Loss_D: -0.5401 Loss_G: 1.5891, Wasserstein_dist: 2.2240, real_ins_contras_loss: 1.2400,fake_ins_contras_loss : 8.3583, CL_contra_loss : 0.0310
model 1 is saved
cls traing
unseen=0.5926, seen=0.3691, h=0.4549
kl_loss G is 0.006739147938787937
kl_loss G is 0.006457278039306402
kl_loss G is 0.006769256666302681
kl_loss G is 0.006681375205516815
[2500/3001] Loss_D: -0.5552 Loss_G: 1.6288, Wasserstein_dist: 2.2167, real_ins_contras_loss: 1.2230,fake_ins_contras_loss : 8.2775, CL_contra_loss : 0.0302
[2600/3001] Loss_D: -0.5123 Loss_G: 1.6939, Wasserstein_dist: 2.2144, real_ins_contras_loss: 1.2323,fake_ins_contras_loss : 8.2612, CL_contra_loss : 0.0320
model 1 is saved
cls traing
unseen=0.5582, seen=0.3792, h=0.4516
[2700/3001] Loss_D: -0.4630 Loss_G: 1.8434, Wasserstein_dist: 2.1987, real_ins_contras_loss: 1.2804,fake_ins_contras_loss : 8.3580, CL_contra_loss : 0.0302
[2800/3001] Loss_D: -0.5395 Loss_G: 1.6963, Wasserstein_dist: 2.2421, real_ins_contras_loss: 1.2417,fake_ins_contras_loss : 8.2631, CL_contra_loss : 0.0292
model 1 is saved
cls traing
unseen=0.5582, seen=0.3869, h=0.4570
[2900/3001] Loss_D: -0.5562 Loss_G: 1.7589, Wasserstein_dist: 2.2139, real_ins_contras_loss: 1.2003,fake_ins_contras_loss : 8.2327, CL_contra_loss : 0.0337
kl_loss G is 0.0067862397991120815
kl_loss G is 0.00670818705111742
kl_loss G is 0.006589449942111969
kl_loss G is 0.006641237065196037
[3000/3001] Loss_D: -0.5650 Loss_G: 1.8638, Wasserstein_dist: 2.2103, real_ins_contras_loss: 1.2220,fake_ins_contras_loss : 8.2866, CL_contra_loss : 0.0257
model 1 is saved
cls traing
unseen=0.5265, seen=0.3807, h=0.4419
average_acc is 0.43515697788007496
 hightest s is 0.38687258687258724, u is 0.5582010582010583, H is 0.4570071094606194,
----------------------------------------
loading task 2th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]
current seen_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 63, 663, 548, 105, 453, 551, 555, 198, 143, 559, 50, 229, 209, 6, 199, 34, 76, 317, 501, 521, 102, 113, 25, 341, 331, 382, 288, 281, 64, 572, 665, 179, 21, 421, 320, 147, 166, 693, 700, 454, 376, 155, 480, 111, 83, 92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169, 642, 337, 106, 552, 55, 567, 478, 161, 139, 396, 544, 386, 195, 531, 661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483, 175, 383, 464, 28, 211, 14, 354, 702, 568, 119, 87, 611, 214, 706, 689, 512, 62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285, 27, 685, 391, 419, 539, 468, 42, 418, 614, 126, 33, 295, 450, 458, 293, 167, 620, 659]
curr_unseen_label is [24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635, 493, 196, 72, 74, 57, 99, 558, 674, 342, 95, 482, 53, 381, 560, 508, 10, 656, 712]
pre_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635, 63, 663, 548, 105, 453, 551, 555, 198, 143, 559, 50, 229, 209, 6, 199, 34, 76, 317, 501, 521, 102, 113, 25, 341, 331, 382, 288, 281, 64, 572, 665, 179, 21, 421, 320, 147, 166, 693, 700, 454, 376, 155, 480, 111, 83, 92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169, 642, 337, 106, 552, 55, 567, 478, 161, 139, 396, 544, 386, 195, 531, 661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483, 175, 383, 464, 28, 211, 14, 354, 702, 568, 119, 87, 611, 214, 706, 689, 512, 62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285, 27, 685, 391, 419, 539, 468, 42, 418, 614, 126, 33, 295, 450, 458, 293, 167, 620, 659, 493, 196, 72, 74, 57, 99, 558, 674, 342, 95, 482, 53, 381, 560, 508, 10, 656, 712]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]
incremental training task 2···
loading task 2 data and the pre model
model 1 is loaded
kl_loss G is 0.0
kl_loss G is 0.0028625153936445713
kl_loss G is 0.0016120211221277714
kl_loss G is 0.0014820005744695663
[0/3001] Loss_D: 4.9191 Loss_G: 2.0345, Wasserstein_dist: 0.8103, real_ins_contras_loss: 4.3708,fake_ins_contras_loss : 8.6506, CL_contra_loss : 0.1233
model 2 is saved
cls traing
unseen=0.4074, seen=0.2753, h=0.3285
[100/3001] Loss_D: 0.0968 Loss_G: 1.5109, Wasserstein_dist: 1.6526, real_ins_contras_loss: 1.2740,fake_ins_contras_loss : 8.0283, CL_contra_loss : 0.0372
[200/3001] Loss_D: -0.1404 Loss_G: 1.2994, Wasserstein_dist: 1.8440, real_ins_contras_loss: 1.2502,fake_ins_contras_loss : 8.0400, CL_contra_loss : 0.0325
model 2 is saved
cls traing
unseen=0.4008, seen=0.2856, h=0.3335
[300/3001] Loss_D: -0.2305 Loss_G: 1.3239, Wasserstein_dist: 1.9142, real_ins_contras_loss: 1.2442,fake_ins_contras_loss : 8.0931, CL_contra_loss : 0.0327
[400/3001] Loss_D: -0.2565 Loss_G: 1.3467, Wasserstein_dist: 1.9521, real_ins_contras_loss: 1.2409,fake_ins_contras_loss : 8.0981, CL_contra_loss : 0.0313
model 2 is saved
cls traing
unseen=0.3942, seen=0.3077, h=0.3456
kl_loss G is 0.004556655418127775
kl_loss G is 0.004486816003918648
kl_loss G is 0.0046301959082484245
kl_loss G is 0.004559529945254326
[500/3001] Loss_D: -0.2838 Loss_G: 1.3970, Wasserstein_dist: 2.0125, real_ins_contras_loss: 1.2810,fake_ins_contras_loss : 8.1767, CL_contra_loss : 0.0303
[600/3001] Loss_D: -0.3223 Loss_G: 1.4623, Wasserstein_dist: 2.0378, real_ins_contras_loss: 1.2552,fake_ins_contras_loss : 8.1910, CL_contra_loss : 0.0309
model 2 is saved
cls traing
unseen=0.4101, seen=0.3021, h=0.3479
[700/3001] Loss_D: -0.3956 Loss_G: 1.6161, Wasserstein_dist: 2.0900, real_ins_contras_loss: 1.2632,fake_ins_contras_loss : 8.1528, CL_contra_loss : 0.0292
[800/3001] Loss_D: -0.3810 Loss_G: 1.6242, Wasserstein_dist: 2.0886, real_ins_contras_loss: 1.2798,fake_ins_contras_loss : 8.2268, CL_contra_loss : 0.0274
model 2 is saved
cls traing
unseen=0.3889, seen=0.3232, h=0.3530
[900/3001] Loss_D: -0.4285 Loss_G: 1.8185, Wasserstein_dist: 2.0850, real_ins_contras_loss: 1.2431,fake_ins_contras_loss : 8.1955, CL_contra_loss : 0.0284
kl_loss G is 0.005036651156842709
kl_loss G is 0.005120383575558662
kl_loss G is 0.005013349931687117
kl_loss G is 0.005120422691106796
[1000/3001] Loss_D: -0.4621 Loss_G: 1.8644, Wasserstein_dist: 2.1419, real_ins_contras_loss: 1.2473,fake_ins_contras_loss : 8.1833, CL_contra_loss : 0.0307
model 2 is saved
cls traing
unseen=0.4140, seen=0.3144, h=0.3574
[1100/3001] Loss_D: -0.3901 Loss_G: 1.9523, Wasserstein_dist: 2.1280, real_ins_contras_loss: 1.2911,fake_ins_contras_loss : 8.2427, CL_contra_loss : 0.0286
[1200/3001] Loss_D: -0.4132 Loss_G: 1.9174, Wasserstein_dist: 2.0869, real_ins_contras_loss: 1.2394,fake_ins_contras_loss : 8.1578, CL_contra_loss : 0.0295
model 2 is saved
cls traing
unseen=0.4127, seen=0.3232, h=0.3625
[1300/3001] Loss_D: -0.5169 Loss_G: 1.9868, Wasserstein_dist: 2.1746, real_ins_contras_loss: 1.2319,fake_ins_contras_loss : 8.2683, CL_contra_loss : 0.0300
[1400/3001] Loss_D: -0.4562 Loss_G: 2.0977, Wasserstein_dist: 2.0995, real_ins_contras_loss: 1.2341,fake_ins_contras_loss : 8.0743, CL_contra_loss : 0.0281
model 2 is saved
cls traing
unseen=0.4352, seen=0.3093, h=0.3616
kl_loss G is 0.005186602473258972
kl_loss G is 0.005163431167602539
kl_loss G is 0.005304970778524876
kl_loss G is 0.005377328954637051
[1500/3001] Loss_D: -0.4575 Loss_G: 2.0981, Wasserstein_dist: 2.1633, real_ins_contras_loss: 1.2478,fake_ins_contras_loss : 8.2773, CL_contra_loss : 0.0280
[1600/3001] Loss_D: -0.4829 Loss_G: 2.2588, Wasserstein_dist: 2.1610, real_ins_contras_loss: 1.2336,fake_ins_contras_loss : 8.2668, CL_contra_loss : 0.0308
model 2 is saved
cls traing
unseen=0.4286, seen=0.3175, h=0.3648
[1700/3001] Loss_D: -0.4252 Loss_G: 2.1813, Wasserstein_dist: 2.1435, real_ins_contras_loss: 1.2819,fake_ins_contras_loss : 8.2720, CL_contra_loss : 0.0273
[1800/3001] Loss_D: -0.4332 Loss_G: 2.2617, Wasserstein_dist: 2.1701, real_ins_contras_loss: 1.2934,fake_ins_contras_loss : 8.2636, CL_contra_loss : 0.0273
model 2 is saved
cls traing
unseen=0.3968, seen=0.3412, h=0.3669
[1900/3001] Loss_D: -0.4085 Loss_G: 2.2892, Wasserstein_dist: 2.1798, real_ins_contras_loss: 1.3046,fake_ins_contras_loss : 8.2656, CL_contra_loss : 0.0275
kl_loss G is 0.005474869627505541
kl_loss G is 0.005348546430468559
kl_loss G is 0.005471721291542053
kl_loss G is 0.005527965724468231
[2000/3001] Loss_D: -0.4025 Loss_G: 2.2976, Wasserstein_dist: 2.1666, real_ins_contras_loss: 1.3148,fake_ins_contras_loss : 8.2181, CL_contra_loss : 0.0255
model 2 is saved
cls traing
unseen=0.4193, seen=0.3232, h=0.3650
[2100/3001] Loss_D: -0.4965 Loss_G: 2.3226, Wasserstein_dist: 2.1537, real_ins_contras_loss: 1.2416,fake_ins_contras_loss : 8.2673, CL_contra_loss : 0.0266
[2200/3001] Loss_D: -0.4748 Loss_G: 2.2340, Wasserstein_dist: 2.1347, real_ins_contras_loss: 1.2415,fake_ins_contras_loss : 8.3030, CL_contra_loss : 0.0270
model 2 is saved
cls traing
unseen=0.4101, seen=0.3216, h=0.3605
[2300/3001] Loss_D: -0.4858 Loss_G: 2.3423, Wasserstein_dist: 2.1346, real_ins_contras_loss: 1.2332,fake_ins_contras_loss : 8.1587, CL_contra_loss : 0.0266
[2400/3001] Loss_D: -0.4949 Loss_G: 2.4927, Wasserstein_dist: 2.1730, real_ins_contras_loss: 1.2404,fake_ins_contras_loss : 8.2603, CL_contra_loss : 0.0307
model 2 is saved
cls traing
unseen=0.4153, seen=0.3289, h=0.3671
kl_loss G is 0.005524943582713604
kl_loss G is 0.005536144133657217
kl_loss G is 0.005552896298468113
kl_loss G is 0.005466620437800884
[2500/3001] Loss_D: -0.4742 Loss_G: 2.4259, Wasserstein_dist: 2.1602, real_ins_contras_loss: 1.2421,fake_ins_contras_loss : 8.3261, CL_contra_loss : 0.0283
[2600/3001] Loss_D: -0.5651 Loss_G: 2.4992, Wasserstein_dist: 2.1559, real_ins_contras_loss: 1.2020,fake_ins_contras_loss : 8.2748, CL_contra_loss : 0.0268
model 2 is saved
cls traing
unseen=0.4299, seen=0.3299, h=0.3733
[2700/3001] Loss_D: -0.4969 Loss_G: 2.6227, Wasserstein_dist: 2.1512, real_ins_contras_loss: 1.2368,fake_ins_contras_loss : 8.2463, CL_contra_loss : 0.0274
[2800/3001] Loss_D: -0.4370 Loss_G: 2.6354, Wasserstein_dist: 2.1118, real_ins_contras_loss: 1.2359,fake_ins_contras_loss : 8.3088, CL_contra_loss : 0.0277
model 2 is saved
cls traing
unseen=0.4193, seen=0.3345, h=0.3722
[2900/3001] Loss_D: -0.5527 Loss_G: 2.5647, Wasserstein_dist: 2.1751, real_ins_contras_loss: 1.2137,fake_ins_contras_loss : 8.2200, CL_contra_loss : 0.0275
kl_loss G is 0.0055643790401518345
kl_loss G is 0.0056908694095909595
kl_loss G is 0.005645831115543842
kl_loss G is 0.005667287856340408
[3000/3001] Loss_D: -0.5338 Loss_G: 2.6375, Wasserstein_dist: 2.1419, real_ins_contras_loss: 1.2028,fake_ins_contras_loss : 8.1939, CL_contra_loss : 0.0327
model 2 is saved
cls traing
unseen=0.4101, seen=0.3418, h=0.3728
average_acc is 0.36027668599380197
 hightest s is 0.3298969072164955, u is 0.4298941798941798, H is 0.3733151461851863,
----------------------------------------
loading task 3th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552]
current seen_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 63, 663, 548, 105, 453, 551, 555, 198, 143, 559, 50, 229, 209, 6, 199, 34, 76, 317, 501, 521, 102, 113, 25, 341, 331, 382, 288, 281, 64, 572, 665, 179, 21, 421, 320, 147, 166, 693, 700, 454, 376, 155, 480, 111, 83, 92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169, 642, 337, 106, 552, 55, 567, 478, 161, 139, 396, 544, 386, 195, 531, 661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483, 175, 383, 464, 28, 211, 14, 354, 702, 568, 119, 87, 611, 214, 706, 689, 512, 62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285, 27, 685, 391, 419, 539, 468, 42, 418, 614, 126, 33, 295, 450, 458, 293, 167, 620, 659, 662, 271, 287, 549, 278, 240, 499, 682, 703, 545, 41, 136, 462, 193, 697, 290, 60, 445, 78, 510, 404, 571, 416, 607, 362, 461, 368, 46, 59, 194, 670, 333, 442, 357, 554, 270, 9, 30, 678, 108, 69, 311, 100, 81, 356, 11, 497, 599, 667, 503, 236, 20, 542, 117, 231, 66, 202, 307, 532, 115, 472, 537, 213, 319, 256, 380, 600, 180, 588, 200, 447, 277, 429, 171, 360, 88, 422, 173, 690, 655, 303, 439, 189, 694, 489, 709, 205, 348, 613, 641, 413, 595, 519, 339, 176, 178, 19, 672, 511, 225, 121, 364, 321, 615, 165, 91, 400, 621, 579, 172, 616, 564, 255, 118, 232, 533, 128, 16, 490, 491, 186, 589, 441, 292, 120, 430, 585, 646, 469]
curr_unseen_label is [24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635, 493, 196, 72, 74, 57, 99, 558, 674, 342, 95, 482, 53, 381, 560, 508, 10, 656, 712, 124, 328, 85, 420, 103, 517, 471, 237, 315, 379, 246, 423, 645, 425, 710, 262, 448, 715]
pre_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635, 63, 663, 548, 105, 453, 551, 555, 198, 143, 559, 50, 229, 209, 6, 199, 34, 76, 317, 501, 521, 102, 113, 25, 341, 331, 382, 288, 281, 64, 572, 665, 179, 21, 421, 320, 147, 166, 693, 700, 454, 376, 155, 480, 111, 83, 92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169, 642, 337, 106, 552, 55, 567, 478, 161, 139, 396, 544, 386, 195, 531, 661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483, 175, 383, 464, 28, 211, 14, 354, 702, 568, 119, 87, 611, 214, 706, 689, 512, 62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285, 27, 685, 391, 419, 539, 468, 42, 418, 614, 126, 33, 295, 450, 458, 293, 167, 620, 659, 493, 196, 72, 74, 57, 99, 558, 674, 342, 95, 482, 53, 381, 560, 508, 10, 656, 712, 662, 271, 287, 549, 278, 240, 499, 682, 703, 545, 41, 136, 462, 193, 697, 290, 60, 445, 78, 510, 404, 571, 416, 607, 362, 461, 368, 46, 59, 194, 670, 333, 442, 357, 554, 270, 9, 30, 678, 108, 69, 311, 100, 81, 356, 11, 497, 599, 667, 503, 236, 20, 542, 117, 231, 66, 202, 307, 532, 115, 472, 537, 213, 319, 256, 380, 600, 180, 588, 200, 447, 277, 429, 171, 360, 88, 422, 173, 690, 655, 303, 439, 189, 694, 489, 709, 205, 348, 613, 641, 413, 595, 519, 339, 176, 178, 19, 672, 511, 225, 121, 364, 321, 615, 165, 91, 400, 621, 579, 172, 616, 564, 255, 118, 232, 533, 128, 16, 490, 491, 186, 589, 441, 292, 120, 430, 585, 646, 469, 124, 328, 85, 420, 103, 517, 471, 237, 315, 379, 246, 423, 645, 425, 710, 262, 448, 715]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570]
incremental training task 3···
loading task 3 data and the pre model
model 2 is loaded
kl_loss G is 0.0
kl_loss G is 0.002735451329499483
kl_loss G is 0.0015943656908348203
kl_loss G is 0.0014892725739628077
[0/3001] Loss_D: 4.7057 Loss_G: 2.6907, Wasserstein_dist: 0.6385, real_ins_contras_loss: 4.2232,fake_ins_contras_loss : 8.4996, CL_contra_loss : 0.1035
model 3 is saved
cls traing
unseen=0.3862, seen=0.2549, h=0.3071
[100/3001] Loss_D: 0.2641 Loss_G: 2.0032, Wasserstein_dist: 1.5380, real_ins_contras_loss: 1.3204,fake_ins_contras_loss : 8.0243, CL_contra_loss : 0.0351
[200/3001] Loss_D: -0.0387 Loss_G: 1.9905, Wasserstein_dist: 1.7628, real_ins_contras_loss: 1.2897,fake_ins_contras_loss : 8.0645, CL_contra_loss : 0.0309
model 3 is saved
cls traing
unseen=0.3677, seen=0.2778, h=0.3165
[300/3001] Loss_D: -0.1519 Loss_G: 2.0573, Wasserstein_dist: 1.8014, real_ins_contras_loss: 1.2379,fake_ins_contras_loss : 8.0597, CL_contra_loss : 0.0308
[400/3001] Loss_D: -0.1914 Loss_G: 2.1310, Wasserstein_dist: 1.8803, real_ins_contras_loss: 1.2677,fake_ins_contras_loss : 8.1303, CL_contra_loss : 0.0307
model 3 is saved
cls traing
unseen=0.3554, seen=0.2805, h=0.3135
kl_loss G is 0.004259463865309954
kl_loss G is 0.004311862867325544
kl_loss G is 0.004226387478411198
kl_loss G is 0.00419730506837368
[500/3001] Loss_D: -0.2471 Loss_G: 2.2016, Wasserstein_dist: 1.9424, real_ins_contras_loss: 1.2697,fake_ins_contras_loss : 8.0466, CL_contra_loss : 0.0287
[600/3001] Loss_D: -0.2471 Loss_G: 2.2580, Wasserstein_dist: 1.9805, real_ins_contras_loss: 1.3036,fake_ins_contras_loss : 8.0450, CL_contra_loss : 0.0284
model 3 is saved
cls traing
unseen=0.3527, seen=0.2975, h=0.3228
[700/3001] Loss_D: -0.3025 Loss_G: 2.4237, Wasserstein_dist: 1.9944, real_ins_contras_loss: 1.2474,fake_ins_contras_loss : 8.0803, CL_contra_loss : 0.0276
[800/3001] Loss_D: -0.3816 Loss_G: 2.5473, Wasserstein_dist: 2.0431, real_ins_contras_loss: 1.2482,fake_ins_contras_loss : 8.1003, CL_contra_loss : 0.0273
model 3 is saved
cls traing
unseen=0.3801, seen=0.2774, h=0.3207
[900/3001] Loss_D: -0.4034 Loss_G: 2.6494, Wasserstein_dist: 2.0704, real_ins_contras_loss: 1.2312,fake_ins_contras_loss : 8.1306, CL_contra_loss : 0.0311
kl_loss G is 0.004721672739833593
kl_loss G is 0.004673328250646591
kl_loss G is 0.004742184653878212
kl_loss G is 0.004733778070658445
[1000/3001] Loss_D: -0.3649 Loss_G: 2.5988, Wasserstein_dist: 2.0877, real_ins_contras_loss: 1.2979,fake_ins_contras_loss : 8.1115, CL_contra_loss : 0.0252
model 3 is saved
cls traing
unseen=0.3801, seen=0.2797, h=0.3222
[1100/3001] Loss_D: -0.4537 Loss_G: 2.6393, Wasserstein_dist: 2.1013, real_ins_contras_loss: 1.2366,fake_ins_contras_loss : 8.0266, CL_contra_loss : 0.0296
[1200/3001] Loss_D: -0.4412 Loss_G: 2.8517, Wasserstein_dist: 2.0811, real_ins_contras_loss: 1.2583,fake_ins_contras_loss : 8.1977, CL_contra_loss : 0.0258
model 3 is saved
cls traing
unseen=0.3915, seen=0.2824, h=0.3281
[1300/3001] Loss_D: -0.4491 Loss_G: 2.7639, Wasserstein_dist: 2.1161, real_ins_contras_loss: 1.2293,fake_ins_contras_loss : 8.2087, CL_contra_loss : 0.0269
[1400/3001] Loss_D: -0.4048 Loss_G: 2.8675, Wasserstein_dist: 2.0973, real_ins_contras_loss: 1.2901,fake_ins_contras_loss : 8.2453, CL_contra_loss : 0.0256
model 3 is saved
cls traing
unseen=0.3854, seen=0.2785, h=0.3234
kl_loss G is 0.00489491131156683
kl_loss G is 0.004988903179764748
kl_loss G is 0.0049479324370622635
kl_loss G is 0.004929929040372372
[1500/3001] Loss_D: -0.4026 Loss_G: 2.9568, Wasserstein_dist: 2.0938, real_ins_contras_loss: 1.2853,fake_ins_contras_loss : 8.1998, CL_contra_loss : 0.0289
[1600/3001] Loss_D: -0.4647 Loss_G: 2.9609, Wasserstein_dist: 2.0962, real_ins_contras_loss: 1.2333,fake_ins_contras_loss : 8.1519, CL_contra_loss : 0.0253
model 3 is saved
cls traing
unseen=0.3836, seen=0.2878, h=0.3289
[1700/3001] Loss_D: -0.4668 Loss_G: 2.9720, Wasserstein_dist: 2.0724, real_ins_contras_loss: 1.2181,fake_ins_contras_loss : 8.1768, CL_contra_loss : 0.0261
[1800/3001] Loss_D: -0.4895 Loss_G: 2.9419, Wasserstein_dist: 2.1670, real_ins_contras_loss: 1.2476,fake_ins_contras_loss : 8.2669, CL_contra_loss : 0.0256
model 3 is saved
cls traing
unseen=0.3607, seen=0.2843, h=0.3180
[1900/3001] Loss_D: -0.4990 Loss_G: 2.9637, Wasserstein_dist: 2.1628, real_ins_contras_loss: 1.2551,fake_ins_contras_loss : 8.1382, CL_contra_loss : 0.0278
kl_loss G is 0.005025679245591164
kl_loss G is 0.005001049023121595
kl_loss G is 0.005086060613393784
kl_loss G is 0.004988991655409336
[2000/3001] Loss_D: -0.4511 Loss_G: 3.1354, Wasserstein_dist: 2.1106, real_ins_contras_loss: 1.2370,fake_ins_contras_loss : 8.2520, CL_contra_loss : 0.0261
model 3 is saved
cls traing
unseen=0.3774, seen=0.2878, h=0.3266
[2100/3001] Loss_D: -0.5296 Loss_G: 3.1122, Wasserstein_dist: 2.1787, real_ins_contras_loss: 1.2302,fake_ins_contras_loss : 8.2654, CL_contra_loss : 0.0254
[2200/3001] Loss_D: -0.4151 Loss_G: 3.1794, Wasserstein_dist: 2.0906, real_ins_contras_loss: 1.2427,fake_ins_contras_loss : 8.1997, CL_contra_loss : 0.0287
model 3 is saved
cls traing
unseen=0.3624, seen=0.2851, h=0.3192
[2300/3001] Loss_D: -0.3854 Loss_G: 3.1800, Wasserstein_dist: 2.0663, real_ins_contras_loss: 1.2643,fake_ins_contras_loss : 8.1507, CL_contra_loss : 0.0266
[2400/3001] Loss_D: -0.4306 Loss_G: 3.2522, Wasserstein_dist: 2.0889, real_ins_contras_loss: 1.2610,fake_ins_contras_loss : 8.2393, CL_contra_loss : 0.0251
model 3 is saved
cls traing
unseen=0.3668, seen=0.2897, h=0.3238
kl_loss G is 0.005023523233830929
kl_loss G is 0.005026896018534899
kl_loss G is 0.004999562166631222
kl_loss G is 0.00509267533197999
[2500/3001] Loss_D: -0.4506 Loss_G: 3.1065, Wasserstein_dist: 2.1181, real_ins_contras_loss: 1.2793,fake_ins_contras_loss : 8.2542, CL_contra_loss : 0.0233
[2600/3001] Loss_D: -0.4649 Loss_G: 3.2153, Wasserstein_dist: 2.1292, real_ins_contras_loss: 1.2403,fake_ins_contras_loss : 8.1397, CL_contra_loss : 0.0265
model 3 is saved
cls traing
unseen=0.3633, seen=0.2959, h=0.3262
[2700/3001] Loss_D: -0.4568 Loss_G: 3.2559, Wasserstein_dist: 2.1254, real_ins_contras_loss: 1.2637,fake_ins_contras_loss : 8.2202, CL_contra_loss : 0.0274
[2800/3001] Loss_D: -0.5610 Loss_G: 3.1241, Wasserstein_dist: 2.1849, real_ins_contras_loss: 1.2290,fake_ins_contras_loss : 8.1366, CL_contra_loss : 0.0247
model 3 is saved
cls traing
unseen=0.3642, seen=0.2917, h=0.3239
[2900/3001] Loss_D: -0.4911 Loss_G: 3.2940, Wasserstein_dist: 2.1131, real_ins_contras_loss: 1.2675,fake_ins_contras_loss : 8.2282, CL_contra_loss : 0.0250
kl_loss G is 0.0051738061010837555
kl_loss G is 0.0051771001890301704
kl_loss G is 0.005173421930521727
kl_loss G is 0.0050646597519516945
[3000/3001] Loss_D: -0.5800 Loss_G: 3.2050, Wasserstein_dist: 2.1840, real_ins_contras_loss: 1.2052,fake_ins_contras_loss : 8.1312, CL_contra_loss : 0.0246
model 3 is saved
cls traing
unseen=0.3968, seen=0.2762, h=0.3257
average_acc is 0.3226235606424967
 hightest s is 0.2878143133462282, u is 0.3835978835978835, H is 0.3288738630942128,
----------------------------------------
loading task 4th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698]
current seen_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 63, 663, 548, 105, 453, 551, 555, 198, 143, 559, 50, 229, 209, 6, 199, 34, 76, 317, 501, 521, 102, 113, 25, 341, 331, 382, 288, 281, 64, 572, 665, 179, 21, 421, 320, 147, 166, 693, 700, 454, 376, 155, 480, 111, 83, 92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169, 642, 337, 106, 552, 55, 567, 478, 161, 139, 396, 544, 386, 195, 531, 661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483, 175, 383, 464, 28, 211, 14, 354, 702, 568, 119, 87, 611, 214, 706, 689, 512, 62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285, 27, 685, 391, 419, 539, 468, 42, 418, 614, 126, 33, 295, 450, 458, 293, 167, 620, 659, 662, 271, 287, 549, 278, 240, 499, 682, 703, 545, 41, 136, 462, 193, 697, 290, 60, 445, 78, 510, 404, 571, 416, 607, 362, 461, 368, 46, 59, 194, 670, 333, 442, 357, 554, 270, 9, 30, 678, 108, 69, 311, 100, 81, 356, 11, 497, 599, 667, 503, 236, 20, 542, 117, 231, 66, 202, 307, 532, 115, 472, 537, 213, 319, 256, 380, 600, 180, 588, 200, 447, 277, 429, 171, 360, 88, 422, 173, 690, 655, 303, 439, 189, 694, 489, 709, 205, 348, 613, 641, 413, 595, 519, 339, 176, 178, 19, 672, 511, 225, 121, 364, 321, 615, 165, 91, 400, 621, 579, 172, 616, 564, 255, 118, 232, 533, 128, 16, 490, 491, 186, 589, 441, 292, 120, 430, 585, 646, 469, 540, 640, 296, 485, 638, 424, 90, 432, 275, 649, 628, 206, 253, 406, 125, 639, 56, 297, 291, 159, 541, 428, 373, 327, 443, 26, 228, 7, 684, 465, 345, 0, 407, 207, 624, 70, 330, 340, 487, 338, 538, 598, 168, 505, 44, 457, 504, 29, 241, 137, 329, 603, 514, 547, 67, 80, 592, 15, 516, 343, 151, 390, 351, 634, 239, 575, 561, 135, 280, 273, 261, 134, 546, 467, 584, 355, 578, 535, 630, 692, 498, 107, 582, 433, 140, 563, 704, 215, 691, 515, 156, 605, 234, 494, 597, 701, 581, 495, 146, 45, 177, 708, 526, 619, 131, 463, 565, 377, 707, 263, 304, 673, 163, 452, 84, 427, 426, 401, 608, 238, 475, 502, 669, 133, 534, 378, 361, 89]
curr_unseen_label is [24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635, 493, 196, 72, 74, 57, 99, 558, 674, 342, 95, 482, 53, 381, 560, 508, 10, 656, 712, 124, 328, 85, 420, 103, 517, 471, 237, 315, 379, 246, 423, 645, 425, 710, 262, 448, 715, 158, 75, 245, 138, 254, 298, 622, 152, 509, 679, 145, 184, 3, 32, 658, 650, 353, 23]
pre_label is [367, 8, 279, 610, 269, 164, 609, 387, 243, 459, 671, 5, 570, 346, 375, 212, 65, 264, 154, 122, 664, 54, 305, 536, 323, 18, 109, 657, 399, 47, 637, 455, 392, 82, 566, 98, 174, 626, 302, 438, 250, 636, 274, 324, 417, 114, 153, 651, 477, 486, 217, 2, 94, 569, 612, 444, 192, 436, 162, 129, 366, 101, 402, 116, 527, 518, 142, 500, 272, 222, 301, 686, 372, 576, 574, 267, 446, 644, 208, 316, 550, 528, 79, 12, 294, 39, 654, 242, 363, 306, 591, 617, 157, 596, 226, 300, 543, 473, 252, 248, 556, 141, 666, 312, 513, 688, 648, 223, 244, 334, 394, 148, 299, 435, 58, 562, 577, 557, 61, 520, 632, 352, 397, 474, 601, 393, 699, 705, 602, 289, 77, 227, 395, 332, 326, 523, 371, 431, 313, 86, 388, 403, 492, 51, 344, 52, 359, 470, 384, 335, 4, 374, 268, 627, 43, 31, 123, 653, 266, 411, 410, 219, 188, 412, 71, 573, 643, 93, 604, 149, 530, 586, 224, 35, 506, 127, 398, 347, 283, 37, 449, 488, 629, 623, 13, 633, 22, 1, 185, 370, 414, 713, 524, 36, 308, 310, 181, 73, 553, 460, 314, 668, 660, 522, 144, 484, 349, 680, 618, 183, 230, 687, 97, 104, 160, 258, 434, 235, 593, 322, 132, 714, 247, 197, 284, 40, 276, 437, 48, 587, 17, 677, 365, 204, 606, 479, 210, 260, 675, 369, 481, 257, 507, 456, 191, 265, 251, 49, 182, 190, 68, 594, 476, 405, 408, 325, 218, 96, 583, 24, 681, 286, 38, 221, 440, 130, 216, 631, 358, 580, 711, 529, 695, 112, 259, 336, 635, 63, 663, 548, 105, 453, 551, 555, 198, 143, 559, 50, 229, 209, 6, 199, 34, 76, 317, 501, 521, 102, 113, 25, 341, 331, 382, 288, 281, 64, 572, 665, 179, 21, 421, 320, 147, 166, 693, 700, 454, 376, 155, 480, 111, 83, 92, 676, 451, 110, 201, 625, 496, 683, 385, 590, 169, 642, 337, 106, 552, 55, 567, 478, 161, 139, 396, 544, 386, 195, 531, 661, 652, 309, 647, 170, 282, 415, 150, 698, 203, 187, 220, 318, 483, 175, 383, 464, 28, 211, 14, 354, 702, 568, 119, 87, 611, 214, 706, 689, 512, 62, 466, 409, 525, 696, 350, 389, 233, 249, 716, 285, 27, 685, 391, 419, 539, 468, 42, 418, 614, 126, 33, 295, 450, 458, 293, 167, 620, 659, 493, 196, 72, 74, 57, 99, 558, 674, 342, 95, 482, 53, 381, 560, 508, 10, 656, 712, 662, 271, 287, 549, 278, 240, 499, 682, 703, 545, 41, 136, 462, 193, 697, 290, 60, 445, 78, 510, 404, 571, 416, 607, 362, 461, 368, 46, 59, 194, 670, 333, 442, 357, 554, 270, 9, 30, 678, 108, 69, 311, 100, 81, 356, 11, 497, 599, 667, 503, 236, 20, 542, 117, 231, 66, 202, 307, 532, 115, 472, 537, 213, 319, 256, 380, 600, 180, 588, 200, 447, 277, 429, 171, 360, 88, 422, 173, 690, 655, 303, 439, 189, 694, 489, 709, 205, 348, 613, 641, 413, 595, 519, 339, 176, 178, 19, 672, 511, 225, 121, 364, 321, 615, 165, 91, 400, 621, 579, 172, 616, 564, 255, 118, 232, 533, 128, 16, 490, 491, 186, 589, 441, 292, 120, 430, 585, 646, 469, 124, 328, 85, 420, 103, 517, 471, 237, 315, 379, 246, 423, 645, 425, 710, 262, 448, 715, 540, 640, 296, 485, 638, 424, 90, 432, 275, 649, 628, 206, 253, 406, 125, 639, 56, 297, 291, 159, 541, 428, 373, 327, 443, 26, 228, 7, 684, 465, 345, 0, 407, 207, 624, 70, 330, 340, 487, 338, 538, 598, 168, 505, 44, 457, 504, 29, 241, 137, 329, 603, 514, 547, 67, 80, 592, 15, 516, 343, 151, 390, 351, 634, 239, 575, 561, 135, 280, 273, 261, 134, 546, 467, 584, 355, 578, 535, 630, 692, 498, 107, 582, 433, 140, 563, 704, 215, 691, 515, 156, 605, 234, 494, 597, 701, 581, 495, 146, 45, 177, 708, 526, 619, 131, 463, 565, 377, 707, 263, 304, 673, 163, 452, 84, 427, 426, 401, 608, 238, 475, 502, 669, 133, 534, 378, 361, 89, 158, 75, 245, 138, 254, 298, 622, 152, 509, 679, 145, 184, 3, 32, 658, 650, 353, 23]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716]
incremental training task 4···
loading task 4 data and the pre model
model 3 is loaded
kl_loss G is 0.0
kl_loss G is 0.0026422948576509953
kl_loss G is 0.0015554720303043723
kl_loss G is 0.001440504565834999
[0/3001] Loss_D: 4.7652 Loss_G: 3.2537, Wasserstein_dist: 0.5434, real_ins_contras_loss: 4.2721,fake_ins_contras_loss : 8.5173, CL_contra_loss : 0.0943
model 4 is saved
cls traing
unseen=0.3472, seen=0.2353, h=0.2805
[100/3001] Loss_D: 0.2437 Loss_G: 2.3733, Wasserstein_dist: 1.4988, real_ins_contras_loss: 1.3217,fake_ins_contras_loss : 8.0749, CL_contra_loss : 0.0339
[200/3001] Loss_D: -0.0327 Loss_G: 2.4148, Wasserstein_dist: 1.6846, real_ins_contras_loss: 1.2495,fake_ins_contras_loss : 7.9378, CL_contra_loss : 0.0299
model 4 is saved
cls traing
unseen=0.3512, seen=0.2416, h=0.2862
[300/3001] Loss_D: -0.1754 Loss_G: 2.5545, Wasserstein_dist: 1.8204, real_ins_contras_loss: 1.2535,fake_ins_contras_loss : 8.0205, CL_contra_loss : 0.0281
[400/3001] Loss_D: -0.1955 Loss_G: 2.6370, Wasserstein_dist: 1.8693, real_ins_contras_loss: 1.2997,fake_ins_contras_loss : 8.0374, CL_contra_loss : 0.0269
model 4 is saved
cls traing
unseen=0.3763, seen=0.2291, h=0.2848
kl_loss G is 0.004137761890888214
kl_loss G is 0.004029448144137859
kl_loss G is 0.00412442022934556
kl_loss G is 0.004042300395667553
[500/3001] Loss_D: -0.2848 Loss_G: 2.8051, Wasserstein_dist: 1.9555, real_ins_contras_loss: 1.2843,fake_ins_contras_loss : 8.0836, CL_contra_loss : 0.0271
[600/3001] Loss_D: -0.3271 Loss_G: 2.9727, Wasserstein_dist: 1.9748, real_ins_contras_loss: 1.2346,fake_ins_contras_loss : 8.0225, CL_contra_loss : 0.0266
model 4 is saved
cls traing
unseen=0.3724, seen=0.2341, h=0.2875
[700/3001] Loss_D: -0.2558 Loss_G: 2.9651, Wasserstein_dist: 1.9276, real_ins_contras_loss: 1.2657,fake_ins_contras_loss : 8.0685, CL_contra_loss : 0.0255
[800/3001] Loss_D: -0.3486 Loss_G: 3.1747, Wasserstein_dist: 2.0311, real_ins_contras_loss: 1.2710,fake_ins_contras_loss : 8.0507, CL_contra_loss : 0.0246
model 4 is saved
cls traing
unseen=0.3657, seen=0.2369, h=0.2875
[900/3001] Loss_D: -0.4263 Loss_G: 3.2457, Wasserstein_dist: 2.0479, real_ins_contras_loss: 1.2482,fake_ins_contras_loss : 8.1076, CL_contra_loss : 0.0259
kl_loss G is 0.004581180866807699
kl_loss G is 0.004543325863778591
kl_loss G is 0.004575841128826141
kl_loss G is 0.004571078345179558
[1000/3001] Loss_D: -0.4367 Loss_G: 3.3424, Wasserstein_dist: 2.1109, real_ins_contras_loss: 1.2805,fake_ins_contras_loss : 8.1314, CL_contra_loss : 0.0259
model 4 is saved
cls traing
unseen=0.3631, seen=0.2431, h=0.2912
[1100/3001] Loss_D: -0.4682 Loss_G: 3.3376, Wasserstein_dist: 2.0996, real_ins_contras_loss: 1.2409,fake_ins_contras_loss : 8.0362, CL_contra_loss : 0.0278
[1200/3001] Loss_D: -0.5156 Loss_G: 3.3879, Wasserstein_dist: 2.1426, real_ins_contras_loss: 1.2365,fake_ins_contras_loss : 8.1220, CL_contra_loss : 0.0256
model 4 is saved
cls traing
unseen=0.3571, seen=0.2434, h=0.2895
[1300/3001] Loss_D: -0.4202 Loss_G: 3.5195, Wasserstein_dist: 2.0817, real_ins_contras_loss: 1.2497,fake_ins_contras_loss : 8.0912, CL_contra_loss : 0.0246
[1400/3001] Loss_D: -0.4423 Loss_G: 3.6370, Wasserstein_dist: 2.0981, real_ins_contras_loss: 1.2599,fake_ins_contras_loss : 8.1684, CL_contra_loss : 0.0242
model 4 is saved
cls traing
unseen=0.3684, seen=0.2440, h=0.2936
kl_loss G is 0.004787429701536894
kl_loss G is 0.004636330530047417
kl_loss G is 0.004765662364661694
kl_loss G is 0.004589476622641087
[1500/3001] Loss_D: -0.4500 Loss_G: 3.4982, Wasserstein_dist: 2.0839, real_ins_contras_loss: 1.2565,fake_ins_contras_loss : 8.1685, CL_contra_loss : 0.0234
[1600/3001] Loss_D: -0.4485 Loss_G: 3.5944, Wasserstein_dist: 2.1385, real_ins_contras_loss: 1.2974,fake_ins_contras_loss : 8.1303, CL_contra_loss : 0.0280
model 4 is saved
cls traing
unseen=0.3532, seen=0.2353, h=0.2825
[1700/3001] Loss_D: -0.5012 Loss_G: 3.7024, Wasserstein_dist: 2.1093, real_ins_contras_loss: 1.2319,fake_ins_contras_loss : 8.1828, CL_contra_loss : 0.0245
[1800/3001] Loss_D: -0.4959 Loss_G: 3.7014, Wasserstein_dist: 2.1209, real_ins_contras_loss: 1.2250,fake_ins_contras_loss : 8.1581, CL_contra_loss : 0.0264
model 4 is saved
cls traing
unseen=0.3347, seen=0.2605, h=0.2929
[1900/3001] Loss_D: -0.4870 Loss_G: 3.6164, Wasserstein_dist: 2.1137, real_ins_contras_loss: 1.2446,fake_ins_contras_loss : 8.1009, CL_contra_loss : 0.0256
kl_loss G is 0.005054930225014687
kl_loss G is 0.004834194667637348
kl_loss G is 0.004789415746927261
kl_loss G is 0.00473988801240921
[2000/3001] Loss_D: -0.5348 Loss_G: 3.7866, Wasserstein_dist: 2.1772, real_ins_contras_loss: 1.2373,fake_ins_contras_loss : 8.0878, CL_contra_loss : 0.0246
model 4 is saved
cls traing
unseen=0.3386, seen=0.2617, h=0.2952
[2100/3001] Loss_D: -0.5068 Loss_G: 3.7842, Wasserstein_dist: 2.1695, real_ins_contras_loss: 1.2555,fake_ins_contras_loss : 8.1669, CL_contra_loss : 0.0261
[2200/3001] Loss_D: -0.5207 Loss_G: 3.8185, Wasserstein_dist: 2.1514, real_ins_contras_loss: 1.2398,fake_ins_contras_loss : 8.0863, CL_contra_loss : 0.0247
model 4 is saved
cls traing
unseen=0.3307, seen=0.2567, h=0.2891
[2300/3001] Loss_D: -0.5341 Loss_G: 3.7188, Wasserstein_dist: 2.1684, real_ins_contras_loss: 1.2513,fake_ins_contras_loss : 8.0705, CL_contra_loss : 0.0235
[2400/3001] Loss_D: -0.5143 Loss_G: 3.8173, Wasserstein_dist: 2.1626, real_ins_contras_loss: 1.2338,fake_ins_contras_loss : 8.2265, CL_contra_loss : 0.0246
model 4 is saved
cls traing
unseen=0.3254, seen=0.2651, h=0.2922
kl_loss G is 0.004865254741162062
kl_loss G is 0.004895181395113468
kl_loss G is 0.005000153556466103
kl_loss G is 0.004997185431420803
[2500/3001] Loss_D: -0.5734 Loss_G: 3.7873, Wasserstein_dist: 2.1662, real_ins_contras_loss: 1.2245,fake_ins_contras_loss : 8.1860, CL_contra_loss : 0.0261
[2600/3001] Loss_D: -0.5580 Loss_G: 3.8047, Wasserstein_dist: 2.1813, real_ins_contras_loss: 1.2421,fake_ins_contras_loss : 8.1475, CL_contra_loss : 0.0243
model 4 is saved
cls traing
unseen=0.3538, seen=0.2369, h=0.2838
[2700/3001] Loss_D: -0.5070 Loss_G: 3.7902, Wasserstein_dist: 2.1780, real_ins_contras_loss: 1.2582,fake_ins_contras_loss : 8.1277, CL_contra_loss : 0.0248
[2800/3001] Loss_D: -0.5252 Loss_G: 3.7607, Wasserstein_dist: 2.1749, real_ins_contras_loss: 1.2679,fake_ins_contras_loss : 8.1718, CL_contra_loss : 0.0252
model 4 is saved
cls traing
unseen=0.3604, seen=0.2425, h=0.2899
[2900/3001] Loss_D: -0.5389 Loss_G: 3.9436, Wasserstein_dist: 2.1763, real_ins_contras_loss: 1.2705,fake_ins_contras_loss : 8.1733, CL_contra_loss : 0.0231
kl_loss G is 0.004959946498274803
kl_loss G is 0.005074869841337204
kl_loss G is 0.004795866087079048
kl_loss G is 0.0048997048288583755
[3000/3001] Loss_D: -0.4863 Loss_G: 3.8896, Wasserstein_dist: 2.1717, real_ins_contras_loss: 1.2990,fake_ins_contras_loss : 8.1428, CL_contra_loss : 0.0249
model 4 is saved
cls traing
unseen=0.3333, seen=0.2667, h=0.2963
average_acc is 0.2894872385153514
 hightest s is 0.2666666666666667, u is 0.33333333333333326, H is 0.29629629629629634,
----------------------------------------
loading task 5th data
Traceback (most recent call last):
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 372, in <module>
    train_one_task(premodel = pregan)
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 348, in train_one_task
    data.current_class_index()
  File "E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3\utils\data_pretrain.py", line 270, in current_class_index
    current_data = self.splited_seen_class[self.current_taskid].unsqueeze(dim=1)
IndexError: list index out of range

Process finished with exit code 1
