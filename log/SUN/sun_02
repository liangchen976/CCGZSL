F:\anaconda3_new\python.exe "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py"
Namespace(attSize=102, batch_size=512, beta=60, beta1=0.5, beta_1=10, class_embedding='att',
classifier_lr=0.001, critic_iter=5, cuda=True, dataroot='data', dataset='SUN', dir='models\\SUN',
distill_proj_hidden_dim=4096, embedSize=2048, epochs=100, image_embedding='res101', ins_temp=0.1,
ins_weight=0.001, lambda1=10, lr=0.0001, matdataset=True, nclass_all=717, nclass_seen=645, ndh=4096,
neh=4096, nepoch=3001, ngh=4096, nz=102, outzSize=64, preprocessing=True, pretrain_class_number=130,
pretrain_gan=True, recons_weight=0.001, resSize=2048, shuffer_class=True, standardization=False,
syn_num=256, syn_num_rp=50, syn_num_s=60, syn_num_u=100, task_num=4, validation=False)
data/SUN/res101.mat E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3
att: torch.Size([717, 102])
splited_seen_class is [tensor([424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490,
        587, 534,  33, 345,  46, 127, 455, 258, 554, 567,  70, 340, 547,   6,
         12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477,
        641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341,
        362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519,  14, 118, 417,
        511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269,  28,  89, 227,
        585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330,
         68, 348, 521, 161, 648, 156, 299,  90, 157, 100, 555, 664, 136, 222,
        619,  92, 693, 306,  34, 572, 323, 265, 166, 367, 296, 151, 360,  36,
        125, 445, 604]), tensor([502, 400,   8, 449, 427,  50, 466, 309, 231, 356, 422,  67, 518, 167,
        497, 663, 688, 683, 642,   9,  51, 141,  16, 613, 574, 412, 701,  15,
        322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557,  66, 250, 655,
        174,  81, 264, 457, 194, 256, 537,   2, 395, 411,  91, 121, 357, 467,
        415, 106, 275,  17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595,
        527, 552, 434, 155, 201,  30, 164, 204, 607, 596, 181, 501, 630, 618,
        188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514,
        432, 426, 503, 219, 387,  69, 563,  76,   4, 684,  44, 149,  77,   1,
        586, 675, 414, 146, 627, 153, 276,  54, 212, 463, 405, 556,  11, 713,
        373, 226, 398]), tensor([ 93, 523, 611, 102, 659, 608, 233, 203, 168, 288, 305, 280,  78, 539,
        317, 377,  83, 122, 283, 217, 421, 185, 629, 639, 313, 499, 651, 533,
        653, 284, 433, 208, 458,   5, 115,  82, 344, 248, 191, 617, 626, 251,
        662, 549, 397, 505,  45, 235,  73, 389, 616, 133, 550, 187,  88, 303,
        293, 176, 385, 492, 591,  52, 175, 649, 169, 672,  98, 582, 564, 290,
         37, 113, 689, 229, 172, 578, 333, 661, 171,  19, 140, 668, 491,  97,
        338, 390, 625, 513, 335, 410, 665, 429, 446, 538, 261, 159, 304, 507,
        671, 402, 606, 486, 500, 403, 368, 453, 510, 694, 465, 570, 197, 128,
        464, 535, 346, 372, 702, 331,  21, 515, 487, 528, 364, 257, 301, 481,
         39, 669, 107]), tensor([244, 488, 408,  35, 143, 483, 452, 430, 359, 599,  80, 399, 396, 135,
        343, 239, 318, 300, 339, 170, 699, 271, 347, 404, 709, 571, 247, 636,
         20, 542, 678, 242, 553,  26, 691, 716, 544, 186,  42, 267, 182, 193,
        413, 569, 129, 697, 407, 687,  71, 210, 205, 277, 714,  56, 624, 706,
        677, 531, 388, 255, 512, 579, 605, 548, 273, 310, 576, 588, 524,  96,
          7, 120, 202, 444, 634, 623, 406, 215, 183, 240, 602, 394,  13, 637,
        294, 589, 105, 137, 218, 327, 680,  58, 454, 536, 479, 632, 337, 366,
        391, 468, 451, 437, 532, 370, 312, 473,  62, 600, 526, 480, 384, 447,
         79, 704, 646, 376, 462,  27, 522, 110, 474, 123, 431,  59, 311, 504,
        597, 708])]
splited_unseen_class is [tensor([152, 237, 508, 471, 715, 130, 650,  32, 262, 425, 420, 138, 712, 286,
         23,  99, 440, 246]), tensor([674,  53, 580, 358,  75, 710, 681,  85, 196, 631,  72, 560, 184, 622,
        259, 529,  74, 448]), tensor([103,  38, 645, 298,  24,  57, 711,   3, 379, 216, 635, 695, 145, 656,
        679, 124, 315, 254]), tensor([658, 509, 517, 353, 158, 423, 342, 328, 381, 482, 112, 336, 493, 245,
         10, 221,  95, 558])]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
pre_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643]
curr_seen_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643]
**************************************************
pretraining cgan model ···
[0/3001] Loss_D: 2.9949 Loss_G: -0.7037, Wasserstein_dist: 1.6293, real_ins_contras_loss: 4.5050,fake_ins_contras_loss : 7.5272
[50/3001] Loss_D: -0.1988 Loss_G: 0.1599, Wasserstein_dist: 1.5735, real_ins_contras_loss: 1.2667,fake_ins_contras_loss : 7.4001
[100/3001] Loss_D: 0.0642 Loss_G: -0.1312, Wasserstein_dist: 1.2343, real_ins_contras_loss: 1.2002,fake_ins_contras_loss : 7.5127
[150/3001] Loss_D: 0.1205 Loss_G: 0.4142, Wasserstein_dist: 1.1464, real_ins_contras_loss: 1.2215,fake_ins_contras_loss : 7.4406
model 0 is saved
cls traing
Training classifier loss= 3.4983
seen=0.0523
[200/3001] Loss_D: 0.5809 Loss_G: -0.3102, Wasserstein_dist: 0.7037, real_ins_contras_loss: 1.2608,fake_ins_contras_loss : 7.2965
[250/3001] Loss_D: 0.5618 Loss_G: -0.6896, Wasserstein_dist: 0.6463, real_ins_contras_loss: 1.1960,fake_ins_contras_loss : 7.2523
[300/3001] Loss_D: 0.5455 Loss_G: -0.6973, Wasserstein_dist: 0.6818, real_ins_contras_loss: 1.2067,fake_ins_contras_loss : 7.1148
[350/3001] Loss_D: 0.4669 Loss_G: -0.6234, Wasserstein_dist: 0.7970, real_ins_contras_loss: 1.2277,fake_ins_contras_loss : 7.0891
model 0 is saved
cls traing
Training classifier loss= 2.5526
seen=0.1677
[400/3001] Loss_D: 0.5656 Loss_G: -0.8357, Wasserstein_dist: 0.6883, real_ins_contras_loss: 1.2201,fake_ins_contras_loss : 6.7573
[450/3001] Loss_D: 0.5664 Loss_G: -1.0135, Wasserstein_dist: 0.6601, real_ins_contras_loss: 1.2020,fake_ins_contras_loss : 6.1640
[500/3001] Loss_D: 0.5431 Loss_G: -0.9276, Wasserstein_dist: 0.7093, real_ins_contras_loss: 1.2224,fake_ins_contras_loss : 6.1317
[550/3001] Loss_D: 0.5344 Loss_G: -1.0352, Wasserstein_dist: 0.6787, real_ins_contras_loss: 1.1902,fake_ins_contras_loss : 5.9218
model 0 is saved
cls traing
Training classifier loss= 1.8547
seen=0.2831
[600/3001] Loss_D: 0.6341 Loss_G: -1.0755, Wasserstein_dist: 0.6336, real_ins_contras_loss: 1.2513,fake_ins_contras_loss : 5.7897
[650/3001] Loss_D: 0.4784 Loss_G: -0.7816, Wasserstein_dist: 0.7789, real_ins_contras_loss: 1.2293,fake_ins_contras_loss : 5.7920
[700/3001] Loss_D: 0.4295 Loss_G: -0.7885, Wasserstein_dist: 0.8080, real_ins_contras_loss: 1.2099,fake_ins_contras_loss : 5.6425
[750/3001] Loss_D: 0.4603 Loss_G: -0.7436, Wasserstein_dist: 0.8237, real_ins_contras_loss: 1.2518,fake_ins_contras_loss : 5.6095
model 0 is saved
cls traing
Training classifier loss= 1.4874
seen=0.3708
[800/3001] Loss_D: 0.4096 Loss_G: -0.6713, Wasserstein_dist: 0.8432, real_ins_contras_loss: 1.2062,fake_ins_contras_loss : 5.4908
[850/3001] Loss_D: 0.3276 Loss_G: -0.5751, Wasserstein_dist: 0.9479, real_ins_contras_loss: 1.2250,fake_ins_contras_loss : 5.3013
[900/3001] Loss_D: 0.3318 Loss_G: -0.5138, Wasserstein_dist: 0.9635, real_ins_contras_loss: 1.2543,fake_ins_contras_loss : 5.0726
[950/3001] Loss_D: 0.2375 Loss_G: -0.5266, Wasserstein_dist: 1.0134, real_ins_contras_loss: 1.2165,fake_ins_contras_loss : 4.7989
model 0 is saved
cls traing
Training classifier loss= 1.0931
seen=0.4031
[1000/3001] Loss_D: 0.1803 Loss_G: -0.4364, Wasserstein_dist: 1.0888, real_ins_contras_loss: 1.2163,fake_ins_contras_loss : 4.7337
[1050/3001] Loss_D: 0.1912 Loss_G: -0.3627, Wasserstein_dist: 1.1091, real_ins_contras_loss: 1.2566,fake_ins_contras_loss : 4.5944
[1100/3001] Loss_D: 0.1220 Loss_G: -0.4476, Wasserstein_dist: 1.1009, real_ins_contras_loss: 1.1879,fake_ins_contras_loss : 4.4667
[1150/3001] Loss_D: 0.2050 Loss_G: -0.2979, Wasserstein_dist: 1.0905, real_ins_contras_loss: 1.2617,fake_ins_contras_loss : 3.9594
model 0 is saved
cls traing
Training classifier loss= 0.9358
seen=0.4462
[1200/3001] Loss_D: 0.1059 Loss_G: -0.3234, Wasserstein_dist: 1.1366, real_ins_contras_loss: 1.2024,fake_ins_contras_loss : 3.8439
[1250/3001] Loss_D: 0.0965 Loss_G: -0.3215, Wasserstein_dist: 1.1669, real_ins_contras_loss: 1.2221,fake_ins_contras_loss : 3.8086
[1300/3001] Loss_D: 0.1201 Loss_G: -0.3207, Wasserstein_dist: 1.1638, real_ins_contras_loss: 1.2412,fake_ins_contras_loss : 3.7541
[1350/3001] Loss_D: 0.0423 Loss_G: -0.1846, Wasserstein_dist: 1.2333, real_ins_contras_loss: 1.2303,fake_ins_contras_loss : 3.6992
model 0 is saved
cls traing
Training classifier loss= 0.8323
seen=0.4769
[1400/3001] Loss_D: -0.0225 Loss_G: -0.0731, Wasserstein_dist: 1.3196, real_ins_contras_loss: 1.2155,fake_ins_contras_loss : 3.5797
[1450/3001] Loss_D: 0.0122 Loss_G: -0.0483, Wasserstein_dist: 1.2555, real_ins_contras_loss: 1.2135,fake_ins_contras_loss : 3.5216
[1500/3001] Loss_D: 0.0170 Loss_G: -0.1466, Wasserstein_dist: 1.2499, real_ins_contras_loss: 1.2152,fake_ins_contras_loss : 3.5251
[1550/3001] Loss_D: 0.0049 Loss_G: -0.0169, Wasserstein_dist: 1.2746, real_ins_contras_loss: 1.2249,fake_ins_contras_loss : 3.5173
model 0 is saved
cls traing
Training classifier loss= 0.7401
seen=0.4877
[1600/3001] Loss_D: -0.0376 Loss_G: -0.0176, Wasserstein_dist: 1.3522, real_ins_contras_loss: 1.2327,fake_ins_contras_loss : 3.4053
[1650/3001] Loss_D: -0.0221 Loss_G: 0.0079, Wasserstein_dist: 1.3264, real_ins_contras_loss: 1.2498,fake_ins_contras_loss : 3.3548
[1700/3001] Loss_D: 0.0029 Loss_G: -0.0305, Wasserstein_dist: 1.3344, real_ins_contras_loss: 1.2682,fake_ins_contras_loss : 3.2931
[1750/3001] Loss_D: -0.0852 Loss_G: 0.0720, Wasserstein_dist: 1.3828, real_ins_contras_loss: 1.2299,fake_ins_contras_loss : 3.1654
model 0 is saved
cls traing
Training classifier loss= 0.6827
seen=0.5000
[1800/3001] Loss_D: -0.0502 Loss_G: 0.1894, Wasserstein_dist: 1.3894, real_ins_contras_loss: 1.2430,fake_ins_contras_loss : 3.1748
[1850/3001] Loss_D: -0.0704 Loss_G: 0.1764, Wasserstein_dist: 1.3537, real_ins_contras_loss: 1.2171,fake_ins_contras_loss : 3.1046
[1900/3001] Loss_D: -0.0563 Loss_G: 0.3051, Wasserstein_dist: 1.3559, real_ins_contras_loss: 1.2203,fake_ins_contras_loss : 3.1031
[1950/3001] Loss_D: -0.0576 Loss_G: 0.3368, Wasserstein_dist: 1.3689, real_ins_contras_loss: 1.2361,fake_ins_contras_loss : 2.9878
model 0 is saved
cls traing
Training classifier loss= 0.5660
seen=0.5169
[2000/3001] Loss_D: -0.0566 Loss_G: 0.3333, Wasserstein_dist: 1.3774, real_ins_contras_loss: 1.2397,fake_ins_contras_loss : 2.9227
[2050/3001] Loss_D: -0.0934 Loss_G: 0.5041, Wasserstein_dist: 1.4535, real_ins_contras_loss: 1.2559,fake_ins_contras_loss : 2.9960
[2100/3001] Loss_D: -0.0840 Loss_G: 0.4537, Wasserstein_dist: 1.3810, real_ins_contras_loss: 1.2041,fake_ins_contras_loss : 2.8509
[2150/3001] Loss_D: -0.0571 Loss_G: 0.4956, Wasserstein_dist: 1.4536, real_ins_contras_loss: 1.3003,fake_ins_contras_loss : 2.8355
model 0 is saved
cls traing
Training classifier loss= 0.5549
seen=0.5108
[2200/3001] Loss_D: -0.1799 Loss_G: 0.6186, Wasserstein_dist: 1.4964, real_ins_contras_loss: 1.2153,fake_ins_contras_loss : 2.7579
[2250/3001] Loss_D: -0.1385 Loss_G: 0.4601, Wasserstein_dist: 1.4553, real_ins_contras_loss: 1.2140,fake_ins_contras_loss : 2.7747
[2300/3001] Loss_D: -0.1910 Loss_G: 0.5950, Wasserstein_dist: 1.4939, real_ins_contras_loss: 1.1882,fake_ins_contras_loss : 2.7576
[2350/3001] Loss_D: -0.1893 Loss_G: 0.6997, Wasserstein_dist: 1.5197, real_ins_contras_loss: 1.2048,fake_ins_contras_loss : 2.6687
model 0 is saved
cls traing
Training classifier loss= 0.5761
seen=0.5092
[2400/3001] Loss_D: -0.1099 Loss_G: 0.8279, Wasserstein_dist: 1.4144, real_ins_contras_loss: 1.1968,fake_ins_contras_loss : 2.6425
[2450/3001] Loss_D: -0.2131 Loss_G: 0.7620, Wasserstein_dist: 1.5303, real_ins_contras_loss: 1.2048,fake_ins_contras_loss : 2.6140
[2500/3001] Loss_D: -0.2077 Loss_G: 0.6342, Wasserstein_dist: 1.5235, real_ins_contras_loss: 1.2088,fake_ins_contras_loss : 2.5684
[2550/3001] Loss_D: -0.2032 Loss_G: 0.8167, Wasserstein_dist: 1.5116, real_ins_contras_loss: 1.1926,fake_ins_contras_loss : 2.5920
model 0 is saved
cls traing
Training classifier loss= 0.5242
seen=0.5077
[2600/3001] Loss_D: -0.1966 Loss_G: 0.9981, Wasserstein_dist: 1.5687, real_ins_contras_loss: 1.2468,fake_ins_contras_loss : 2.5668
[2650/3001] Loss_D: -0.1776 Loss_G: 1.1308, Wasserstein_dist: 1.4974, real_ins_contras_loss: 1.2155,fake_ins_contras_loss : 2.5280
[2700/3001] Loss_D: -0.1610 Loss_G: 1.0390, Wasserstein_dist: 1.4870, real_ins_contras_loss: 1.2177,fake_ins_contras_loss : 2.5136
[2750/3001] Loss_D: -0.2162 Loss_G: 0.9907, Wasserstein_dist: 1.5903, real_ins_contras_loss: 1.2469,fake_ins_contras_loss : 2.4862
model 0 is saved
cls traing
Training classifier loss= 0.4149
seen=0.5277
[2800/3001] Loss_D: -0.2223 Loss_G: 0.9235, Wasserstein_dist: 1.5188, real_ins_contras_loss: 1.1793,fake_ins_contras_loss : 2.4643
[2850/3001] Loss_D: -0.2337 Loss_G: 0.8387, Wasserstein_dist: 1.5778, real_ins_contras_loss: 1.2036,fake_ins_contras_loss : 2.4143
[2900/3001] Loss_D: -0.1987 Loss_G: 1.0790, Wasserstein_dist: 1.5280, real_ins_contras_loss: 1.2114,fake_ins_contras_loss : 2.4115
[2950/3001] Loss_D: -0.2480 Loss_G: 1.1932, Wasserstein_dist: 1.5800, real_ins_contras_loss: 1.2024,fake_ins_contras_loss : 2.4281
model 0 is saved
cls traing
Training classifier loss= 0.4725
seen=0.5185
[3000/3001] Loss_D: -0.2617 Loss_G: 1.2416, Wasserstein_dist: 1.6027, real_ins_contras_loss: 1.2057,fake_ins_contras_loss : 2.3866
average_acc is 0.4447252747252746
----------------------------------------
loading task 1th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258]
current seen_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604]
curr_unseen_label is [152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246]
pre_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276]
finish pretraining cgan model ···
incremental training task 1···
loading task 1 data and the pre model
model 0 is loaded
kl_loss G is 0.008384359069168568
kl_loss G is 0.005395825486630201
kl_loss G is 0.0034460758324712515
kl_loss G is 0.002592511707916856
[0/3001] Loss_D: 9.5875 Loss_G: 1.0544, Wasserstein_dist: 0.1000, real_ins_contras_loss: 4.6900,fake_ins_contras_loss : 8.1860, CL_contra_loss : 0.4515
model 1 is saved
cls traing
unseen=0.4894, seen=0.2479, h=0.3291
[100/3001] Loss_D: 0.4519 Loss_G: 2.0709, Wasserstein_dist: 1.4141, real_ins_contras_loss: 1.2901,fake_ins_contras_loss : 8.0069, CL_contra_loss : 0.0471
[200/3001] Loss_D: 0.1756 Loss_G: 2.2910, Wasserstein_dist: 1.6542, real_ins_contras_loss: 1.2937,fake_ins_contras_loss : 8.0249, CL_contra_loss : 0.0377
model 1 is saved
cls traing
unseen=0.4048, seen=0.3042, h=0.3474
[300/3001] Loss_D: -0.0426 Loss_G: 2.2185, Wasserstein_dist: 1.8032, real_ins_contras_loss: 1.2715,fake_ins_contras_loss : 8.0192, CL_contra_loss : 0.0362
[400/3001] Loss_D: -0.0931 Loss_G: 2.2791, Wasserstein_dist: 1.8422, real_ins_contras_loss: 1.2386,fake_ins_contras_loss : 8.0424, CL_contra_loss : 0.0347
model 1 is saved
cls traing
unseen=0.4550, seen=0.3151, h=0.3723
kl_loss G is 0.0059813931584358215
kl_loss G is 0.005880553275346756
kl_loss G is 0.006022544577717781
kl_loss G is 0.005937919020652771
[500/3001] Loss_D: -0.1318 Loss_G: 2.1758, Wasserstein_dist: 1.9008, real_ins_contras_loss: 1.2572,fake_ins_contras_loss : 8.0970, CL_contra_loss : 0.0344
[600/3001] Loss_D: -0.2260 Loss_G: 2.2519, Wasserstein_dist: 1.9662, real_ins_contras_loss: 1.2497,fake_ins_contras_loss : 8.1398, CL_contra_loss : 0.0330
model 1 is saved
cls traing
unseen=0.4630, seen=0.3436, h=0.3945
[700/3001] Loss_D: -0.3223 Loss_G: 2.2622, Wasserstein_dist: 2.0851, real_ins_contras_loss: 1.2802,fake_ins_contras_loss : 8.2160, CL_contra_loss : 0.0327
[800/3001] Loss_D: -0.2866 Loss_G: 2.3434, Wasserstein_dist: 2.0453, real_ins_contras_loss: 1.2526,fake_ins_contras_loss : 8.1554, CL_contra_loss : 0.0327
model 1 is saved
cls traing
unseen=0.4550, seen=0.3398, h=0.3890
[900/3001] Loss_D: -0.2925 Loss_G: 2.3480, Wasserstein_dist: 2.0176, real_ins_contras_loss: 1.2427,fake_ins_contras_loss : 8.2321, CL_contra_loss : 0.0309
kl_loss G is 0.006566914729773998
kl_loss G is 0.006572169717401266
kl_loss G is 0.006526833400130272
kl_loss G is 0.0067298524081707
[1000/3001] Loss_D: -0.3904 Loss_G: 2.4317, Wasserstein_dist: 2.1000, real_ins_contras_loss: 1.2386,fake_ins_contras_loss : 8.1426, CL_contra_loss : 0.0298
model 1 is saved
cls traing
unseen=0.4762, seen=0.3290, h=0.3891
[1100/3001] Loss_D: -0.2730 Loss_G: 2.5430, Wasserstein_dist: 2.0454, real_ins_contras_loss: 1.2890,fake_ins_contras_loss : 8.2577, CL_contra_loss : 0.0327
[1200/3001] Loss_D: -0.3606 Loss_G: 2.4579, Wasserstein_dist: 2.1184, real_ins_contras_loss: 1.2840,fake_ins_contras_loss : 8.1635, CL_contra_loss : 0.0289
model 1 is saved
cls traing
unseen=0.4550, seen=0.3429, h=0.3911
[1300/3001] Loss_D: -0.3356 Loss_G: 2.6103, Wasserstein_dist: 2.0297, real_ins_contras_loss: 1.2210,fake_ins_contras_loss : 8.1978, CL_contra_loss : 0.0327
[1400/3001] Loss_D: -0.4054 Loss_G: 2.6877, Wasserstein_dist: 2.1460, real_ins_contras_loss: 1.2494,fake_ins_contras_loss : 8.3301, CL_contra_loss : 0.0320
model 1 is saved
cls traing
unseen=0.4788, seen=0.3483, h=0.4032
kl_loss G is 0.006751748267561197
kl_loss G is 0.006588037591427565
kl_loss G is 0.006739362608641386
kl_loss G is 0.006875406485050917
[1500/3001] Loss_D: -0.3896 Loss_G: 2.7207, Wasserstein_dist: 2.1415, real_ins_contras_loss: 1.2714,fake_ins_contras_loss : 8.1837, CL_contra_loss : 0.0305
[1600/3001] Loss_D: -0.3676 Loss_G: 2.7175, Wasserstein_dist: 2.1223, real_ins_contras_loss: 1.2716,fake_ins_contras_loss : 8.2679, CL_contra_loss : 0.0304
model 1 is saved
cls traing
unseen=0.4603, seen=0.3622, h=0.4054
[1700/3001] Loss_D: -0.3721 Loss_G: 2.8575, Wasserstein_dist: 2.1030, real_ins_contras_loss: 1.2626,fake_ins_contras_loss : 8.2761, CL_contra_loss : 0.0327
[1800/3001] Loss_D: -0.4859 Loss_G: 2.9023, Wasserstein_dist: 2.1517, real_ins_contras_loss: 1.2057,fake_ins_contras_loss : 8.2433, CL_contra_loss : 0.0336
model 1 is saved
cls traing
unseen=0.4921, seen=0.3444, h=0.4052
[1900/3001] Loss_D: -0.4528 Loss_G: 2.8882, Wasserstein_dist: 2.1222, real_ins_contras_loss: 1.2463,fake_ins_contras_loss : 8.3452, CL_contra_loss : 0.0296
kl_loss G is 0.006856522522866726
kl_loss G is 0.006765764206647873
kl_loss G is 0.006727077532559633
kl_loss G is 0.0068701403215527534
[2000/3001] Loss_D: -0.3924 Loss_G: 3.0399, Wasserstein_dist: 2.1240, real_ins_contras_loss: 1.2657,fake_ins_contras_loss : 8.2807, CL_contra_loss : 0.0309
model 1 is saved
cls traing
unseen=0.5000, seen=0.3737, h=0.4278
[2100/3001] Loss_D: -0.4734 Loss_G: 3.0836, Wasserstein_dist: 2.1111, real_ins_contras_loss: 1.1984,fake_ins_contras_loss : 8.2589, CL_contra_loss : 0.0308
[2200/3001] Loss_D: -0.4349 Loss_G: 3.0940, Wasserstein_dist: 2.1415, real_ins_contras_loss: 1.2614,fake_ins_contras_loss : 8.2760, CL_contra_loss : 0.0308
model 1 is saved
cls traing
unseen=0.5238, seen=0.3683, h=0.4325
[2300/3001] Loss_D: -0.4596 Loss_G: 3.2283, Wasserstein_dist: 2.1576, real_ins_contras_loss: 1.2557,fake_ins_contras_loss : 8.3089, CL_contra_loss : 0.0282
[2400/3001] Loss_D: -0.4129 Loss_G: 3.2050, Wasserstein_dist: 2.1474, real_ins_contras_loss: 1.2644,fake_ins_contras_loss : 8.2894, CL_contra_loss : 0.0287
model 1 is saved
cls traing
unseen=0.4974, seen=0.3792, h=0.4303
kl_loss G is 0.006834486499428749
kl_loss G is 0.006807483732700348
kl_loss G is 0.0068677919916808605
kl_loss G is 0.006801437586545944
[2500/3001] Loss_D: -0.4820 Loss_G: 3.3149, Wasserstein_dist: 2.1803, real_ins_contras_loss: 1.2468,fake_ins_contras_loss : 8.2685, CL_contra_loss : 0.0267
[2600/3001] Loss_D: -0.4487 Loss_G: 3.3288, Wasserstein_dist: 2.1822, real_ins_contras_loss: 1.2843,fake_ins_contras_loss : 8.3182, CL_contra_loss : 0.0301
model 1 is saved
cls traing
unseen=0.5026, seen=0.3629, h=0.4215
[2700/3001] Loss_D: -0.4739 Loss_G: 3.4276, Wasserstein_dist: 2.1719, real_ins_contras_loss: 1.2329,fake_ins_contras_loss : 8.2593, CL_contra_loss : 0.0297
[2800/3001] Loss_D: -0.4240 Loss_G: 3.4074, Wasserstein_dist: 2.1214, real_ins_contras_loss: 1.2621,fake_ins_contras_loss : 8.3222, CL_contra_loss : 0.0267
model 1 is saved
cls traing
unseen=0.4868, seen=0.3807, h=0.4272
[2900/3001] Loss_D: -0.4861 Loss_G: 3.3326, Wasserstein_dist: 2.1606, real_ins_contras_loss: 1.2332,fake_ins_contras_loss : 8.3647, CL_contra_loss : 0.0271
kl_loss G is 0.006810653023421764
kl_loss G is 0.006702326238155365
kl_loss G is 0.0067526912316679955
kl_loss G is 0.007021714933216572
[3000/3001] Loss_D: -0.4530 Loss_G: 3.5729, Wasserstein_dist: 2.1942, real_ins_contras_loss: 1.2794,fake_ins_contras_loss : 8.3135, CL_contra_loss : 0.0271
model 1 is saved
cls traing
unseen=0.5000, seen=0.3815, h=0.4328
average_acc is 0.4046194655952496
 hightest s is 0.3814671814671815, u is 0.5, H is 0.43276390713972845,
----------------------------------------
loading task 2th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]
current seen_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 502, 400, 8, 449, 427, 50, 466, 309, 231, 356, 422, 67, 518, 167, 497, 663, 688, 683, 642, 9, 51, 141, 16, 613, 574, 412, 701, 15, 322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557, 66, 250, 655, 174, 81, 264, 457, 194, 256, 537, 2, 395, 411, 91, 121, 357, 467, 415, 106, 275, 17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595, 527, 552, 434, 155, 201, 30, 164, 204, 607, 596, 181, 501, 630, 618, 188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514, 432, 426, 503, 219, 387, 69, 563, 76, 4, 684, 44, 149, 77, 1, 586, 675, 414, 146, 627, 153, 276, 54, 212, 463, 405, 556, 11, 713, 373, 226, 398]
curr_unseen_label is [152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246, 674, 53, 580, 358, 75, 710, 681, 85, 196, 631, 72, 560, 184, 622, 259, 529, 74, 448]
pre_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246, 502, 400, 8, 449, 427, 50, 466, 309, 231, 356, 422, 67, 518, 167, 497, 663, 688, 683, 642, 9, 51, 141, 16, 613, 574, 412, 701, 15, 322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557, 66, 250, 655, 174, 81, 264, 457, 194, 256, 537, 2, 395, 411, 91, 121, 357, 467, 415, 106, 275, 17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595, 527, 552, 434, 155, 201, 30, 164, 204, 607, 596, 181, 501, 630, 618, 188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514, 432, 426, 503, 219, 387, 69, 563, 76, 4, 684, 44, 149, 77, 1, 586, 675, 414, 146, 627, 153, 276, 54, 212, 463, 405, 556, 11, 713, 373, 226, 398, 674, 53, 580, 358, 75, 710, 681, 85, 196, 631, 72, 560, 184, 622, 259, 529, 74, 448]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]
incremental training task 2···
loading task 2 data and the pre model
model 1 is loaded
kl_loss G is 0.0
kl_loss G is 0.0027516321279108524
kl_loss G is 0.0016063631046563387
kl_loss G is 0.0015048731584101915
[0/3001] Loss_D: 4.7437 Loss_G: 3.6503, Wasserstein_dist: 0.7722, real_ins_contras_loss: 4.1423,fake_ins_contras_loss : 8.4416, CL_contra_loss : 0.1249
model 2 is saved
cls traing
unseen=0.4259, seen=0.2701, h=0.3306
[100/3001] Loss_D: 0.1039 Loss_G: 2.8148, Wasserstein_dist: 1.6828, real_ins_contras_loss: 1.3074,fake_ins_contras_loss : 8.0825, CL_contra_loss : 0.0373
[200/3001] Loss_D: -0.0568 Loss_G: 2.4998, Wasserstein_dist: 1.8152, real_ins_contras_loss: 1.2852,fake_ins_contras_loss : 8.1204, CL_contra_loss : 0.0317
model 2 is saved
cls traing
unseen=0.4034, seen=0.2851, h=0.3341
[300/3001] Loss_D: -0.2586 Loss_G: 2.4950, Wasserstein_dist: 1.9767, real_ins_contras_loss: 1.2808,fake_ins_contras_loss : 8.0807, CL_contra_loss : 0.0309
[400/3001] Loss_D: -0.2880 Loss_G: 2.4705, Wasserstein_dist: 2.0234, real_ins_contras_loss: 1.2848,fake_ins_contras_loss : 8.1032, CL_contra_loss : 0.0316
model 2 is saved
cls traing
unseen=0.4630, seen=0.2825, h=0.3509
kl_loss G is 0.004783208481967449
kl_loss G is 0.00483283493667841
kl_loss G is 0.004834515508264303
kl_loss G is 0.004848673474043608
[500/3001] Loss_D: -0.2928 Loss_G: 2.6713, Wasserstein_dist: 2.0447, real_ins_contras_loss: 1.2947,fake_ins_contras_loss : 8.1247, CL_contra_loss : 0.0292
[600/3001] Loss_D: -0.3666 Loss_G: 2.8150, Wasserstein_dist: 2.0713, real_ins_contras_loss: 1.2659,fake_ins_contras_loss : 8.1397, CL_contra_loss : 0.0295
model 2 is saved
cls traing
unseen=0.4590, seen=0.2825, h=0.3497
[700/3001] Loss_D: -0.4402 Loss_G: 2.8636, Wasserstein_dist: 2.1201, real_ins_contras_loss: 1.2313,fake_ins_contras_loss : 8.1684, CL_contra_loss : 0.0283
[800/3001] Loss_D: -0.4707 Loss_G: 3.0909, Wasserstein_dist: 2.1021, real_ins_contras_loss: 1.2195,fake_ins_contras_loss : 8.1023, CL_contra_loss : 0.0327
model 2 is saved
cls traing
unseen=0.4087, seen=0.3088, h=0.3518
[900/3001] Loss_D: -0.4578 Loss_G: 3.0637, Wasserstein_dist: 2.1518, real_ins_contras_loss: 1.2475,fake_ins_contras_loss : 8.1978, CL_contra_loss : 0.0295
kl_loss G is 0.005356143228709698
kl_loss G is 0.0054564094170928
kl_loss G is 0.005353604443371296
kl_loss G is 0.005445102695375681
[1000/3001] Loss_D: -0.3975 Loss_G: 3.2905, Wasserstein_dist: 2.0837, real_ins_contras_loss: 1.2377,fake_ins_contras_loss : 8.1528, CL_contra_loss : 0.0289
model 2 is saved
cls traing
unseen=0.4272, seen=0.3119, h=0.3605
[1100/3001] Loss_D: -0.4538 Loss_G: 3.3715, Wasserstein_dist: 2.1120, real_ins_contras_loss: 1.2449,fake_ins_contras_loss : 8.2193, CL_contra_loss : 0.0271
[1200/3001] Loss_D: -0.4922 Loss_G: 3.4120, Wasserstein_dist: 2.1301, real_ins_contras_loss: 1.2123,fake_ins_contras_loss : 8.1324, CL_contra_loss : 0.0306
model 2 is saved
cls traing
unseen=0.4603, seen=0.2974, h=0.3614
[1300/3001] Loss_D: -0.5382 Loss_G: 3.5549, Wasserstein_dist: 2.2150, real_ins_contras_loss: 1.2487,fake_ins_contras_loss : 8.2562, CL_contra_loss : 0.0270
[1400/3001] Loss_D: -0.5275 Loss_G: 3.4926, Wasserstein_dist: 2.2331, real_ins_contras_loss: 1.2577,fake_ins_contras_loss : 8.2208, CL_contra_loss : 0.0289
model 2 is saved
cls traing
unseen=0.4603, seen=0.2918, h=0.3571
kl_loss G is 0.005721712484955788
kl_loss G is 0.00565876392647624
kl_loss G is 0.005654292181134224
kl_loss G is 0.00559750571846962
[1500/3001] Loss_D: -0.4586 Loss_G: 3.6469, Wasserstein_dist: 2.1779, real_ins_contras_loss: 1.2886,fake_ins_contras_loss : 8.2047, CL_contra_loss : 0.0271
[1600/3001] Loss_D: -0.4198 Loss_G: 3.7238, Wasserstein_dist: 2.1432, real_ins_contras_loss: 1.2721,fake_ins_contras_loss : 8.2406, CL_contra_loss : 0.0294
model 2 is saved
cls traing
unseen=0.4325, seen=0.3082, h=0.3600
[1700/3001] Loss_D: -0.4993 Loss_G: 3.7571, Wasserstein_dist: 2.1544, real_ins_contras_loss: 1.2117,fake_ins_contras_loss : 8.1559, CL_contra_loss : 0.0286
[1800/3001] Loss_D: -0.5561 Loss_G: 3.7135, Wasserstein_dist: 2.1987, real_ins_contras_loss: 1.2187,fake_ins_contras_loss : 8.1833, CL_contra_loss : 0.0298
model 2 is saved
cls traing
unseen=0.4272, seen=0.3062, h=0.3567
[1900/3001] Loss_D: -0.4820 Loss_G: 3.8637, Wasserstein_dist: 2.2025, real_ins_contras_loss: 1.2946,fake_ins_contras_loss : 8.2061, CL_contra_loss : 0.0265
kl_loss G is 0.00584863405674696
kl_loss G is 0.005749027710407972
kl_loss G is 0.005861734040081501
kl_loss G is 0.0056715295650064945
[2000/3001] Loss_D: -0.4433 Loss_G: 3.8795, Wasserstein_dist: 2.1425, real_ins_contras_loss: 1.2559,fake_ins_contras_loss : 8.2005, CL_contra_loss : 0.0275
model 2 is saved
cls traing
unseen=0.4153, seen=0.3077, h=0.3535
[2100/3001] Loss_D: -0.5385 Loss_G: 3.8759, Wasserstein_dist: 2.1881, real_ins_contras_loss: 1.2272,fake_ins_contras_loss : 8.2504, CL_contra_loss : 0.0273
[2200/3001] Loss_D: -0.5363 Loss_G: 3.7610, Wasserstein_dist: 2.1935, real_ins_contras_loss: 1.2455,fake_ins_contras_loss : 8.2353, CL_contra_loss : 0.0289
model 2 is saved
cls traing
unseen=0.3942, seen=0.3304, h=0.3595
[2300/3001] Loss_D: -0.4512 Loss_G: 3.9733, Wasserstein_dist: 2.1224, real_ins_contras_loss: 1.2273,fake_ins_contras_loss : 8.2318, CL_contra_loss : 0.0295
[2400/3001] Loss_D: -0.4528 Loss_G: 4.0597, Wasserstein_dist: 2.1380, real_ins_contras_loss: 1.2558,fake_ins_contras_loss : 8.2834, CL_contra_loss : 0.0291
model 2 is saved
cls traing
unseen=0.3995, seen=0.3253, h=0.3586
kl_loss G is 0.006038527935743332
kl_loss G is 0.006029799580574036
kl_loss G is 0.005982071161270142
kl_loss G is 0.0059248413890600204
[2500/3001] Loss_D: -0.4955 Loss_G: 3.9782, Wasserstein_dist: 2.1951, real_ins_contras_loss: 1.2489,fake_ins_contras_loss : 8.3531, CL_contra_loss : 0.0271
[2600/3001] Loss_D: -0.4833 Loss_G: 4.0040, Wasserstein_dist: 2.1718, real_ins_contras_loss: 1.2607,fake_ins_contras_loss : 8.2193, CL_contra_loss : 0.0268
model 2 is saved
cls traing
unseen=0.4577, seen=0.3000, h=0.3624
[2700/3001] Loss_D: -0.4657 Loss_G: 4.1956, Wasserstein_dist: 2.1612, real_ins_contras_loss: 1.2317,fake_ins_contras_loss : 8.2775, CL_contra_loss : 0.0281
[2800/3001] Loss_D: -0.4827 Loss_G: 4.0950, Wasserstein_dist: 2.1675, real_ins_contras_loss: 1.2573,fake_ins_contras_loss : 8.2768, CL_contra_loss : 0.0252
model 2 is saved
cls traing
unseen=0.4431, seen=0.3005, h=0.3581
[2900/3001] Loss_D: -0.5495 Loss_G: 4.0021, Wasserstein_dist: 2.1843, real_ins_contras_loss: 1.2149,fake_ins_contras_loss : 8.3016, CL_contra_loss : 0.0300
kl_loss G is 0.00611006747931242
kl_loss G is 0.0059313299134373665
kl_loss G is 0.006145774386823177
kl_loss G is 0.006068709306418896
[3000/3001] Loss_D: -0.4555 Loss_G: 4.1276, Wasserstein_dist: 2.1357, real_ins_contras_loss: 1.2543,fake_ins_contras_loss : 8.1946, CL_contra_loss : 0.0267
model 2 is saved
cls traing
unseen=0.4233, seen=0.3258, h=0.3682
average_acc is 0.3561681315691153
 hightest s is 0.32577319587628906, u is 0.42328042328042326, H is 0.3681803617721595,
----------------------------------------
loading task 3th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552]
current seen_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 502, 400, 8, 449, 427, 50, 466, 309, 231, 356, 422, 67, 518, 167, 497, 663, 688, 683, 642, 9, 51, 141, 16, 613, 574, 412, 701, 15, 322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557, 66, 250, 655, 174, 81, 264, 457, 194, 256, 537, 2, 395, 411, 91, 121, 357, 467, 415, 106, 275, 17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595, 527, 552, 434, 155, 201, 30, 164, 204, 607, 596, 181, 501, 630, 618, 188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514, 432, 426, 503, 219, 387, 69, 563, 76, 4, 684, 44, 149, 77, 1, 586, 675, 414, 146, 627, 153, 276, 54, 212, 463, 405, 556, 11, 713, 373, 226, 398, 93, 523, 611, 102, 659, 608, 233, 203, 168, 288, 305, 280, 78, 539, 317, 377, 83, 122, 283, 217, 421, 185, 629, 639, 313, 499, 651, 533, 653, 284, 433, 208, 458, 5, 115, 82, 344, 248, 191, 617, 626, 251, 662, 549, 397, 505, 45, 235, 73, 389, 616, 133, 550, 187, 88, 303, 293, 176, 385, 492, 591, 52, 175, 649, 169, 672, 98, 582, 564, 290, 37, 113, 689, 229, 172, 578, 333, 661, 171, 19, 140, 668, 491, 97, 338, 390, 625, 513, 335, 410, 665, 429, 446, 538, 261, 159, 304, 507, 671, 402, 606, 486, 500, 403, 368, 453, 510, 694, 465, 570, 197, 128, 464, 535, 346, 372, 702, 331, 21, 515, 487, 528, 364, 257, 301, 481, 39, 669, 107]
curr_unseen_label is [152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246, 674, 53, 580, 358, 75, 710, 681, 85, 196, 631, 72, 560, 184, 622, 259, 529, 74, 448, 103, 38, 645, 298, 24, 57, 711, 3, 379, 216, 635, 695, 145, 656, 679, 124, 315, 254]
pre_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246, 502, 400, 8, 449, 427, 50, 466, 309, 231, 356, 422, 67, 518, 167, 497, 663, 688, 683, 642, 9, 51, 141, 16, 613, 574, 412, 701, 15, 322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557, 66, 250, 655, 174, 81, 264, 457, 194, 256, 537, 2, 395, 411, 91, 121, 357, 467, 415, 106, 275, 17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595, 527, 552, 434, 155, 201, 30, 164, 204, 607, 596, 181, 501, 630, 618, 188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514, 432, 426, 503, 219, 387, 69, 563, 76, 4, 684, 44, 149, 77, 1, 586, 675, 414, 146, 627, 153, 276, 54, 212, 463, 405, 556, 11, 713, 373, 226, 398, 674, 53, 580, 358, 75, 710, 681, 85, 196, 631, 72, 560, 184, 622, 259, 529, 74, 448, 93, 523, 611, 102, 659, 608, 233, 203, 168, 288, 305, 280, 78, 539, 317, 377, 83, 122, 283, 217, 421, 185, 629, 639, 313, 499, 651, 533, 653, 284, 433, 208, 458, 5, 115, 82, 344, 248, 191, 617, 626, 251, 662, 549, 397, 505, 45, 235, 73, 389, 616, 133, 550, 187, 88, 303, 293, 176, 385, 492, 591, 52, 175, 649, 169, 672, 98, 582, 564, 290, 37, 113, 689, 229, 172, 578, 333, 661, 171, 19, 140, 668, 491, 97, 338, 390, 625, 513, 335, 410, 665, 429, 446, 538, 261, 159, 304, 507, 671, 402, 606, 486, 500, 403, 368, 453, 510, 694, 465, 570, 197, 128, 464, 535, 346, 372, 702, 331, 21, 515, 487, 528, 364, 257, 301, 481, 39, 669, 107, 103, 38, 645, 298, 24, 57, 711, 3, 379, 216, 635, 695, 145, 656, 679, 124, 315, 254]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570]
incremental training task 3···
loading task 3 data and the pre model
model 2 is loaded
kl_loss G is 0.0
kl_loss G is 0.0029037650674581528
kl_loss G is 0.0016399429878219962
kl_loss G is 0.0014913479099050164
[0/3001] Loss_D: 4.5901 Loss_G: 4.3526, Wasserstein_dist: 0.6183, real_ins_contras_loss: 4.1024,fake_ins_contras_loss : 8.6860, CL_contra_loss : 0.1020
model 3 is saved
cls traing
unseen=0.3483, seen=0.2607, h=0.2982
[100/3001] Loss_D: 0.1900 Loss_G: 3.1906, Wasserstein_dist: 1.5867, real_ins_contras_loss: 1.3314,fake_ins_contras_loss : 8.0555, CL_contra_loss : 0.0351
[200/3001] Loss_D: 0.0372 Loss_G: 3.0845, Wasserstein_dist: 1.7052, real_ins_contras_loss: 1.3206,fake_ins_contras_loss : 8.1009, CL_contra_loss : 0.0303
model 3 is saved
cls traing
unseen=0.3554, seen=0.2665, h=0.3046
[300/3001] Loss_D: -0.1092 Loss_G: 3.1591, Wasserstein_dist: 1.8301, real_ins_contras_loss: 1.2946,fake_ins_contras_loss : 8.1391, CL_contra_loss : 0.0295
[400/3001] Loss_D: -0.2452 Loss_G: 3.1941, Wasserstein_dist: 1.8784, real_ins_contras_loss: 1.2710,fake_ins_contras_loss : 8.0427, CL_contra_loss : 0.0272
model 3 is saved
cls traing
unseen=0.3660, seen=0.2750, h=0.3141
kl_loss G is 0.00456369249150157
kl_loss G is 0.004491970408707857
kl_loss G is 0.004548604134470224
kl_loss G is 0.004573489539325237
[500/3001] Loss_D: -0.2680 Loss_G: 3.2144, Wasserstein_dist: 1.9037, real_ins_contras_loss: 1.2548,fake_ins_contras_loss : 8.1122, CL_contra_loss : 0.0298
[600/3001] Loss_D: -0.3192 Loss_G: 3.5320, Wasserstein_dist: 1.9688, real_ins_contras_loss: 1.2379,fake_ins_contras_loss : 8.0720, CL_contra_loss : 0.0273
model 3 is saved
cls traing
unseen=0.3695, seen=0.2689, h=0.3112
[700/3001] Loss_D: -0.3838 Loss_G: 3.5349, Wasserstein_dist: 2.0304, real_ins_contras_loss: 1.2381,fake_ins_contras_loss : 8.1288, CL_contra_loss : 0.0295
[800/3001] Loss_D: -0.3747 Loss_G: 3.6396, Wasserstein_dist: 2.0252, real_ins_contras_loss: 1.2492,fake_ins_contras_loss : 8.0720, CL_contra_loss : 0.0276
model 3 is saved
cls traing
unseen=0.3589, seen=0.2832, h=0.3166
[900/3001] Loss_D: -0.3453 Loss_G: 3.7481, Wasserstein_dist: 2.0319, real_ins_contras_loss: 1.2451,fake_ins_contras_loss : 8.1474, CL_contra_loss : 0.0299
kl_loss G is 0.004974359646439552
kl_loss G is 0.0048846956342458725
kl_loss G is 0.004981103353202343
kl_loss G is 0.004876112565398216
[1000/3001] Loss_D: -0.3879 Loss_G: 3.9427, Wasserstein_dist: 2.0386, real_ins_contras_loss: 1.2211,fake_ins_contras_loss : 8.1456, CL_contra_loss : 0.0276
model 3 is saved
cls traing
unseen=0.3933, seen=0.2669, h=0.3180
[1100/3001] Loss_D: -0.3402 Loss_G: 3.9019, Wasserstein_dist: 2.0266, real_ins_contras_loss: 1.2764,fake_ins_contras_loss : 8.1203, CL_contra_loss : 0.0249
[1200/3001] Loss_D: -0.4661 Loss_G: 4.1003, Wasserstein_dist: 2.0729, real_ins_contras_loss: 1.2225,fake_ins_contras_loss : 8.1997, CL_contra_loss : 0.0267
model 3 is saved
cls traing
unseen=0.3836, seen=0.2789, h=0.3230
[1300/3001] Loss_D: -0.4056 Loss_G: 4.0105, Wasserstein_dist: 2.0841, real_ins_contras_loss: 1.2785,fake_ins_contras_loss : 8.2856, CL_contra_loss : 0.0247
[1400/3001] Loss_D: -0.4424 Loss_G: 4.1800, Wasserstein_dist: 2.1265, real_ins_contras_loss: 1.2591,fake_ins_contras_loss : 8.1999, CL_contra_loss : 0.0269
model 3 is saved
cls traing
unseen=0.3995, seen=0.2650, h=0.3186
kl_loss G is 0.005105981603264809
kl_loss G is 0.005146493669599295
kl_loss G is 0.005149373784661293
kl_loss G is 0.005206140223890543
[1500/3001] Loss_D: -0.4003 Loss_G: 4.0046, Wasserstein_dist: 2.1399, real_ins_contras_loss: 1.3121,fake_ins_contras_loss : 8.2669, CL_contra_loss : 0.0247
[1600/3001] Loss_D: -0.5108 Loss_G: 4.0764, Wasserstein_dist: 2.1562, real_ins_contras_loss: 1.2427,fake_ins_contras_loss : 8.1542, CL_contra_loss : 0.0244
model 3 is saved
cls traing
unseen=0.3783, seen=0.2801, h=0.3219
[1700/3001] Loss_D: -0.4765 Loss_G: 4.2290, Wasserstein_dist: 2.0893, real_ins_contras_loss: 1.2305,fake_ins_contras_loss : 8.1543, CL_contra_loss : 0.0266
[1800/3001] Loss_D: -0.5266 Loss_G: 4.3545, Wasserstein_dist: 2.1798, real_ins_contras_loss: 1.2515,fake_ins_contras_loss : 8.2608, CL_contra_loss : 0.0259
model 3 is saved
cls traing
unseen=0.4056, seen=0.2634, h=0.3194
[1900/3001] Loss_D: -0.5186 Loss_G: 4.3071, Wasserstein_dist: 2.1594, real_ins_contras_loss: 1.2280,fake_ins_contras_loss : 8.2157, CL_contra_loss : 0.0282
kl_loss G is 0.00513651967048645
kl_loss G is 0.005220877006649971
kl_loss G is 0.005294628441333771
kl_loss G is 0.005255738273262978
[2000/3001] Loss_D: -0.5030 Loss_G: 4.3116, Wasserstein_dist: 2.1484, real_ins_contras_loss: 1.2524,fake_ins_contras_loss : 8.3147, CL_contra_loss : 0.0270
model 3 is saved
cls traing
unseen=0.3748, seen=0.2839, h=0.3231
[2100/3001] Loss_D: -0.4804 Loss_G: 4.3982, Wasserstein_dist: 2.1750, real_ins_contras_loss: 1.2808,fake_ins_contras_loss : 8.0961, CL_contra_loss : 0.0251
[2200/3001] Loss_D: -0.4920 Loss_G: 4.3273, Wasserstein_dist: 2.1614, real_ins_contras_loss: 1.2596,fake_ins_contras_loss : 8.2363, CL_contra_loss : 0.0264
model 3 is saved
cls traing
unseen=0.3783, seen=0.2820, h=0.3231
[2300/3001] Loss_D: -0.5047 Loss_G: 4.3798, Wasserstein_dist: 2.1766, real_ins_contras_loss: 1.2438,fake_ins_contras_loss : 8.1890, CL_contra_loss : 0.0250
[2400/3001] Loss_D: -0.4458 Loss_G: 4.4785, Wasserstein_dist: 2.1084, real_ins_contras_loss: 1.2386,fake_ins_contras_loss : 8.2384, CL_contra_loss : 0.0266
model 3 is saved
cls traing
unseen=0.3765, seen=0.2890, h=0.3270
kl_loss G is 0.005210991948843002
kl_loss G is 0.005246701650321484
kl_loss G is 0.005235676653683186
kl_loss G is 0.005199355073273182
[2500/3001] Loss_D: -0.5304 Loss_G: 4.5770, Wasserstein_dist: 2.1670, real_ins_contras_loss: 1.2317,fake_ins_contras_loss : 8.2567, CL_contra_loss : 0.0277
[2600/3001] Loss_D: -0.5219 Loss_G: 4.4820, Wasserstein_dist: 2.1757, real_ins_contras_loss: 1.2449,fake_ins_contras_loss : 8.1258, CL_contra_loss : 0.0246
model 3 is saved
cls traing
unseen=0.3915, seen=0.2758, h=0.3236
[2700/3001] Loss_D: -0.5126 Loss_G: 4.4835, Wasserstein_dist: 2.1627, real_ins_contras_loss: 1.2265,fake_ins_contras_loss : 8.2692, CL_contra_loss : 0.0270
[2800/3001] Loss_D: -0.5596 Loss_G: 4.4864, Wasserstein_dist: 2.1846, real_ins_contras_loss: 1.2319,fake_ins_contras_loss : 8.2914, CL_contra_loss : 0.0259
model 3 is saved
cls traing
unseen=0.3871, seen=0.2805, h=0.3253
[2900/3001] Loss_D: -0.4858 Loss_G: 4.4401, Wasserstein_dist: 2.1627, real_ins_contras_loss: 1.2758,fake_ins_contras_loss : 8.2269, CL_contra_loss : 0.0255
kl_loss G is 0.005367273464798927
kl_loss G is 0.005272781476378441
kl_loss G is 0.005401508882641792
kl_loss G is 0.0053516412153840065
[3000/3001] Loss_D: -0.5340 Loss_G: 4.5594, Wasserstein_dist: 2.1248, real_ins_contras_loss: 1.2062,fake_ins_contras_loss : 8.2776, CL_contra_loss : 0.0273
model 3 is saved
cls traing
unseen=0.3677, seen=0.2855, h=0.3214
average_acc is 0.31939983695814805
 hightest s is 0.28897485493230174, u is 0.37654320987654316, H is 0.32699794401845717,
----------------------------------------
loading task 4th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698]
current seen_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 502, 400, 8, 449, 427, 50, 466, 309, 231, 356, 422, 67, 518, 167, 497, 663, 688, 683, 642, 9, 51, 141, 16, 613, 574, 412, 701, 15, 322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557, 66, 250, 655, 174, 81, 264, 457, 194, 256, 537, 2, 395, 411, 91, 121, 357, 467, 415, 106, 275, 17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595, 527, 552, 434, 155, 201, 30, 164, 204, 607, 596, 181, 501, 630, 618, 188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514, 432, 426, 503, 219, 387, 69, 563, 76, 4, 684, 44, 149, 77, 1, 586, 675, 414, 146, 627, 153, 276, 54, 212, 463, 405, 556, 11, 713, 373, 226, 398, 93, 523, 611, 102, 659, 608, 233, 203, 168, 288, 305, 280, 78, 539, 317, 377, 83, 122, 283, 217, 421, 185, 629, 639, 313, 499, 651, 533, 653, 284, 433, 208, 458, 5, 115, 82, 344, 248, 191, 617, 626, 251, 662, 549, 397, 505, 45, 235, 73, 389, 616, 133, 550, 187, 88, 303, 293, 176, 385, 492, 591, 52, 175, 649, 169, 672, 98, 582, 564, 290, 37, 113, 689, 229, 172, 578, 333, 661, 171, 19, 140, 668, 491, 97, 338, 390, 625, 513, 335, 410, 665, 429, 446, 538, 261, 159, 304, 507, 671, 402, 606, 486, 500, 403, 368, 453, 510, 694, 465, 570, 197, 128, 464, 535, 346, 372, 702, 331, 21, 515, 487, 528, 364, 257, 301, 481, 39, 669, 107, 244, 488, 408, 35, 143, 483, 452, 430, 359, 599, 80, 399, 396, 135, 343, 239, 318, 300, 339, 170, 699, 271, 347, 404, 709, 571, 247, 636, 20, 542, 678, 242, 553, 26, 691, 716, 544, 186, 42, 267, 182, 193, 413, 569, 129, 697, 407, 687, 71, 210, 205, 277, 714, 56, 624, 706, 677, 531, 388, 255, 512, 579, 605, 548, 273, 310, 576, 588, 524, 96, 7, 120, 202, 444, 634, 623, 406, 215, 183, 240, 602, 394, 13, 637, 294, 589, 105, 137, 218, 327, 680, 58, 454, 536, 479, 632, 337, 366, 391, 468, 451, 437, 532, 370, 312, 473, 62, 600, 526, 480, 384, 447, 79, 704, 646, 376, 462, 27, 522, 110, 474, 123, 431, 59, 311, 504, 597, 708]
curr_unseen_label is [152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246, 674, 53, 580, 358, 75, 710, 681, 85, 196, 631, 72, 560, 184, 622, 259, 529, 74, 448, 103, 38, 645, 298, 24, 57, 711, 3, 379, 216, 635, 695, 145, 656, 679, 124, 315, 254, 658, 509, 517, 353, 158, 423, 342, 328, 381, 482, 112, 336, 493, 245, 10, 221, 95, 558]
pre_label is [609, 31, 401, 603, 65, 230, 94, 545, 584, 190, 25, 601, 363, 139, 232, 199, 566, 154, 461, 22, 365, 29, 484, 516, 63, 41, 64, 375, 214, 495, 236, 180, 104, 177, 0, 612, 285, 703, 111, 302, 525, 200, 382, 228, 61, 496, 568, 291, 393, 436, 272, 577, 386, 131, 620, 47, 243, 55, 700, 559, 207, 108, 354, 628, 289, 86, 581, 349, 329, 116, 292, 485, 192, 320, 144, 209, 117, 150, 696, 647, 134, 460, 498, 206, 274, 494, 279, 178, 43, 442, 238, 60, 114, 220, 142, 654, 132, 644, 540, 18, 84, 316, 562, 378, 690, 416, 179, 49, 561, 40, 126, 682, 325, 610, 419, 87, 48, 670, 287, 698, 189, 456, 705, 470, 409, 361, 268, 350, 371, 643, 424, 374, 573, 324, 551, 101, 165, 435, 281, 297, 163, 475, 282, 490, 587, 534, 33, 345, 46, 127, 455, 258, 554, 567, 70, 340, 547, 6, 12, 249, 530, 592, 392, 334, 615, 686, 213, 253, 266, 109, 450, 477, 641, 234, 565, 543, 307, 575, 380, 541, 319, 224, 119, 590, 657, 341, 362, 469, 160, 685, 326, 162, 598, 472, 225, 667, 519, 14, 118, 417, 511, 707, 278, 173, 439, 263, 428, 633, 676, 478, 269, 28, 89, 227, 585, 506, 355, 520, 546, 489, 270, 638, 621, 438, 692, 314, 673, 330, 68, 348, 521, 161, 648, 156, 299, 90, 157, 100, 555, 664, 136, 222, 619, 92, 693, 306, 34, 572, 323, 265, 166, 367, 296, 151, 360, 36, 125, 445, 604, 152, 237, 508, 471, 715, 130, 650, 32, 262, 425, 420, 138, 712, 286, 23, 99, 440, 246, 502, 400, 8, 449, 427, 50, 466, 309, 231, 356, 422, 67, 518, 167, 497, 663, 688, 683, 642, 9, 51, 141, 16, 613, 574, 412, 701, 15, 322, 476, 369, 198, 418, 614, 332, 223, 352, 594, 557, 66, 250, 655, 174, 81, 264, 457, 194, 256, 537, 2, 395, 411, 91, 121, 357, 467, 415, 106, 275, 17, 321, 241, 252, 443, 660, 147, 666, 211, 260, 595, 527, 552, 434, 155, 201, 30, 164, 204, 607, 596, 181, 501, 630, 618, 188, 459, 383, 351, 195, 583, 640, 295, 652, 308, 148, 593, 441, 514, 432, 426, 503, 219, 387, 69, 563, 76, 4, 684, 44, 149, 77, 1, 586, 675, 414, 146, 627, 153, 276, 54, 212, 463, 405, 556, 11, 713, 373, 226, 398, 674, 53, 580, 358, 75, 710, 681, 85, 196, 631, 72, 560, 184, 622, 259, 529, 74, 448, 93, 523, 611, 102, 659, 608, 233, 203, 168, 288, 305, 280, 78, 539, 317, 377, 83, 122, 283, 217, 421, 185, 629, 639, 313, 499, 651, 533, 653, 284, 433, 208, 458, 5, 115, 82, 344, 248, 191, 617, 626, 251, 662, 549, 397, 505, 45, 235, 73, 389, 616, 133, 550, 187, 88, 303, 293, 176, 385, 492, 591, 52, 175, 649, 169, 672, 98, 582, 564, 290, 37, 113, 689, 229, 172, 578, 333, 661, 171, 19, 140, 668, 491, 97, 338, 390, 625, 513, 335, 410, 665, 429, 446, 538, 261, 159, 304, 507, 671, 402, 606, 486, 500, 403, 368, 453, 510, 694, 465, 570, 197, 128, 464, 535, 346, 372, 702, 331, 21, 515, 487, 528, 364, 257, 301, 481, 39, 669, 107, 103, 38, 645, 298, 24, 57, 711, 3, 379, 216, 635, 695, 145, 656, 679, 124, 315, 254, 244, 488, 408, 35, 143, 483, 452, 430, 359, 599, 80, 399, 396, 135, 343, 239, 318, 300, 339, 170, 699, 271, 347, 404, 709, 571, 247, 636, 20, 542, 678, 242, 553, 26, 691, 716, 544, 186, 42, 267, 182, 193, 413, 569, 129, 697, 407, 687, 71, 210, 205, 277, 714, 56, 624, 706, 677, 531, 388, 255, 512, 579, 605, 548, 273, 310, 576, 588, 524, 96, 7, 120, 202, 444, 634, 623, 406, 215, 183, 240, 602, 394, 13, 637, 294, 589, 105, 137, 218, 327, 680, 58, 454, 536, 479, 632, 337, 366, 391, 468, 451, 437, 532, 370, 312, 473, 62, 600, 526, 480, 384, 447, 79, 704, 646, 376, 462, 27, 522, 110, 474, 123, 431, 59, 311, 504, 597, 708, 658, 509, 517, 353, 158, 423, 342, 328, 381, 482, 112, 336, 493, 245, 10, 221, 95, 558]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716]
incremental training task 4···
loading task 4 data and the pre model
model 3 is loaded
kl_loss G is 0.0
kl_loss G is 0.0028392490930855274
kl_loss G is 0.0015829852782189846
kl_loss G is 0.001482290681451559
[0/3001] Loss_D: 4.6150 Loss_G: 4.6247, Wasserstein_dist: 0.5381, real_ins_contras_loss: 4.1339,fake_ins_contras_loss : 8.4660, CL_contra_loss : 0.0930
model 4 is saved
cls traing
unseen=0.3644, seen=0.2360, h=0.2865
[100/3001] Loss_D: 0.2073 Loss_G: 3.6438, Wasserstein_dist: 1.5374, real_ins_contras_loss: 1.3079,fake_ins_contras_loss : 8.0835, CL_contra_loss : 0.0355
[200/3001] Loss_D: 0.0234 Loss_G: 3.6565, Wasserstein_dist: 1.6636, real_ins_contras_loss: 1.2942,fake_ins_contras_loss : 8.0903, CL_contra_loss : 0.0296
model 4 is saved
cls traing
unseen=0.3657, seen=0.2391, h=0.2891
[300/3001] Loss_D: -0.1313 Loss_G: 3.7696, Wasserstein_dist: 1.7872, real_ins_contras_loss: 1.2720,fake_ins_contras_loss : 8.1234, CL_contra_loss : 0.0280
[400/3001] Loss_D: -0.2340 Loss_G: 3.7285, Wasserstein_dist: 1.8587, real_ins_contras_loss: 1.2443,fake_ins_contras_loss : 8.1013, CL_contra_loss : 0.0271
model 4 is saved
cls traing
unseen=0.3538, seen=0.2453, h=0.2897
kl_loss G is 0.004013848025351763
kl_loss G is 0.004143748432397842
kl_loss G is 0.004041492473334074
kl_loss G is 0.004085068590939045
[500/3001] Loss_D: -0.2327 Loss_G: 3.9018, Wasserstein_dist: 1.8563, real_ins_contras_loss: 1.2423,fake_ins_contras_loss : 8.1865, CL_contra_loss : 0.0283
[600/3001] Loss_D: -0.3120 Loss_G: 4.0925, Wasserstein_dist: 1.9314, real_ins_contras_loss: 1.2535,fake_ins_contras_loss : 8.1174, CL_contra_loss : 0.0265
model 4 is saved
cls traing
unseen=0.3677, seen=0.2447, h=0.2938
[700/3001] Loss_D: -0.3160 Loss_G: 4.2174, Wasserstein_dist: 1.9941, real_ins_contras_loss: 1.2937,fake_ins_contras_loss : 8.1225, CL_contra_loss : 0.0262
[800/3001] Loss_D: -0.3468 Loss_G: 4.1781, Wasserstein_dist: 2.0346, real_ins_contras_loss: 1.2804,fake_ins_contras_loss : 8.1429, CL_contra_loss : 0.0253
model 4 is saved
cls traing
unseen=0.3558, seen=0.2561, h=0.2979
[900/3001] Loss_D: -0.3541 Loss_G: 4.4616, Wasserstein_dist: 2.0270, real_ins_contras_loss: 1.2906,fake_ins_contras_loss : 8.1274, CL_contra_loss : 0.0267
kl_loss G is 0.004645716864615679
kl_loss G is 0.0047207726165652275
kl_loss G is 0.004616839811205864
kl_loss G is 0.004622628912329674
[1000/3001] Loss_D: -0.3672 Loss_G: 4.4133, Wasserstein_dist: 2.0806, real_ins_contras_loss: 1.2895,fake_ins_contras_loss : 8.0981, CL_contra_loss : 0.0251
model 4 is saved
cls traing
unseen=0.3452, seen=0.2620, h=0.2979
[1100/3001] Loss_D: -0.3857 Loss_G: 4.6022, Wasserstein_dist: 2.0796, real_ins_contras_loss: 1.2881,fake_ins_contras_loss : 8.1752, CL_contra_loss : 0.0251
[1200/3001] Loss_D: -0.3955 Loss_G: 4.7886, Wasserstein_dist: 2.0398, real_ins_contras_loss: 1.2456,fake_ins_contras_loss : 8.1786, CL_contra_loss : 0.0253
model 4 is saved
cls traing
unseen=0.3585, seen=0.2589, h=0.3007
[1300/3001] Loss_D: -0.4512 Loss_G: 4.6933, Wasserstein_dist: 2.0703, real_ins_contras_loss: 1.2241,fake_ins_contras_loss : 8.2068, CL_contra_loss : 0.0245
[1400/3001] Loss_D: -0.4126 Loss_G: 4.7139, Wasserstein_dist: 2.0675, real_ins_contras_loss: 1.2650,fake_ins_contras_loss : 8.1820, CL_contra_loss : 0.0247
model 4 is saved
cls traing
unseen=0.3492, seen=0.2617, h=0.2992
kl_loss G is 0.0047670407220721245
kl_loss G is 0.00472800899296999
kl_loss G is 0.004858864471316338
kl_loss G is 0.004778506699949503
[1500/3001] Loss_D: -0.4279 Loss_G: 4.8157, Wasserstein_dist: 2.1072, real_ins_contras_loss: 1.2872,fake_ins_contras_loss : 8.1855, CL_contra_loss : 0.0225
[1600/3001] Loss_D: -0.4364 Loss_G: 4.8844, Wasserstein_dist: 2.0849, real_ins_contras_loss: 1.2623,fake_ins_contras_loss : 8.1394, CL_contra_loss : 0.0260
model 4 is saved
cls traing
unseen=0.3724, seen=0.2459, h=0.2962
[1700/3001] Loss_D: -0.4452 Loss_G: 4.8464, Wasserstein_dist: 2.1410, real_ins_contras_loss: 1.3085,fake_ins_contras_loss : 8.1560, CL_contra_loss : 0.0250
[1800/3001] Loss_D: -0.4315 Loss_G: 4.9452, Wasserstein_dist: 2.1073, real_ins_contras_loss: 1.2725,fake_ins_contras_loss : 8.2629, CL_contra_loss : 0.0246
model 4 is saved
cls traing
unseen=0.3433, seen=0.2719, h=0.3035
[1900/3001] Loss_D: -0.5061 Loss_G: 4.8819, Wasserstein_dist: 2.1503, real_ins_contras_loss: 1.2616,fake_ins_contras_loss : 8.1832, CL_contra_loss : 0.0222
kl_loss G is 0.00489425053820014
kl_loss G is 0.00486470153555274
kl_loss G is 0.004903607536107302
kl_loss G is 0.004870395176112652
[2000/3001] Loss_D: -0.4660 Loss_G: 4.8952, Wasserstein_dist: 2.1136, real_ins_contras_loss: 1.2467,fake_ins_contras_loss : 8.2413, CL_contra_loss : 0.0244
model 4 is saved
cls traing
unseen=0.3690, seen=0.2490, h=0.2974
[2100/3001] Loss_D: -0.5415 Loss_G: 4.9320, Wasserstein_dist: 2.1433, real_ins_contras_loss: 1.2256,fake_ins_contras_loss : 8.2339, CL_contra_loss : 0.0271
[2200/3001] Loss_D: -0.4591 Loss_G: 5.0437, Wasserstein_dist: 2.1217, real_ins_contras_loss: 1.2614,fake_ins_contras_loss : 8.3071, CL_contra_loss : 0.0253
model 4 is saved
cls traing
unseen=0.3644, seen=0.2530, h=0.2987
[2300/3001] Loss_D: -0.4657 Loss_G: 4.8566, Wasserstein_dist: 2.1177, real_ins_contras_loss: 1.2531,fake_ins_contras_loss : 8.1610, CL_contra_loss : 0.0237
[2400/3001] Loss_D: -0.4959 Loss_G: 5.0316, Wasserstein_dist: 2.0945, real_ins_contras_loss: 1.2258,fake_ins_contras_loss : 8.1987, CL_contra_loss : 0.0243
model 4 is saved
cls traing
unseen=0.3565, seen=0.2558, h=0.2979
kl_loss G is 0.005008884239941835
kl_loss G is 0.004991522990167141
kl_loss G is 0.0048721833154559135
kl_loss G is 0.004910041578114033
[2500/3001] Loss_D: -0.4882 Loss_G: 5.1703, Wasserstein_dist: 2.1428, real_ins_contras_loss: 1.2484,fake_ins_contras_loss : 8.1582, CL_contra_loss : 0.0238
[2600/3001] Loss_D: -0.4561 Loss_G: 5.0807, Wasserstein_dist: 2.1313, real_ins_contras_loss: 1.2603,fake_ins_contras_loss : 8.2341, CL_contra_loss : 0.0255
model 4 is saved
cls traing
unseen=0.3710, seen=0.2561, h=0.3031
[2700/3001] Loss_D: -0.5820 Loss_G: 5.1656, Wasserstein_dist: 2.1557, real_ins_contras_loss: 1.1953,fake_ins_contras_loss : 8.1867, CL_contra_loss : 0.0251
[2800/3001] Loss_D: -0.5008 Loss_G: 5.0408, Wasserstein_dist: 2.1439, real_ins_contras_loss: 1.2553,fake_ins_contras_loss : 8.1753, CL_contra_loss : 0.0255
model 4 is saved
cls traing
unseen=0.3624, seen=0.2574, h=0.3010
[2900/3001] Loss_D: -0.4811 Loss_G: 4.9956, Wasserstein_dist: 2.1404, real_ins_contras_loss: 1.2758,fake_ins_contras_loss : 8.2567, CL_contra_loss : 0.0234
kl_loss G is 0.004956771619617939
kl_loss G is 0.0048994324170053005
kl_loss G is 0.004976894706487656
kl_loss G is 0.005021760240197182
[3000/3001] Loss_D: -0.4969 Loss_G: 5.0638, Wasserstein_dist: 2.1452, real_ins_contras_loss: 1.2396,fake_ins_contras_loss : 8.1301, CL_contra_loss : 0.0244
model 4 is saved
cls traing
unseen=0.3638, seen=0.2623, h=0.3048
average_acc is 0.29804871886079626
 hightest s is 0.262325581395349, u is 0.36375661375661356, H is 0.3048247208721407,
----------------------------------------
loading task 5th data
Traceback (most recent call last):
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 372, in <module>
    train_one_task(premodel = pregan)
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 348, in train_one_task
    data.current_class_index()
  File "E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3\utils\data_pretrain.py", line 270, in current_class_index
    current_data = self.splited_seen_class[self.current_taskid].unsqueeze(dim=1)
IndexError: list index out of range

Process finished with exit code 1




F:\anaconda3_new\python.exe "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py"
Namespace(attSize=102, batch_size=512, beta=60, beta1=0.5, beta_1=10, class_embedding='att', classifier_lr=0.001, critic_iter=5, cuda=True, dataroot='data', dataset='SUN', dir='models\\SUN', distill_proj_hidden_dim=4096, embedSize=2048, epochs=100, image_embedding='res101', ins_temp=0.1, ins_weight=0.001, lambda1=10, lr=0.0001, matdataset=True, nclass_all=717, nclass_seen=645, ndh=4096, neh=4096, nepoch=3001, ngh=4096, nz=102, outzSize=64, preprocessing=True, pretrain_class_number=130, pretrain_gan=True, recons_weight=0.001, resSize=2048, shuffer_class=True, standardization=False, syn_num=256, syn_num_rp=50, syn_num_s=60, syn_num_u=100, task_num=4, validation=False)
data/SUN/res101.mat E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3
att: torch.Size([717, 102])
splited_seen_class is [tensor([167, 519, 220, 170, 300, 500, 705, 288, 140,   4, 102, 518, 310, 675,
        240, 274,  19, 459, 682, 591, 441, 223, 338,  12, 533, 155, 400, 354,
         62, 434, 621, 618, 452, 555, 296, 249,  49, 457, 535, 489,  56, 642,
        343, 191, 256, 664, 295, 592, 189, 685,   1, 677, 615, 537, 596, 598,
        207, 411,   6,  71, 541,  37, 416, 161, 492, 603, 325, 222, 676, 582,
        108, 657, 382,  82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589,
        464, 174, 131,  28, 405,  64, 527, 599, 386, 687, 244, 250, 584, 424,
        334, 323,  44, 453, 704,  18, 113,   0, 235, 110, 239, 515, 483, 654,
        634, 355,  48, 412,  21, 613, 311, 648, 313,  81, 526, 590, 610, 309,
        456, 588, 544]), tensor([601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417,
        577, 281, 132,  73, 123,  94, 548, 609, 617, 180, 305, 512, 394, 393,
         22, 701, 605, 390, 376,  15, 347, 532, 458,  88, 643, 333, 477,  61,
        219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523,  17, 714, 186,
        689, 133,  50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655,
        520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593,
        561, 248, 665, 232, 301, 447,  16, 502, 646, 528, 688, 120, 205, 426,
        568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422,
        151,  51, 708, 165, 419, 214, 168, 686, 147, 209,  41,  83,  29, 553,
        188, 114, 202]), tensor([566, 573, 652, 473, 608,  40, 565, 257, 375, 227,  80,  34,  93, 269,
         54, 263, 299,  31,  43, 451, 234, 172, 335, 363, 551, 443, 125, 218,
        550, 620, 279, 247, 157, 466, 669, 380, 649, 178, 581, 692, 572, 436,
         87,   7, 368, 276, 662, 633, 597, 461, 360, 241, 327,  76, 199, 193,
        491,  25, 303, 703, 408,  11, 616, 623, 432,  68, 362, 332,  90, 225,
          9, 552, 330, 384,  26, 569, 549, 600, 586, 534, 653, 297, 510, 197,
        401, 316, 709, 389, 639, 628, 402, 684, 149,  14, 242,  69, 252,  66,
         67, 127, 511, 264, 307, 364, 271, 578, 435, 198, 680, 539, 487, 554,
        678, 559, 280, 369, 707, 524, 228, 261, 359, 304, 146, 437, 370, 449,
        287, 122, 516]), tensor([346, 341, 637, 647, 410,  47,  39, 253, 538, 361, 404, 267, 521, 294,
        713, 612, 607,   5, 121, 331, 666, 395, 556, 352, 171, 277, 278, 663,
        636, 378, 150, 289, 211,  86, 374, 195, 317, 396, 366, 562, 693, 268,
        324, 460, 258,  13, 314, 463, 292, 564, 606, 706, 632, 229, 696, 495,
        201,  60, 106, 318, 356, 391, 467, 166, 144, 118, 321, 126, 691, 111,
         84, 660,  89, 644, 506, 431,   2, 101,  97, 141, 429, 501, 406, 340,
        372, 272, 176, 450, 104, 661, 236, 233, 481, 468, 200,  42, 156, 673,
         78, 128, 251, 351, 119,  45,  52, 485, 439, 415,  59, 522, 159, 407,
        630,   8, 282, 392, 115, 503, 385,  30, 570, 454, 594,  20, 571, 143,
        667,  36])]
splited_unseen_class is [tensor([ 23, 656, 508, 712, 695,  85, 196, 145, 681, 103, 124, 254, 631, 679,
        328, 353,  10, 529]), tensor([645,  32, 112, 358, 237, 286,  95, 298, 342, 379, 622, 138,  75, 184,
        493, 381, 482, 245]), tensor([262, 715, 471, 221, 711, 509, 710, 674, 158, 336,  99,  24, 216, 425,
        650,  53,  38, 440]), tensor([517,  72, 315, 420,   3, 246, 259, 152,  57, 658, 423, 448,  74, 558,
        580, 560, 130, 635])]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
pre_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604]
curr_seen_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604]
**************************************************
pretraining cgan model ···
[0/3001] Loss_D: 3.0304 Loss_G: -0.7289, Wasserstein_dist: 1.5122, real_ins_contras_loss: 4.4261,fake_ins_contras_loss : 7.4048
[50/3001] Loss_D: -0.6585 Loss_G: 0.7148, Wasserstein_dist: 2.0618, real_ins_contras_loss: 1.2809,fake_ins_contras_loss : 7.5486
[100/3001] Loss_D: 1.2144 Loss_G: -2.0036, Wasserstein_dist: 0.0306, real_ins_contras_loss: 1.2200,fake_ins_contras_loss : 7.6320
[150/3001] Loss_D: 0.9081 Loss_G: -0.8151, Wasserstein_dist: 0.3612, real_ins_contras_loss: 1.2550,fake_ins_contras_loss : 7.1869
model 0 is saved
cls traing
Training classifier loss= 3.5094
seen=0.0446
[200/3001] Loss_D: 0.6131 Loss_G: -0.5471, Wasserstein_dist: 0.5782, real_ins_contras_loss: 1.1785,fake_ins_contras_loss : 7.3670
[250/3001] Loss_D: 0.4970 Loss_G: -0.2856, Wasserstein_dist: 0.7206, real_ins_contras_loss: 1.2018,fake_ins_contras_loss : 7.3187
[300/3001] Loss_D: 0.5223 Loss_G: -0.4946, Wasserstein_dist: 0.6788, real_ins_contras_loss: 1.1798,fake_ins_contras_loss : 7.1211
[350/3001] Loss_D: 0.5440 Loss_G: -0.6667, Wasserstein_dist: 0.7265, real_ins_contras_loss: 1.2389,fake_ins_contras_loss : 6.9366
model 0 is saved
cls traing
Training classifier loss= 2.3182
seen=0.2231
[400/3001] Loss_D: 0.4978 Loss_G: -0.7011, Wasserstein_dist: 0.7327, real_ins_contras_loss: 1.1891,fake_ins_contras_loss : 6.7354
[450/3001] Loss_D: 0.5827 Loss_G: -0.7853, Wasserstein_dist: 0.6971, real_ins_contras_loss: 1.2486,fake_ins_contras_loss : 6.2476
[500/3001] Loss_D: 0.5784 Loss_G: -1.0044, Wasserstein_dist: 0.6994, real_ins_contras_loss: 1.2501,fake_ins_contras_loss : 6.0691
[550/3001] Loss_D: 0.5148 Loss_G: -0.9281, Wasserstein_dist: 0.7395, real_ins_contras_loss: 1.2264,fake_ins_contras_loss : 6.0041
model 0 is saved
cls traing
Training classifier loss= 1.6189
seen=0.3185
[600/3001] Loss_D: 0.4375 Loss_G: -0.7710, Wasserstein_dist: 0.7797, real_ins_contras_loss: 1.1928,fake_ins_contras_loss : 5.8849
[650/3001] Loss_D: 0.4872 Loss_G: -0.8980, Wasserstein_dist: 0.8179, real_ins_contras_loss: 1.2761,fake_ins_contras_loss : 5.6737
[700/3001] Loss_D: 0.4114 Loss_G: -0.7933, Wasserstein_dist: 0.8685, real_ins_contras_loss: 1.2448,fake_ins_contras_loss : 5.6856
[750/3001] Loss_D: 0.3671 Loss_G: -0.7160, Wasserstein_dist: 0.8915, real_ins_contras_loss: 1.2166,fake_ins_contras_loss : 5.3976
model 0 is saved
cls traing
Training classifier loss= 1.2315
seen=0.3892
[800/3001] Loss_D: 0.3004 Loss_G: -0.4432, Wasserstein_dist: 0.9968, real_ins_contras_loss: 1.2465,fake_ins_contras_loss : 5.2205
[850/3001] Loss_D: 0.2940 Loss_G: -0.5614, Wasserstein_dist: 0.9788, real_ins_contras_loss: 1.2245,fake_ins_contras_loss : 5.1042
[900/3001] Loss_D: 0.2520 Loss_G: -0.6344, Wasserstein_dist: 1.0416, real_ins_contras_loss: 1.2422,fake_ins_contras_loss : 4.8255
[950/3001] Loss_D: 0.2578 Loss_G: -0.5228, Wasserstein_dist: 1.0302, real_ins_contras_loss: 1.2500,fake_ins_contras_loss : 4.7916
model 0 is saved
cls traing
Training classifier loss= 0.9344
seen=0.4585
[1000/3001] Loss_D: 0.1754 Loss_G: -0.6136, Wasserstein_dist: 1.0758, real_ins_contras_loss: 1.2081,fake_ins_contras_loss : 4.5280
[1050/3001] Loss_D: 0.1825 Loss_G: -0.5121, Wasserstein_dist: 1.1059, real_ins_contras_loss: 1.2384,fake_ins_contras_loss : 3.6362
[1100/3001] Loss_D: 0.1692 Loss_G: -0.4698, Wasserstein_dist: 1.1016, real_ins_contras_loss: 1.2334,fake_ins_contras_loss : 3.8134
[1150/3001] Loss_D: 0.1300 Loss_G: -0.5338, Wasserstein_dist: 1.1732, real_ins_contras_loss: 1.2567,fake_ins_contras_loss : 3.8026
model 0 is saved
cls traing
Training classifier loss= 0.8472
seen=0.4892
[1200/3001] Loss_D: 0.0973 Loss_G: -0.4437, Wasserstein_dist: 1.1779, real_ins_contras_loss: 1.2295,fake_ins_contras_loss : 3.7417
[1250/3001] Loss_D: 0.0391 Loss_G: -0.3235, Wasserstein_dist: 1.2431, real_ins_contras_loss: 1.2175,fake_ins_contras_loss : 3.6516
[1300/3001] Loss_D: -0.0165 Loss_G: -0.2991, Wasserstein_dist: 1.2458, real_ins_contras_loss: 1.1810,fake_ins_contras_loss : 3.4851
[1350/3001] Loss_D: 0.0981 Loss_G: -0.3246, Wasserstein_dist: 1.2023, real_ins_contras_loss: 1.2525,fake_ins_contras_loss : 3.4876
model 0 is saved
cls traing
Training classifier loss= 0.6455
seen=0.4985
[1400/3001] Loss_D: 0.0198 Loss_G: -0.2847, Wasserstein_dist: 1.3205, real_ins_contras_loss: 1.2310,fake_ins_contras_loss : 3.2982
[1450/3001] Loss_D: -0.0326 Loss_G: -0.2098, Wasserstein_dist: 1.3101, real_ins_contras_loss: 1.2138,fake_ins_contras_loss : 3.2784
[1500/3001] Loss_D: 0.0270 Loss_G: -0.1192, Wasserstein_dist: 1.2768, real_ins_contras_loss: 1.2284,fake_ins_contras_loss : 3.1566
[1550/3001] Loss_D: 0.0324 Loss_G: -0.4337, Wasserstein_dist: 1.2363, real_ins_contras_loss: 1.1857,fake_ins_contras_loss : 3.1449
model 0 is saved
cls traing
Training classifier loss= 0.5968
seen=0.5231
[1600/3001] Loss_D: 0.0487 Loss_G: -0.2360, Wasserstein_dist: 1.2752, real_ins_contras_loss: 1.2083,fake_ins_contras_loss : 3.1972
[1650/3001] Loss_D: 0.1234 Loss_G: -0.4125, Wasserstein_dist: 1.2254, real_ins_contras_loss: 1.2575,fake_ins_contras_loss : 3.1701
[1700/3001] Loss_D: 0.0060 Loss_G: -0.2411, Wasserstein_dist: 1.2739, real_ins_contras_loss: 1.1965,fake_ins_contras_loss : 3.0711
[1750/3001] Loss_D: -0.0263 Loss_G: -0.1401, Wasserstein_dist: 1.3139, real_ins_contras_loss: 1.2042,fake_ins_contras_loss : 2.9842
model 0 is saved
cls traing
Training classifier loss= 0.5090
seen=0.4954
[1800/3001] Loss_D: 0.0082 Loss_G: -0.1649, Wasserstein_dist: 1.2883, real_ins_contras_loss: 1.2141,fake_ins_contras_loss : 2.9113
[1850/3001] Loss_D: -0.1180 Loss_G: -0.1343, Wasserstein_dist: 1.3730, real_ins_contras_loss: 1.1702,fake_ins_contras_loss : 2.8722
[1900/3001] Loss_D: -0.0879 Loss_G: -0.0258, Wasserstein_dist: 1.3463, real_ins_contras_loss: 1.1834,fake_ins_contras_loss : 2.6275
[1950/3001] Loss_D: -0.0684 Loss_G: 0.0186, Wasserstein_dist: 1.3330, real_ins_contras_loss: 1.1926,fake_ins_contras_loss : 2.7336
model 0 is saved
cls traing
Training classifier loss= 0.4428
seen=0.5108
[2000/3001] Loss_D: -0.0672 Loss_G: 0.1228, Wasserstein_dist: 1.3523, real_ins_contras_loss: 1.2063,fake_ins_contras_loss : 2.6993
[2050/3001] Loss_D: -0.0970 Loss_G: 0.0139, Wasserstein_dist: 1.4234, real_ins_contras_loss: 1.2221,fake_ins_contras_loss : 2.6570
[2100/3001] Loss_D: -0.0759 Loss_G: 0.1292, Wasserstein_dist: 1.4011, real_ins_contras_loss: 1.2322,fake_ins_contras_loss : 2.6581
[2150/3001] Loss_D: -0.1645 Loss_G: 0.1115, Wasserstein_dist: 1.4487, real_ins_contras_loss: 1.1910,fake_ins_contras_loss : 2.5760
model 0 is saved
cls traing
Training classifier loss= 0.4380
seen=0.5231
[2200/3001] Loss_D: -0.1675 Loss_G: 0.2421, Wasserstein_dist: 1.5119, real_ins_contras_loss: 1.2413,fake_ins_contras_loss : 2.5630
[2250/3001] Loss_D: -0.1485 Loss_G: 0.1932, Wasserstein_dist: 1.4773, real_ins_contras_loss: 1.2267,fake_ins_contras_loss : 2.6248
[2300/3001] Loss_D: -0.1613 Loss_G: 0.2249, Wasserstein_dist: 1.4956, real_ins_contras_loss: 1.2117,fake_ins_contras_loss : 2.5891
[2350/3001] Loss_D: -0.1228 Loss_G: 0.3958, Wasserstein_dist: 1.4466, real_ins_contras_loss: 1.2287,fake_ins_contras_loss : 2.5754
model 0 is saved
cls traing
Training classifier loss= 0.4496
seen=0.5215
[2400/3001] Loss_D: -0.1577 Loss_G: 0.4507, Wasserstein_dist: 1.5009, real_ins_contras_loss: 1.2231,fake_ins_contras_loss : 2.5424
[2450/3001] Loss_D: -0.2106 Loss_G: 0.4163, Wasserstein_dist: 1.4895, real_ins_contras_loss: 1.1792,fake_ins_contras_loss : 2.4624
[2500/3001] Loss_D: -0.1521 Loss_G: 0.4153, Wasserstein_dist: 1.4714, real_ins_contras_loss: 1.2113,fake_ins_contras_loss : 2.5449
[2550/3001] Loss_D: -0.2296 Loss_G: 0.6035, Wasserstein_dist: 1.5337, real_ins_contras_loss: 1.1978,fake_ins_contras_loss : 2.5074
model 0 is saved
cls traing
Training classifier loss= 0.4649
seen=0.5246
[2600/3001] Loss_D: -0.1887 Loss_G: 0.4564, Wasserstein_dist: 1.5297, real_ins_contras_loss: 1.2175,fake_ins_contras_loss : 2.4702
[2650/3001] Loss_D: -0.1730 Loss_G: 0.5865, Wasserstein_dist: 1.5146, real_ins_contras_loss: 1.2344,fake_ins_contras_loss : 2.5246
[2700/3001] Loss_D: -0.2596 Loss_G: 0.7163, Wasserstein_dist: 1.5754, real_ins_contras_loss: 1.2019,fake_ins_contras_loss : 2.4411
[2750/3001] Loss_D: -0.2558 Loss_G: 0.4474, Wasserstein_dist: 1.6066, real_ins_contras_loss: 1.2179,fake_ins_contras_loss : 2.4371
model 0 is saved
cls traing
Training classifier loss= 0.4565
seen=0.5246
[2800/3001] Loss_D: -0.2323 Loss_G: 0.6002, Wasserstein_dist: 1.5529, real_ins_contras_loss: 1.1915,fake_ins_contras_loss : 2.4258
[2850/3001] Loss_D: -0.2717 Loss_G: 0.6160, Wasserstein_dist: 1.6497, real_ins_contras_loss: 1.2427,fake_ins_contras_loss : 2.4450
[2900/3001] Loss_D: -0.2256 Loss_G: 0.7372, Wasserstein_dist: 1.5586, real_ins_contras_loss: 1.2159,fake_ins_contras_loss : 2.4020
[2950/3001] Loss_D: -0.2004 Loss_G: 0.7426, Wasserstein_dist: 1.5433, real_ins_contras_loss: 1.2100,fake_ins_contras_loss : 2.4224
model 0 is saved
cls traing
Training classifier loss= 0.4471
seen=0.5308
[3000/3001] Loss_D: -0.2720 Loss_G: 0.9951, Wasserstein_dist: 1.6082, real_ins_contras_loss: 1.2089,fake_ins_contras_loss : 2.3627
average_acc is 0.4664835164835163
----------------------------------------
loading task 1th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258]
current seen_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544]
curr_unseen_label is [23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529]
pre_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276]
finish pretraining cgan model ···
incremental training task 1···
loading task 1 data and the pre model
model 0 is loaded
kl_loss G is 0.010063260793685913
kl_loss G is 0.006716923788189888
kl_loss G is 0.004498369991779327
kl_loss G is 0.003306495025753975
[0/3001] Loss_D: 9.4838 Loss_G: 0.5703, Wasserstein_dist: 0.1257, real_ins_contras_loss: 4.4619,fake_ins_contras_loss : 8.2168, CL_contra_loss : 0.4643
model 1 is saved
cls traing
unseen=0.5212, seen=0.2849, h=0.3684
[100/3001] Loss_D: 0.4300 Loss_G: 1.7830, Wasserstein_dist: 1.4292, real_ins_contras_loss: 1.2981,fake_ins_contras_loss : 8.0306, CL_contra_loss : 0.0428
[200/3001] Loss_D: 0.1509 Loss_G: 1.8994, Wasserstein_dist: 1.6426, real_ins_contras_loss: 1.2776,fake_ins_contras_loss : 8.1195, CL_contra_loss : 0.0385
model 1 is saved
cls traing
unseen=0.5079, seen=0.3336, h=0.4027
[300/3001] Loss_D: -0.0419 Loss_G: 2.0111, Wasserstein_dist: 1.7727, real_ins_contras_loss: 1.2513,fake_ins_contras_loss : 8.1064, CL_contra_loss : 0.0342
[400/3001] Loss_D: -0.0656 Loss_G: 2.0986, Wasserstein_dist: 1.8810, real_ins_contras_loss: 1.2867,fake_ins_contras_loss : 8.1321, CL_contra_loss : 0.0351
model 1 is saved
cls traing
unseen=0.4815, seen=0.3583, h=0.4109
kl_loss G is 0.005849894601851702
kl_loss G is 0.005899748299270868
kl_loss G is 0.006087767891585827
kl_loss G is 0.005942745134234428
[500/3001] Loss_D: -0.1597 Loss_G: 1.9213, Wasserstein_dist: 1.9365, real_ins_contras_loss: 1.2726,fake_ins_contras_loss : 8.1891, CL_contra_loss : 0.0364
[600/3001] Loss_D: -0.2519 Loss_G: 1.9252, Wasserstein_dist: 1.9662, real_ins_contras_loss: 1.2413,fake_ins_contras_loss : 8.1985, CL_contra_loss : 0.0305
model 1 is saved
cls traing
unseen=0.5344, seen=0.3405, h=0.4160
[700/3001] Loss_D: -0.2235 Loss_G: 1.8909, Wasserstein_dist: 2.0106, real_ins_contras_loss: 1.2691,fake_ins_contras_loss : 8.2116, CL_contra_loss : 0.0323
[800/3001] Loss_D: -0.2700 Loss_G: 1.8621, Wasserstein_dist: 2.0724, real_ins_contras_loss: 1.2705,fake_ins_contras_loss : 8.2182, CL_contra_loss : 0.0342
model 1 is saved
cls traing
unseen=0.4841, seen=0.3768, h=0.4238
[900/3001] Loss_D: -0.3853 Loss_G: 1.9376, Wasserstein_dist: 2.0852, real_ins_contras_loss: 1.2006,fake_ins_contras_loss : 8.1924, CL_contra_loss : 0.0327
kl_loss G is 0.006593158468604088
kl_loss G is 0.00659015541896224
kl_loss G is 0.006677057594060898
kl_loss G is 0.006642870604991913
[1000/3001] Loss_D: -0.3847 Loss_G: 1.9358, Wasserstein_dist: 2.0711, real_ins_contras_loss: 1.2241,fake_ins_contras_loss : 8.2082, CL_contra_loss : 0.0327
model 1 is saved
cls traing
unseen=0.5344, seen=0.3776, h=0.4425
[1100/3001] Loss_D: -0.3666 Loss_G: 1.9151, Wasserstein_dist: 2.1367, real_ins_contras_loss: 1.2646,fake_ins_contras_loss : 8.2441, CL_contra_loss : 0.0341
[1200/3001] Loss_D: -0.4293 Loss_G: 1.9471, Wasserstein_dist: 2.1394, real_ins_contras_loss: 1.2397,fake_ins_contras_loss : 8.2550, CL_contra_loss : 0.0310
model 1 is saved
cls traing
unseen=0.5370, seen=0.3753, h=0.4418
[1300/3001] Loss_D: -0.4218 Loss_G: 2.1269, Wasserstein_dist: 2.1650, real_ins_contras_loss: 1.2530,fake_ins_contras_loss : 8.2190, CL_contra_loss : 0.0316
[1400/3001] Loss_D: -0.3534 Loss_G: 2.1012, Wasserstein_dist: 2.1227, real_ins_contras_loss: 1.2634,fake_ins_contras_loss : 8.3328, CL_contra_loss : 0.0332
model 1 is saved
cls traing
unseen=0.5344, seen=0.3784, h=0.4431
kl_loss G is 0.006867915857583284
kl_loss G is 0.006718470714986324
kl_loss G is 0.006814025342464447
kl_loss G is 0.006870614364743233
[1500/3001] Loss_D: -0.4416 Loss_G: 1.9909, Wasserstein_dist: 2.1658, real_ins_contras_loss: 1.2747,fake_ins_contras_loss : 8.2581, CL_contra_loss : 0.0293
[1600/3001] Loss_D: -0.4458 Loss_G: 2.0491, Wasserstein_dist: 2.1830, real_ins_contras_loss: 1.2579,fake_ins_contras_loss : 8.3139, CL_contra_loss : 0.0326
model 1 is saved
cls traing
unseen=0.4921, seen=0.4093, h=0.4469
[1700/3001] Loss_D: -0.5343 Loss_G: 2.1171, Wasserstein_dist: 2.1940, real_ins_contras_loss: 1.2170,fake_ins_contras_loss : 8.2680, CL_contra_loss : 0.0321
[1800/3001] Loss_D: -0.5195 Loss_G: 2.1587, Wasserstein_dist: 2.2378, real_ins_contras_loss: 1.2356,fake_ins_contras_loss : 8.2495, CL_contra_loss : 0.0306
model 1 is saved
cls traing
unseen=0.5397, seen=0.3977, h=0.4579
[1900/3001] Loss_D: -0.5311 Loss_G: 2.1117, Wasserstein_dist: 2.2272, real_ins_contras_loss: 1.2253,fake_ins_contras_loss : 8.3354, CL_contra_loss : 0.0330
kl_loss G is 0.007021117024123669
kl_loss G is 0.006717470474541187
kl_loss G is 0.006790193263441324
kl_loss G is 0.006858504377305508
[2000/3001] Loss_D: -0.5503 Loss_G: 2.2477, Wasserstein_dist: 2.2248, real_ins_contras_loss: 1.1930,fake_ins_contras_loss : 8.3694, CL_contra_loss : 0.0348
model 1 is saved
cls traing
unseen=0.5317, seen=0.4054, h=0.4601
[2100/3001] Loss_D: -0.4913 Loss_G: 2.2432, Wasserstein_dist: 2.2146, real_ins_contras_loss: 1.2527,fake_ins_contras_loss : 8.3117, CL_contra_loss : 0.0300
[2200/3001] Loss_D: -0.5380 Loss_G: 2.3279, Wasserstein_dist: 2.2059, real_ins_contras_loss: 1.2340,fake_ins_contras_loss : 8.2340, CL_contra_loss : 0.0305
model 1 is saved
cls traing
unseen=0.5503, seen=0.4039, h=0.4658
[2300/3001] Loss_D: -0.4957 Loss_G: 2.3006, Wasserstein_dist: 2.1972, real_ins_contras_loss: 1.2431,fake_ins_contras_loss : 8.2608, CL_contra_loss : 0.0315
[2400/3001] Loss_D: -0.5051 Loss_G: 2.3026, Wasserstein_dist: 2.2100, real_ins_contras_loss: 1.2554,fake_ins_contras_loss : 8.2576, CL_contra_loss : 0.0285
model 1 is saved
cls traing
unseen=0.5397, seen=0.4100, h=0.4660
kl_loss G is 0.006784600205719471
kl_loss G is 0.006853198632597923
kl_loss G is 0.006802456919103861
kl_loss G is 0.006954242940992117
[2500/3001] Loss_D: -0.4749 Loss_G: 2.2866, Wasserstein_dist: 2.2038, real_ins_contras_loss: 1.2317,fake_ins_contras_loss : 8.3301, CL_contra_loss : 0.0297
[2600/3001] Loss_D: -0.5604 Loss_G: 2.3631, Wasserstein_dist: 2.2479, real_ins_contras_loss: 1.2439,fake_ins_contras_loss : 8.2723, CL_contra_loss : 0.0295
model 1 is saved
cls traing
unseen=0.5503, seen=0.4139, h=0.4724
[2700/3001] Loss_D: -0.5138 Loss_G: 2.4765, Wasserstein_dist: 2.2388, real_ins_contras_loss: 1.2684,fake_ins_contras_loss : 8.3760, CL_contra_loss : 0.0297
[2800/3001] Loss_D: -0.4403 Loss_G: 2.4552, Wasserstein_dist: 2.1967, real_ins_contras_loss: 1.3008,fake_ins_contras_loss : 8.2631, CL_contra_loss : 0.0290
model 1 is saved
cls traing
unseen=0.5185, seen=0.4224, h=0.4655
[2900/3001] Loss_D: -0.5066 Loss_G: 2.4804, Wasserstein_dist: 2.2089, real_ins_contras_loss: 1.2343,fake_ins_contras_loss : 8.2691, CL_contra_loss : 0.0322
kl_loss G is 0.0069168489426374435
kl_loss G is 0.006893821060657501
kl_loss G is 0.00700528547167778
kl_loss G is 0.006776610389351845
[3000/3001] Loss_D: -0.5595 Loss_G: 2.5070, Wasserstein_dist: 2.2471, real_ins_contras_loss: 1.2567,fake_ins_contras_loss : 8.2899, CL_contra_loss : 0.0281
model 1 is saved
cls traing
unseen=0.5503, seen=0.4124, h=0.4714
average_acc is 0.44579020459802854
 hightest s is 0.41389961389961394, u is 0.5502645502645502, H is 0.47243880941080757,
----------------------------------------
loading task 2th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]
current seen_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417, 577, 281, 132, 73, 123, 94, 548, 609, 617, 180, 305, 512, 394, 393, 22, 701, 605, 390, 376, 15, 347, 532, 458, 88, 643, 333, 477, 61, 219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523, 17, 714, 186, 689, 133, 50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655, 520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593, 561, 248, 665, 232, 301, 447, 16, 502, 646, 528, 688, 120, 205, 426, 568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422, 151, 51, 708, 165, 419, 214, 168, 686, 147, 209, 41, 83, 29, 553, 188, 114, 202]
curr_unseen_label is [23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529, 645, 32, 112, 358, 237, 286, 95, 298, 342, 379, 622, 138, 75, 184, 493, 381, 482, 245]
pre_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529, 601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417, 577, 281, 132, 73, 123, 94, 548, 609, 617, 180, 305, 512, 394, 393, 22, 701, 605, 390, 376, 15, 347, 532, 458, 88, 643, 333, 477, 61, 219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523, 17, 714, 186, 689, 133, 50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655, 520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593, 561, 248, 665, 232, 301, 447, 16, 502, 646, 528, 688, 120, 205, 426, 568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422, 151, 51, 708, 165, 419, 214, 168, 686, 147, 209, 41, 83, 29, 553, 188, 114, 202, 645, 32, 112, 358, 237, 286, 95, 298, 342, 379, 622, 138, 75, 184, 493, 381, 482, 245]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]
incremental training task 2···
loading task 2 data and the pre model
model 1 is loaded
kl_loss G is 0.0
kl_loss G is 0.002850578399375081
kl_loss G is 0.0017090574838221073
kl_loss G is 0.0014880045782774687
[0/3001] Loss_D: 4.8266 Loss_G: 2.6977, Wasserstein_dist: 0.7595, real_ins_contras_loss: 4.2071,fake_ins_contras_loss : 8.5476, CL_contra_loss : 0.1255
model 2 is saved
cls traing
unseen=0.4180, seen=0.2825, h=0.3371
[100/3001] Loss_D: 0.1581 Loss_G: 2.0795, Wasserstein_dist: 1.6352, real_ins_contras_loss: 1.3158,fake_ins_contras_loss : 8.1888, CL_contra_loss : 0.0379
[200/3001] Loss_D: 0.0124 Loss_G: 1.8452, Wasserstein_dist: 1.7731, real_ins_contras_loss: 1.3037,fake_ins_contras_loss : 8.0820, CL_contra_loss : 0.0326
model 2 is saved
cls traing
unseen=0.4418, seen=0.2964, h=0.3548
[300/3001] Loss_D: -0.2184 Loss_G: 1.8049, Wasserstein_dist: 1.9060, real_ins_contras_loss: 1.2670,fake_ins_contras_loss : 8.1719, CL_contra_loss : 0.0304
[400/3001] Loss_D: -0.2031 Loss_G: 1.8951, Wasserstein_dist: 1.9172, real_ins_contras_loss: 1.2706,fake_ins_contras_loss : 8.1411, CL_contra_loss : 0.0300
model 2 is saved
cls traing
unseen=0.4087, seen=0.3149, h=0.3558
kl_loss G is 0.004637028090655804
kl_loss G is 0.0046593607403337955
kl_loss G is 0.004607443697750568
kl_loss G is 0.004625404253602028
[500/3001] Loss_D: -0.1893 Loss_G: 2.0036, Wasserstein_dist: 1.9048, real_ins_contras_loss: 1.2860,fake_ins_contras_loss : 8.1243, CL_contra_loss : 0.0314
[600/3001] Loss_D: -0.2553 Loss_G: 2.0923, Wasserstein_dist: 1.9549, real_ins_contras_loss: 1.2529,fake_ins_contras_loss : 8.0514, CL_contra_loss : 0.0305
model 2 is saved
cls traing
unseen=0.4220, seen=0.3088, h=0.3566
[700/3001] Loss_D: -0.3532 Loss_G: 2.2536, Wasserstein_dist: 2.0074, real_ins_contras_loss: 1.2337,fake_ins_contras_loss : 8.1882, CL_contra_loss : 0.0301
[800/3001] Loss_D: -0.3265 Loss_G: 2.3696, Wasserstein_dist: 2.0307, real_ins_contras_loss: 1.2854,fake_ins_contras_loss : 8.1785, CL_contra_loss : 0.0285
model 2 is saved
cls traing
unseen=0.4286, seen=0.3057, h=0.3568
[900/3001] Loss_D: -0.3163 Loss_G: 2.5197, Wasserstein_dist: 2.0028, real_ins_contras_loss: 1.2692,fake_ins_contras_loss : 8.1758, CL_contra_loss : 0.0275
kl_loss G is 0.005142807960510254
kl_loss G is 0.005213161464780569
kl_loss G is 0.00511413998901844
kl_loss G is 0.0051774270832538605
[1000/3001] Loss_D: -0.3734 Loss_G: 2.5895, Wasserstein_dist: 2.0796, real_ins_contras_loss: 1.2768,fake_ins_contras_loss : 8.1916, CL_contra_loss : 0.0281
model 2 is saved
cls traing
unseen=0.4220, seen=0.3082, h=0.3562
[1100/3001] Loss_D: -0.3930 Loss_G: 2.6678, Wasserstein_dist: 2.0774, real_ins_contras_loss: 1.2360,fake_ins_contras_loss : 8.1485, CL_contra_loss : 0.0296
[1200/3001] Loss_D: -0.4090 Loss_G: 2.8254, Wasserstein_dist: 2.0615, real_ins_contras_loss: 1.2286,fake_ins_contras_loss : 8.1611, CL_contra_loss : 0.0302
model 2 is saved
cls traing
unseen=0.3968, seen=0.3258, h=0.3578
[1300/3001] Loss_D: -0.4286 Loss_G: 2.9551, Wasserstein_dist: 2.0628, real_ins_contras_loss: 1.2182,fake_ins_contras_loss : 8.1574, CL_contra_loss : 0.0307
[1400/3001] Loss_D: -0.4216 Loss_G: 2.9482, Wasserstein_dist: 2.0595, real_ins_contras_loss: 1.2327,fake_ins_contras_loss : 8.1871, CL_contra_loss : 0.0254
model 2 is saved
cls traing
unseen=0.4061, seen=0.3268, h=0.3622
kl_loss G is 0.005392956547439098
kl_loss G is 0.00542829092592001
kl_loss G is 0.005467275157570839
kl_loss G is 0.005474059376865625
[1500/3001] Loss_D: -0.4055 Loss_G: 3.0388, Wasserstein_dist: 2.1086, real_ins_contras_loss: 1.2652,fake_ins_contras_loss : 8.1913, CL_contra_loss : 0.0301
[1600/3001] Loss_D: -0.3983 Loss_G: 3.1029, Wasserstein_dist: 2.0879, real_ins_contras_loss: 1.2623,fake_ins_contras_loss : 8.2255, CL_contra_loss : 0.0293
model 2 is saved
cls traing
unseen=0.4061, seen=0.3253, h=0.3612
[1700/3001] Loss_D: -0.3323 Loss_G: 3.1003, Wasserstein_dist: 2.0383, real_ins_contras_loss: 1.2670,fake_ins_contras_loss : 8.1879, CL_contra_loss : 0.0283
[1800/3001] Loss_D: -0.4407 Loss_G: 3.1427, Wasserstein_dist: 2.1211, real_ins_contras_loss: 1.2448,fake_ins_contras_loss : 8.1932, CL_contra_loss : 0.0290
model 2 is saved
cls traing
unseen=0.4021, seen=0.3320, h=0.3637
[1900/3001] Loss_D: -0.3516 Loss_G: 3.3057, Wasserstein_dist: 2.0358, real_ins_contras_loss: 1.2395,fake_ins_contras_loss : 8.2187, CL_contra_loss : 0.0282
kl_loss G is 0.005536423996090889
kl_loss G is 0.0055482638999819756
kl_loss G is 0.0055804625153541565
kl_loss G is 0.005620271433144808
[2000/3001] Loss_D: -0.4211 Loss_G: 3.2409, Wasserstein_dist: 2.1128, real_ins_contras_loss: 1.2512,fake_ins_contras_loss : 8.2642, CL_contra_loss : 0.0288
model 2 is saved
cls traing
unseen=0.4048, seen=0.3309, h=0.3641
[2100/3001] Loss_D: -0.4061 Loss_G: 3.4361, Wasserstein_dist: 2.0980, real_ins_contras_loss: 1.2641,fake_ins_contras_loss : 8.2315, CL_contra_loss : 0.0262
[2200/3001] Loss_D: -0.4675 Loss_G: 3.3742, Wasserstein_dist: 2.1379, real_ins_contras_loss: 1.2357,fake_ins_contras_loss : 8.2714, CL_contra_loss : 0.0290
model 2 is saved
cls traing
unseen=0.3929, seen=0.3366, h=0.3626
[2300/3001] Loss_D: -0.4028 Loss_G: 3.4181, Wasserstein_dist: 2.0804, real_ins_contras_loss: 1.2648,fake_ins_contras_loss : 8.2799, CL_contra_loss : 0.0279
[2400/3001] Loss_D: -0.3859 Loss_G: 3.3690, Wasserstein_dist: 2.0551, real_ins_contras_loss: 1.2431,fake_ins_contras_loss : 8.1193, CL_contra_loss : 0.0275
model 2 is saved
cls traing
unseen=0.4246, seen=0.3247, h=0.3680
kl_loss G is 0.005673141684383154
kl_loss G is 0.0056449901312589645
kl_loss G is 0.005675723776221275
kl_loss G is 0.005629309453070164
[2500/3001] Loss_D: -0.4152 Loss_G: 3.4218, Wasserstein_dist: 2.1151, real_ins_contras_loss: 1.2762,fake_ins_contras_loss : 8.2777, CL_contra_loss : 0.0276
[2600/3001] Loss_D: -0.4218 Loss_G: 3.3806, Wasserstein_dist: 2.0969, real_ins_contras_loss: 1.2520,fake_ins_contras_loss : 8.2465, CL_contra_loss : 0.0283
model 2 is saved
cls traing
unseen=0.4312, seen=0.3242, h=0.3701
[2700/3001] Loss_D: -0.4628 Loss_G: 3.4881, Wasserstein_dist: 2.1010, real_ins_contras_loss: 1.2363,fake_ins_contras_loss : 8.2728, CL_contra_loss : 0.0272
[2800/3001] Loss_D: -0.3794 Loss_G: 3.5790, Wasserstein_dist: 2.1184, real_ins_contras_loss: 1.3126,fake_ins_contras_loss : 8.2310, CL_contra_loss : 0.0277
model 2 is saved
cls traing
unseen=0.3823, seen=0.3387, h=0.3591
[2900/3001] Loss_D: -0.4304 Loss_G: 3.5265, Wasserstein_dist: 2.1129, real_ins_contras_loss: 1.2750,fake_ins_contras_loss : 8.1849, CL_contra_loss : 0.0264
kl_loss G is 0.005737000610679388
kl_loss G is 0.0056295269168913364
kl_loss G is 0.005771571304649115
kl_loss G is 0.0057706767693161964
[3000/3001] Loss_D: -0.5298 Loss_G: 3.6321, Wasserstein_dist: 2.1210, real_ins_contras_loss: 1.2077,fake_ins_contras_loss : 8.1972, CL_contra_loss : 0.0268
model 2 is saved
cls traing
unseen=0.4603, seen=0.3088, h=0.3696
average_acc is 0.36124557826655823
 hightest s is 0.32422680412371213, u is 0.43121693121693133, H is 0.3701456003985695,
----------------------------------------
loading task 3th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552]
current seen_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417, 577, 281, 132, 73, 123, 94, 548, 609, 617, 180, 305, 512, 394, 393, 22, 701, 605, 390, 376, 15, 347, 532, 458, 88, 643, 333, 477, 61, 219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523, 17, 714, 186, 689, 133, 50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655, 520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593, 561, 248, 665, 232, 301, 447, 16, 502, 646, 528, 688, 120, 205, 426, 568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422, 151, 51, 708, 165, 419, 214, 168, 686, 147, 209, 41, 83, 29, 553, 188, 114, 202, 566, 573, 652, 473, 608, 40, 565, 257, 375, 227, 80, 34, 93, 269, 54, 263, 299, 31, 43, 451, 234, 172, 335, 363, 551, 443, 125, 218, 550, 620, 279, 247, 157, 466, 669, 380, 649, 178, 581, 692, 572, 436, 87, 7, 368, 276, 662, 633, 597, 461, 360, 241, 327, 76, 199, 193, 491, 25, 303, 703, 408, 11, 616, 623, 432, 68, 362, 332, 90, 225, 9, 552, 330, 384, 26, 569, 549, 600, 586, 534, 653, 297, 510, 197, 401, 316, 709, 389, 639, 628, 402, 684, 149, 14, 242, 69, 252, 66, 67, 127, 511, 264, 307, 364, 271, 578, 435, 198, 680, 539, 487, 554, 678, 559, 280, 369, 707, 524, 228, 261, 359, 304, 146, 437, 370, 449, 287, 122, 516]
curr_unseen_label is [23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529, 645, 32, 112, 358, 237, 286, 95, 298, 342, 379, 622, 138, 75, 184, 493, 381, 482, 245, 262, 715, 471, 221, 711, 509, 710, 674, 158, 336, 99, 24, 216, 425, 650, 53, 38, 440]
pre_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529, 601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417, 577, 281, 132, 73, 123, 94, 548, 609, 617, 180, 305, 512, 394, 393, 22, 701, 605, 390, 376, 15, 347, 532, 458, 88, 643, 333, 477, 61, 219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523, 17, 714, 186, 689, 133, 50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655, 520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593, 561, 248, 665, 232, 301, 447, 16, 502, 646, 528, 688, 120, 205, 426, 568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422, 151, 51, 708, 165, 419, 214, 168, 686, 147, 209, 41, 83, 29, 553, 188, 114, 202, 645, 32, 112, 358, 237, 286, 95, 298, 342, 379, 622, 138, 75, 184, 493, 381, 482, 245, 566, 573, 652, 473, 608, 40, 565, 257, 375, 227, 80, 34, 93, 269, 54, 263, 299, 31, 43, 451, 234, 172, 335, 363, 551, 443, 125, 218, 550, 620, 279, 247, 157, 466, 669, 380, 649, 178, 581, 692, 572, 436, 87, 7, 368, 276, 662, 633, 597, 461, 360, 241, 327, 76, 199, 193, 491, 25, 303, 703, 408, 11, 616, 623, 432, 68, 362, 332, 90, 225, 9, 552, 330, 384, 26, 569, 549, 600, 586, 534, 653, 297, 510, 197, 401, 316, 709, 389, 639, 628, 402, 684, 149, 14, 242, 69, 252, 66, 67, 127, 511, 264, 307, 364, 271, 578, 435, 198, 680, 539, 487, 554, 678, 559, 280, 369, 707, 524, 228, 261, 359, 304, 146, 437, 370, 449, 287, 122, 516, 262, 715, 471, 221, 711, 509, 710, 674, 158, 336, 99, 24, 216, 425, 650, 53, 38, 440]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570]
incremental training task 3···
loading task 3 data and the pre model
model 2 is loaded
kl_loss G is 0.0
kl_loss G is 0.0028796792030334473
kl_loss G is 0.0016455749282613397
kl_loss G is 0.0015217129839584231
[0/3001] Loss_D: 4.7738 Loss_G: 3.6472, Wasserstein_dist: 0.6572, real_ins_contras_loss: 4.2734,fake_ins_contras_loss : 8.5095, CL_contra_loss : 0.1061
model 3 is saved
cls traing
unseen=0.3519, seen=0.2658, h=0.3028
[100/3001] Loss_D: 0.2469 Loss_G: 2.8836, Wasserstein_dist: 1.5509, real_ins_contras_loss: 1.3171,fake_ins_contras_loss : 8.1097, CL_contra_loss : 0.0363
[200/3001] Loss_D: -0.0043 Loss_G: 2.7887, Wasserstein_dist: 1.7317, real_ins_contras_loss: 1.3157,fake_ins_contras_loss : 8.0296, CL_contra_loss : 0.0335
model 3 is saved
cls traing
unseen=0.3607, seen=0.2720, h=0.3101
[300/3001] Loss_D: -0.1399 Loss_G: 2.7211, Wasserstein_dist: 1.7950, real_ins_contras_loss: 1.2410,fake_ins_contras_loss : 8.0449, CL_contra_loss : 0.0299
[400/3001] Loss_D: -0.1497 Loss_G: 2.8192, Wasserstein_dist: 1.8362, real_ins_contras_loss: 1.2684,fake_ins_contras_loss : 8.0723, CL_contra_loss : 0.0271
model 3 is saved
cls traing
unseen=0.4012, seen=0.2662, h=0.3200
kl_loss G is 0.004432672169059515
kl_loss G is 0.004406249150633812
kl_loss G is 0.004385015461593866
kl_loss G is 0.0043740482069551945
[500/3001] Loss_D: -0.2115 Loss_G: 2.8539, Wasserstein_dist: 1.9091, real_ins_contras_loss: 1.2692,fake_ins_contras_loss : 8.0304, CL_contra_loss : 0.0271
[600/3001] Loss_D: -0.3005 Loss_G: 2.9143, Wasserstein_dist: 1.9750, real_ins_contras_loss: 1.2649,fake_ins_contras_loss : 8.0321, CL_contra_loss : 0.0267
model 3 is saved
cls traing
unseen=0.3739, seen=0.2758, h=0.3175
[700/3001] Loss_D: -0.2192 Loss_G: 3.0699, Wasserstein_dist: 1.9620, real_ins_contras_loss: 1.3290,fake_ins_contras_loss : 8.1370, CL_contra_loss : 0.0253
[800/3001] Loss_D: -0.3440 Loss_G: 3.1278, Wasserstein_dist: 2.0166, real_ins_contras_loss: 1.2801,fake_ins_contras_loss : 8.1124, CL_contra_loss : 0.0264
model 3 is saved
cls traing
unseen=0.3483, seen=0.2944, h=0.3191
[900/3001] Loss_D: -0.3871 Loss_G: 3.1818, Wasserstein_dist: 2.0139, real_ins_contras_loss: 1.2375,fake_ins_contras_loss : 8.1089, CL_contra_loss : 0.0276
kl_loss G is 0.0047951932065188885
kl_loss G is 0.004827643278986216
kl_loss G is 0.004857763182371855
kl_loss G is 0.004824342206120491
[1000/3001] Loss_D: -0.3885 Loss_G: 3.2476, Wasserstein_dist: 2.0296, real_ins_contras_loss: 1.2407,fake_ins_contras_loss : 8.0459, CL_contra_loss : 0.0269
model 3 is saved
cls traing
unseen=0.4056, seen=0.2596, h=0.3166
[1100/3001] Loss_D: -0.3671 Loss_G: 3.3997, Wasserstein_dist: 2.0421, real_ins_contras_loss: 1.2550,fake_ins_contras_loss : 8.1138, CL_contra_loss : 0.0273
[1200/3001] Loss_D: -0.4044 Loss_G: 3.4413, Wasserstein_dist: 2.1000, real_ins_contras_loss: 1.2740,fake_ins_contras_loss : 8.2093, CL_contra_loss : 0.0259
model 3 is saved
cls traing
unseen=0.3598, seen=0.2905, h=0.3215
[1300/3001] Loss_D: -0.3969 Loss_G: 3.5178, Wasserstein_dist: 2.0460, real_ins_contras_loss: 1.2598,fake_ins_contras_loss : 8.1203, CL_contra_loss : 0.0256
[1400/3001] Loss_D: -0.4044 Loss_G: 3.5080, Wasserstein_dist: 2.1016, real_ins_contras_loss: 1.2853,fake_ins_contras_loss : 8.1641, CL_contra_loss : 0.0274
model 3 is saved
cls traing
unseen=0.3854, seen=0.2778, h=0.3228
kl_loss G is 0.004996342584490776
kl_loss G is 0.004879996180534363
kl_loss G is 0.005008494947105646
kl_loss G is 0.004955065902322531
[1500/3001] Loss_D: -0.4501 Loss_G: 3.5655, Wasserstein_dist: 2.1083, real_ins_contras_loss: 1.2301,fake_ins_contras_loss : 8.2032, CL_contra_loss : 0.0289
[1600/3001] Loss_D: -0.3893 Loss_G: 3.5861, Wasserstein_dist: 2.0846, real_ins_contras_loss: 1.2713,fake_ins_contras_loss : 8.1333, CL_contra_loss : 0.0251
model 3 is saved
cls traing
unseen=0.3571, seen=0.2940, h=0.3225
[1700/3001] Loss_D: -0.4220 Loss_G: 3.6568, Wasserstein_dist: 2.1178, real_ins_contras_loss: 1.2712,fake_ins_contras_loss : 8.1354, CL_contra_loss : 0.0273
[1800/3001] Loss_D: -0.4380 Loss_G: 3.6190, Wasserstein_dist: 2.0824, real_ins_contras_loss: 1.2348,fake_ins_contras_loss : 8.1846, CL_contra_loss : 0.0266
model 3 is saved
cls traing
unseen=0.3704, seen=0.2816, h=0.3200
[1900/3001] Loss_D: -0.4530 Loss_G: 3.7030, Wasserstein_dist: 2.0940, real_ins_contras_loss: 1.2198,fake_ins_contras_loss : 8.1716, CL_contra_loss : 0.0282
kl_loss G is 0.0049926647916436195
kl_loss G is 0.004930312279611826
kl_loss G is 0.004940148908644915
kl_loss G is 0.004977503791451454
[2000/3001] Loss_D: -0.4389 Loss_G: 3.7007, Wasserstein_dist: 2.0997, real_ins_contras_loss: 1.2529,fake_ins_contras_loss : 8.0841, CL_contra_loss : 0.0274
model 3 is saved
cls traing
unseen=0.3783, seen=0.2913, h=0.3291
[2100/3001] Loss_D: -0.4466 Loss_G: 3.7398, Wasserstein_dist: 2.1303, real_ins_contras_loss: 1.2575,fake_ins_contras_loss : 8.1778, CL_contra_loss : 0.0247
[2200/3001] Loss_D: -0.4708 Loss_G: 3.6626, Wasserstein_dist: 2.1336, real_ins_contras_loss: 1.2691,fake_ins_contras_loss : 8.1107, CL_contra_loss : 0.0277
model 3 is saved
cls traing
unseen=0.3854, seen=0.2867, h=0.3288
[2300/3001] Loss_D: -0.4776 Loss_G: 3.8704, Wasserstein_dist: 2.1203, real_ins_contras_loss: 1.2301,fake_ins_contras_loss : 8.1889, CL_contra_loss : 0.0272
[2400/3001] Loss_D: -0.4811 Loss_G: 3.8333, Wasserstein_dist: 2.1166, real_ins_contras_loss: 1.2334,fake_ins_contras_loss : 8.1523, CL_contra_loss : 0.0265
model 3 is saved
cls traing
unseen=0.3660, seen=0.2975, h=0.3282
kl_loss G is 0.005011531990021467
kl_loss G is 0.005092713516205549
kl_loss G is 0.00498987827450037
kl_loss G is 0.005032817833125591
[2500/3001] Loss_D: -0.5208 Loss_G: 3.7829, Wasserstein_dist: 2.1364, real_ins_contras_loss: 1.2139,fake_ins_contras_loss : 8.1943, CL_contra_loss : 0.0253
[2600/3001] Loss_D: -0.4980 Loss_G: 3.7582, Wasserstein_dist: 2.1543, real_ins_contras_loss: 1.2515,fake_ins_contras_loss : 8.2334, CL_contra_loss : 0.0267
model 3 is saved
cls traing
unseen=0.3704, seen=0.2824, h=0.3205
[2700/3001] Loss_D: -0.5025 Loss_G: 3.7891, Wasserstein_dist: 2.1033, real_ins_contras_loss: 1.2062,fake_ins_contras_loss : 8.2351, CL_contra_loss : 0.0255
[2800/3001] Loss_D: -0.5149 Loss_G: 3.7733, Wasserstein_dist: 2.1454, real_ins_contras_loss: 1.2296,fake_ins_contras_loss : 8.1898, CL_contra_loss : 0.0263
model 3 is saved
cls traing
unseen=0.3757, seen=0.2863, h=0.3249
[2900/3001] Loss_D: -0.4941 Loss_G: 3.9018, Wasserstein_dist: 2.1520, real_ins_contras_loss: 1.2583,fake_ins_contras_loss : 8.2410, CL_contra_loss : 0.0255
kl_loss G is 0.005089811515063047
kl_loss G is 0.0051371026784181595
kl_loss G is 0.005049406550824642
kl_loss G is 0.0051015811040997505
[3000/3001] Loss_D: -0.4591 Loss_G: 3.9326, Wasserstein_dist: 2.1481, real_ins_contras_loss: 1.2686,fake_ins_contras_loss : 8.2379, CL_contra_loss : 0.0254
model 3 is saved
cls traing
unseen=0.3959, seen=0.2816, h=0.3291
average_acc is 0.32204151584498053
 hightest s is 0.291295938104449, u is 0.3783068783068782, H is 0.32914812873210475,
----------------------------------------
loading task 4th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698]
current seen_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417, 577, 281, 132, 73, 123, 94, 548, 609, 617, 180, 305, 512, 394, 393, 22, 701, 605, 390, 376, 15, 347, 532, 458, 88, 643, 333, 477, 61, 219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523, 17, 714, 186, 689, 133, 50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655, 520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593, 561, 248, 665, 232, 301, 447, 16, 502, 646, 528, 688, 120, 205, 426, 568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422, 151, 51, 708, 165, 419, 214, 168, 686, 147, 209, 41, 83, 29, 553, 188, 114, 202, 566, 573, 652, 473, 608, 40, 565, 257, 375, 227, 80, 34, 93, 269, 54, 263, 299, 31, 43, 451, 234, 172, 335, 363, 551, 443, 125, 218, 550, 620, 279, 247, 157, 466, 669, 380, 649, 178, 581, 692, 572, 436, 87, 7, 368, 276, 662, 633, 597, 461, 360, 241, 327, 76, 199, 193, 491, 25, 303, 703, 408, 11, 616, 623, 432, 68, 362, 332, 90, 225, 9, 552, 330, 384, 26, 569, 549, 600, 586, 534, 653, 297, 510, 197, 401, 316, 709, 389, 639, 628, 402, 684, 149, 14, 242, 69, 252, 66, 67, 127, 511, 264, 307, 364, 271, 578, 435, 198, 680, 539, 487, 554, 678, 559, 280, 369, 707, 524, 228, 261, 359, 304, 146, 437, 370, 449, 287, 122, 516, 346, 341, 637, 647, 410, 47, 39, 253, 538, 361, 404, 267, 521, 294, 713, 612, 607, 5, 121, 331, 666, 395, 556, 352, 171, 277, 278, 663, 636, 378, 150, 289, 211, 86, 374, 195, 317, 396, 366, 562, 693, 268, 324, 460, 258, 13, 314, 463, 292, 564, 606, 706, 632, 229, 696, 495, 201, 60, 106, 318, 356, 391, 467, 166, 144, 118, 321, 126, 691, 111, 84, 660, 89, 644, 506, 431, 2, 101, 97, 141, 429, 501, 406, 340, 372, 272, 176, 450, 104, 661, 236, 233, 481, 468, 200, 42, 156, 673, 78, 128, 251, 351, 119, 45, 52, 485, 439, 415, 59, 522, 159, 407, 630, 8, 282, 392, 115, 503, 385, 30, 570, 454, 594, 20, 571, 143, 667, 36]
curr_unseen_label is [23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529, 645, 32, 112, 358, 237, 286, 95, 298, 342, 379, 622, 138, 75, 184, 493, 381, 482, 245, 262, 715, 471, 221, 711, 509, 710, 674, 158, 336, 99, 24, 216, 425, 650, 53, 38, 440, 517, 72, 315, 420, 3, 246, 259, 152, 57, 658, 423, 448, 74, 558, 580, 560, 130, 635]
pre_label is [557, 476, 388, 496, 63, 105, 614, 659, 671, 418, 27, 162, 490, 260, 291, 427, 403, 497, 702, 194, 625, 697, 35, 238, 98, 700, 626, 542, 312, 187, 377, 383, 543, 690, 433, 514, 683, 567, 306, 530, 116, 699, 670, 546, 154, 428, 339, 474, 217, 230, 117, 58, 203, 365, 576, 472, 293, 91, 192, 206, 79, 231, 575, 96, 421, 46, 55, 226, 525, 175, 183, 109, 387, 716, 319, 349, 224, 243, 499, 70, 602, 33, 371, 290, 399, 595, 255, 137, 153, 488, 444, 478, 469, 507, 273, 484, 540, 210, 213, 326, 409, 579, 398, 190, 77, 337, 536, 480, 65, 320, 619, 164, 265, 344, 100, 373, 182, 148, 430, 462, 92, 169, 672, 329, 641, 413, 142, 668, 638, 604, 167, 519, 220, 170, 300, 500, 705, 288, 140, 4, 102, 518, 310, 675, 240, 274, 19, 459, 682, 591, 441, 223, 338, 12, 533, 155, 400, 354, 62, 434, 621, 618, 452, 555, 296, 249, 49, 457, 535, 489, 56, 642, 343, 191, 256, 664, 295, 592, 189, 685, 1, 677, 615, 537, 596, 598, 207, 411, 6, 71, 541, 37, 416, 161, 492, 603, 325, 222, 676, 582, 108, 657, 382, 82, 129, 107, 414, 397, 367, 486, 698, 624, 442, 589, 464, 174, 131, 28, 405, 64, 527, 599, 386, 687, 244, 250, 584, 424, 334, 323, 44, 453, 704, 18, 113, 0, 235, 110, 239, 515, 483, 654, 634, 355, 48, 412, 21, 613, 311, 648, 313, 81, 526, 590, 610, 309, 456, 588, 544, 23, 656, 508, 712, 695, 85, 196, 145, 681, 103, 124, 254, 631, 679, 328, 353, 10, 529, 601, 629, 208, 215, 181, 640, 204, 513, 545, 134, 455, 611, 547, 417, 577, 281, 132, 73, 123, 94, 548, 609, 617, 180, 305, 512, 394, 393, 22, 701, 605, 390, 376, 15, 347, 532, 458, 88, 643, 333, 477, 61, 219, 345, 173, 302, 585, 574, 285, 357, 185, 694, 523, 17, 714, 186, 689, 133, 50, 275, 651, 587, 465, 470, 504, 498, 136, 212, 627, 655, 520, 322, 348, 160, 445, 283, 179, 308, 583, 139, 438, 563, 177, 593, 561, 248, 665, 232, 301, 447, 16, 502, 646, 528, 688, 120, 205, 426, 568, 446, 163, 505, 270, 350, 284, 479, 531, 494, 475, 266, 135, 422, 151, 51, 708, 165, 419, 214, 168, 686, 147, 209, 41, 83, 29, 553, 188, 114, 202, 645, 32, 112, 358, 237, 286, 95, 298, 342, 379, 622, 138, 75, 184, 493, 381, 482, 245, 566, 573, 652, 473, 608, 40, 565, 257, 375, 227, 80, 34, 93, 269, 54, 263, 299, 31, 43, 451, 234, 172, 335, 363, 551, 443, 125, 218, 550, 620, 279, 247, 157, 466, 669, 380, 649, 178, 581, 692, 572, 436, 87, 7, 368, 276, 662, 633, 597, 461, 360, 241, 327, 76, 199, 193, 491, 25, 303, 703, 408, 11, 616, 623, 432, 68, 362, 332, 90, 225, 9, 552, 330, 384, 26, 569, 549, 600, 586, 534, 653, 297, 510, 197, 401, 316, 709, 389, 639, 628, 402, 684, 149, 14, 242, 69, 252, 66, 67, 127, 511, 264, 307, 364, 271, 578, 435, 198, 680, 539, 487, 554, 678, 559, 280, 369, 707, 524, 228, 261, 359, 304, 146, 437, 370, 449, 287, 122, 516, 262, 715, 471, 221, 711, 509, 710, 674, 158, 336, 99, 24, 216, 425, 650, 53, 38, 440, 346, 341, 637, 647, 410, 47, 39, 253, 538, 361, 404, 267, 521, 294, 713, 612, 607, 5, 121, 331, 666, 395, 556, 352, 171, 277, 278, 663, 636, 378, 150, 289, 211, 86, 374, 195, 317, 396, 366, 562, 693, 268, 324, 460, 258, 13, 314, 463, 292, 564, 606, 706, 632, 229, 696, 495, 201, 60, 106, 318, 356, 391, 467, 166, 144, 118, 321, 126, 691, 111, 84, 660, 89, 644, 506, 431, 2, 101, 97, 141, 429, 501, 406, 340, 372, 272, 176, 450, 104, 661, 236, 233, 481, 468, 200, 42, 156, 673, 78, 128, 251, 351, 119, 45, 52, 485, 439, 415, 59, 522, 159, 407, 630, 8, 282, 392, 115, 503, 385, 30, 570, 454, 594, 20, 571, 143, 667, 36, 517, 72, 315, 420, 3, 246, 259, 152, 57, 658, 423, 448, 74, 558, 580, 560, 130, 635]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716]
incremental training task 4···
loading task 4 data and the pre model
model 3 is loaded
kl_loss G is 0.0
kl_loss G is 0.002755278954282403
kl_loss G is 0.0016475336160510778
kl_loss G is 0.0014670160599052906
[0/3001] Loss_D: 4.8317 Loss_G: 3.9420, Wasserstein_dist: 0.5322, real_ins_contras_loss: 4.3454,fake_ins_contras_loss : 8.5215, CL_contra_loss : 0.0933
model 4 is saved
cls traing
unseen=0.3585, seen=0.2347, h=0.2837
[100/3001] Loss_D: 0.2883 Loss_G: 3.0375, Wasserstein_dist: 1.4827, real_ins_contras_loss: 1.3196,fake_ins_contras_loss : 8.0990, CL_contra_loss : 0.0360
[200/3001] Loss_D: -0.0596 Loss_G: 3.1577, Wasserstein_dist: 1.7152, real_ins_contras_loss: 1.2760,fake_ins_contras_loss : 8.0765, CL_contra_loss : 0.0299
model 4 is saved
cls traing
unseen=0.3360, seen=0.2443, h=0.2829
[300/3001] Loss_D: -0.1910 Loss_G: 3.1794, Wasserstein_dist: 1.8621, real_ins_contras_loss: 1.3041,fake_ins_contras_loss : 7.9911, CL_contra_loss : 0.0261
[400/3001] Loss_D: -0.2004 Loss_G: 3.2025, Wasserstein_dist: 1.8746, real_ins_contras_loss: 1.2762,fake_ins_contras_loss : 8.0732, CL_contra_loss : 0.0290
model 4 is saved
cls traing
unseen=0.3452, seen=0.2487, h=0.2891
kl_loss G is 0.003879708470776677
kl_loss G is 0.003970916382968426
kl_loss G is 0.00397908641025424
kl_loss G is 0.003986230585724115
[500/3001] Loss_D: -0.3011 Loss_G: 3.4238, Wasserstein_dist: 1.9273, real_ins_contras_loss: 1.2454,fake_ins_contras_loss : 7.9869, CL_contra_loss : 0.0267
[600/3001] Loss_D: -0.2749 Loss_G: 3.5640, Wasserstein_dist: 2.0003, real_ins_contras_loss: 1.3187,fake_ins_contras_loss : 8.1399, CL_contra_loss : 0.0251
model 4 is saved
cls traing
unseen=0.3313, seen=0.2465, h=0.2827
[700/3001] Loss_D: -0.3896 Loss_G: 3.5724, Wasserstein_dist: 2.0408, real_ins_contras_loss: 1.2397,fake_ins_contras_loss : 8.0359, CL_contra_loss : 0.0273
[800/3001] Loss_D: -0.3128 Loss_G: 3.6395, Wasserstein_dist: 2.0067, real_ins_contras_loss: 1.2841,fake_ins_contras_loss : 8.1575, CL_contra_loss : 0.0261
model 4 is saved
cls traing
unseen=0.3347, seen=0.2598, h=0.2925
[900/3001] Loss_D: -0.4020 Loss_G: 3.7974, Wasserstein_dist: 2.0393, real_ins_contras_loss: 1.2591,fake_ins_contras_loss : 8.0679, CL_contra_loss : 0.0260
kl_loss G is 0.004412800073623657
kl_loss G is 0.00443689851090312
kl_loss G is 0.004406709689646959
kl_loss G is 0.0043589272536337376
[1000/3001] Loss_D: -0.4596 Loss_G: 3.8649, Wasserstein_dist: 2.1027, real_ins_contras_loss: 1.2602,fake_ins_contras_loss : 8.1083, CL_contra_loss : 0.0269
model 4 is saved
cls traing
unseen=0.3505, seen=0.2558, h=0.2958
[1100/3001] Loss_D: -0.4135 Loss_G: 3.8686, Wasserstein_dist: 2.0675, real_ins_contras_loss: 1.2408,fake_ins_contras_loss : 8.1216, CL_contra_loss : 0.0247
[1200/3001] Loss_D: -0.4093 Loss_G: 3.9098, Wasserstein_dist: 2.1135, real_ins_contras_loss: 1.2995,fake_ins_contras_loss : 8.1988, CL_contra_loss : 0.0257
model 4 is saved
cls traing
unseen=0.3406, seen=0.2527, h=0.2902
[1300/3001] Loss_D: -0.4614 Loss_G: 4.0849, Wasserstein_dist: 2.0975, real_ins_contras_loss: 1.2341,fake_ins_contras_loss : 8.1510, CL_contra_loss : 0.0262
[1400/3001] Loss_D: -0.4784 Loss_G: 4.0681, Wasserstein_dist: 2.1735, real_ins_contras_loss: 1.2679,fake_ins_contras_loss : 8.1505, CL_contra_loss : 0.0237
model 4 is saved
cls traing
unseen=0.3300, seen=0.2595, h=0.2906
kl_loss G is 0.004547119140625
kl_loss G is 0.0045435065403580666
kl_loss G is 0.004524643067270517
kl_loss G is 0.004521775059401989
[1500/3001] Loss_D: -0.4760 Loss_G: 4.1104, Wasserstein_dist: 2.1350, real_ins_contras_loss: 1.2567,fake_ins_contras_loss : 8.1650, CL_contra_loss : 0.0257
[1600/3001] Loss_D: -0.4653 Loss_G: 4.1279, Wasserstein_dist: 2.1455, real_ins_contras_loss: 1.2728,fake_ins_contras_loss : 8.2204, CL_contra_loss : 0.0254
model 4 is saved
cls traing
unseen=0.3585, seen=0.2499, h=0.2945
[1700/3001] Loss_D: -0.5562 Loss_G: 4.1459, Wasserstein_dist: 2.1892, real_ins_contras_loss: 1.2329,fake_ins_contras_loss : 8.1588, CL_contra_loss : 0.0263
[1800/3001] Loss_D: -0.4694 Loss_G: 4.1729, Wasserstein_dist: 2.1132, real_ins_contras_loss: 1.2527,fake_ins_contras_loss : 8.1221, CL_contra_loss : 0.0252
model 4 is saved
cls traing
unseen=0.3406, seen=0.2611, h=0.2956
[1900/3001] Loss_D: -0.5367 Loss_G: 4.2245, Wasserstein_dist: 2.1752, real_ins_contras_loss: 1.2654,fake_ins_contras_loss : 8.1214, CL_contra_loss : 0.0247
kl_loss G is 0.0045981439761817455
kl_loss G is 0.0046468740329146385
kl_loss G is 0.004690137226134539
kl_loss G is 0.004589605610817671
[2000/3001] Loss_D: -0.4768 Loss_G: 4.1870, Wasserstein_dist: 2.1783, real_ins_contras_loss: 1.2748,fake_ins_contras_loss : 8.1809, CL_contra_loss : 0.0253
model 4 is saved
cls traing
unseen=0.3399, seen=0.2598, h=0.2945
[2100/3001] Loss_D: -0.5245 Loss_G: 4.2268, Wasserstein_dist: 2.1583, real_ins_contras_loss: 1.2670,fake_ins_contras_loss : 8.1813, CL_contra_loss : 0.0261
[2200/3001] Loss_D: -0.5271 Loss_G: 4.2665, Wasserstein_dist: 2.1595, real_ins_contras_loss: 1.2361,fake_ins_contras_loss : 8.1557, CL_contra_loss : 0.0249
model 4 is saved
cls traing
unseen=0.3604, seen=0.2533, h=0.2975
[2300/3001] Loss_D: -0.5251 Loss_G: 4.3073, Wasserstein_dist: 2.1805, real_ins_contras_loss: 1.2536,fake_ins_contras_loss : 8.2491, CL_contra_loss : 0.0230
[2400/3001] Loss_D: -0.5334 Loss_G: 4.2330, Wasserstein_dist: 2.1887, real_ins_contras_loss: 1.2759,fake_ins_contras_loss : 8.1770, CL_contra_loss : 0.0246
model 4 is saved
cls traing
unseen=0.3214, seen=0.2710, h=0.2941
kl_loss G is 0.00461325328797102
kl_loss G is 0.004613111726939678
kl_loss G is 0.004667883738875389
kl_loss G is 0.004608737304806709
[2500/3001] Loss_D: -0.5667 Loss_G: 4.2957, Wasserstein_dist: 2.2142, real_ins_contras_loss: 1.2537,fake_ins_contras_loss : 8.2622, CL_contra_loss : 0.0237
[2600/3001] Loss_D: -0.4702 Loss_G: 4.3729, Wasserstein_dist: 2.1374, real_ins_contras_loss: 1.2855,fake_ins_contras_loss : 8.2069, CL_contra_loss : 0.0248
model 4 is saved
cls traing
unseen=0.3618, seen=0.2540, h=0.2984
[2700/3001] Loss_D: -0.5017 Loss_G: 4.2559, Wasserstein_dist: 2.1125, real_ins_contras_loss: 1.2436,fake_ins_contras_loss : 8.1301, CL_contra_loss : 0.0235
[2800/3001] Loss_D: -0.5003 Loss_G: 4.3268, Wasserstein_dist: 2.1597, real_ins_contras_loss: 1.2520,fake_ins_contras_loss : 8.2160, CL_contra_loss : 0.0244
model 4 is saved
cls traing
unseen=0.3499, seen=0.2502, h=0.2918
[2900/3001] Loss_D: -0.5706 Loss_G: 4.3241, Wasserstein_dist: 2.2088, real_ins_contras_loss: 1.2591,fake_ins_contras_loss : 8.2787, CL_contra_loss : 0.0232
kl_loss G is 0.004686275497078896
kl_loss G is 0.004696408286690712
kl_loss G is 0.0046102008782327175
kl_loss G is 0.004824855364859104
[3000/3001] Loss_D: -0.5397 Loss_G: 4.2902, Wasserstein_dist: 2.1561, real_ins_contras_loss: 1.2424,fake_ins_contras_loss : 8.2475, CL_contra_loss : 0.0263
model 4 is saved
cls traing
unseen=0.3459, seen=0.2595, h=0.2966
average_acc is 0.29245332361028364
 hightest s is 0.25395348837209303, u is 0.36177248677248663, H is 0.2984229632064716,
----------------------------------------
loading task 5th data
Traceback (most recent call last):
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 372, in <module>
    train_one_task(premodel = pregan)
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 348, in train_one_task
    data.current_class_index()
  File "E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3\utils\data_pretrain.py", line 270, in current_class_index
    current_data = self.splited_seen_class[self.current_taskid].unsqueeze(dim=1)
IndexError: list index out of range

Process finished with exit code 1
F:\anaconda3_new\python.exe "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py"
Namespace(attSize=102, batch_size=512, beta=60, beta1=0.5, beta_1=10, class_embedding='att', classifier_lr=0.001, critic_iter=5, cuda=True, dataroot='data', dataset='SUN', dir='models\\SUN', distill_proj_hidden_dim=4096, embedSize=2048, epochs=100, image_embedding='res101', ins_temp=0.1, ins_weight=0.001, lambda1=10, lr=0.0001, matdataset=True, nclass_all=717, nclass_seen=645, ndh=4096, neh=4096, nepoch=3001, ngh=4096, nz=102, outzSize=512, preprocessing=True, pretrain_class_number=130, pretrain_gan=True, recons_weight=0.001, resSize=2048, shuffer_class=True, standardization=False, syn_num=256, syn_num_rp=50, syn_num_s=60, syn_num_u=100, task_num=4, validation=False)
data/SUN/res101.mat E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3
att: torch.Size([717, 102])
splited_seen_class is [tensor([ 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477,
        546, 312,  26, 368, 426, 670, 294, 166, 393, 340, 337,  49, 485, 671,
        219,  28, 175, 331, 475, 323, 313,  37, 387, 360, 164, 443,  83, 460,
        503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587,  63, 513,
        476, 515, 276, 115, 155, 660, 189, 409,  80, 690, 396,  62, 654, 626,
        204,  89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527,
        501, 641, 163, 376, 329, 300, 110, 506,  56, 159, 241, 400,  64, 150,
        432, 174,  55,  54, 161, 629, 252, 197, 214, 238, 220,  34, 168, 117,
        714,  96, 413, 453, 239, 639, 472, 113, 326, 445, 535,  73, 172, 605,
        512, 653,  43]), tensor([537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100,
        490,  21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268,
        447, 431, 703, 572, 667,  50, 504, 352, 590, 359, 365, 500,   6, 274,
        280, 270, 258, 713, 548, 648, 211, 514,   4,  52, 607, 233,  60, 165,
        253, 316, 709, 135, 698, 227,  86, 355, 435, 675, 541, 202, 434, 140,
        540,  66, 213, 588, 550, 229, 235, 295, 395, 201,  42, 542, 116, 306,
        444, 101, 232, 391, 685,  93, 680, 510, 454, 154, 264, 544, 109, 285,
        642, 236, 427, 291, 181, 207,  65,  44, 594, 222,  22, 203, 399,  20,
        107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292,  33,
        526, 507,  90]), tensor([499,  14, 686,  79, 539, 404, 348, 212, 457, 553, 410, 408, 139, 633,
         94, 191, 287, 651,  18, 123, 345, 149, 321,  46, 519,  36, 664,  92,
        636, 567, 591, 269, 589,  67,  76, 230, 414, 282, 655, 702, 533, 647,
        568,   9, 418, 170, 364, 480, 351, 474,  41, 279,  91, 188, 251, 275,
        522, 547, 296, 411, 119, 389, 523, 356,  45, 363, 332, 290, 505, 277,
        466, 146, 663, 225, 126, 361, 419,  17, 666, 179, 386,  29,  88, 255,
        570, 132, 531, 536, 327, 646, 248, 111,  48, 250,  13, 310, 382, 330,
        473, 177, 429,  15, 157, 384,  71,  39, 630, 556, 543, 450, 700, 661,
        273, 309, 397, 634,  77, 304,  81, 623, 125, 452, 691, 218, 579, 494,
        625, 436,  70]), tensor([412, 486, 308, 215, 428, 256, 338, 176, 616, 186, 302, 263,  16, 598,
        496,  68, 687,  87, 335, 339, 223, 210, 224, 383, 456, 303, 596, 592,
        459, 421, 620, 297, 609,   2, 704, 433, 604, 128, 644, 322, 495, 599,
        341, 619, 573, 716, 401,  12, 611, 178, 369,  40, 678, 462, 417, 199,
        603, 696, 467, 398, 701, 162, 688, 377, 677, 142, 606, 552, 415, 347,
        652, 260, 692, 131, 451,  82, 684, 305, 375, 569, 144, 615, 481, 173,
        134, 366, 672, 430, 487,  35,   0, 621, 156, 373, 492, 483, 272, 325,
        707, 388, 657, 243, 565, 362, 228,   1, 185, 271, 247, 346, 697, 106,
        278, 148, 206,  47, 521, 600, 344, 562, 637, 638, 104, 283, 324,  58,
        349, 617])]
splited_unseen_class is [tensor([358,   3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517,  75,  38,
        353,  32,  85, 315]), tensor([ 95,  72, 216,  10, 711, 152, 471, 509, 658, 631, 130, 420,  57, 124,
        158, 679, 493, 342]), tensor([138, 286, 650, 336, 259, 221, 262, 425, 560,  99,  23, 508, 112, 635,
        580,  24, 710, 656]), tensor([ 53, 196, 328, 645, 712, 440, 448, 482, 381, 245, 254, 246, 681,  74,
        558, 423, 184, 695])]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129]
pre_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180]
curr_seen_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180]
**************************************************
pretraining cgan model ···
[0/3001] Loss_D: 2.7524 Loss_G: -0.5986, Wasserstein_dist: 1.5506, real_ins_contras_loss: 4.1929,fake_ins_contras_loss : 7.0857
[50/3001] Loss_D: 0.2366 Loss_G: -0.3299, Wasserstein_dist: 1.2516, real_ins_contras_loss: 1.3935,fake_ins_contras_loss : 7.0833
[100/3001] Loss_D: 1.3998 Loss_G: -2.2094, Wasserstein_dist: -0.1183, real_ins_contras_loss: 1.2690,fake_ins_contras_loss : 7.3224
[150/3001] Loss_D: 0.8449 Loss_G: -0.3366, Wasserstein_dist: 0.4042, real_ins_contras_loss: 1.2421,fake_ins_contras_loss : 6.7871
model 0 is saved
cls traing
Training classifier loss= 3.6292
seen=0.0554
[200/3001] Loss_D: 0.6671 Loss_G: -0.5615, Wasserstein_dist: 0.5707, real_ins_contras_loss: 1.2262,fake_ins_contras_loss : 7.0374
[250/3001] Loss_D: 0.6131 Loss_G: -0.4807, Wasserstein_dist: 0.6491, real_ins_contras_loss: 1.2469,fake_ins_contras_loss : 7.1381
[300/3001] Loss_D: 0.6010 Loss_G: -0.5963, Wasserstein_dist: 0.6386, real_ins_contras_loss: 1.2175,fake_ins_contras_loss : 7.0626
[350/3001] Loss_D: 0.5656 Loss_G: -0.6725, Wasserstein_dist: 0.7080, real_ins_contras_loss: 1.2423,fake_ins_contras_loss : 6.7407
model 0 is saved
cls traing
Training classifier loss= 2.7753
seen=0.1892
[400/3001] Loss_D: 0.4786 Loss_G: -0.5826, Wasserstein_dist: 0.7862, real_ins_contras_loss: 1.2231,fake_ins_contras_loss : 6.5743
[450/3001] Loss_D: 0.5070 Loss_G: -0.8051, Wasserstein_dist: 0.7625, real_ins_contras_loss: 1.2311,fake_ins_contras_loss : 6.3024
[500/3001] Loss_D: 0.5875 Loss_G: -0.8752, Wasserstein_dist: 0.7184, real_ins_contras_loss: 1.2735,fake_ins_contras_loss : 6.1218
[550/3001] Loss_D: 0.4969 Loss_G: -0.8533, Wasserstein_dist: 0.7497, real_ins_contras_loss: 1.2152,fake_ins_contras_loss : 5.9158
model 0 is saved
cls traing
Training classifier loss= 1.5333
seen=0.3338
[600/3001] Loss_D: 0.4932 Loss_G: -0.6494, Wasserstein_dist: 0.7564, real_ins_contras_loss: 1.2231,fake_ins_contras_loss : 5.6261
[650/3001] Loss_D: 0.5393 Loss_G: -0.9165, Wasserstein_dist: 0.7012, real_ins_contras_loss: 1.2200,fake_ins_contras_loss : 5.4772
[700/3001] Loss_D: 0.4748 Loss_G: -0.7777, Wasserstein_dist: 0.7657, real_ins_contras_loss: 1.2157,fake_ins_contras_loss : 5.2443
[750/3001] Loss_D: 0.4608 Loss_G: -0.9162, Wasserstein_dist: 0.7953, real_ins_contras_loss: 1.2317,fake_ins_contras_loss : 5.0818
model 0 is saved
cls traing
Training classifier loss= 1.2115
seen=0.4262
[800/3001] Loss_D: 0.4620 Loss_G: -0.8309, Wasserstein_dist: 0.7949, real_ins_contras_loss: 1.2239,fake_ins_contras_loss : 5.0832
[850/3001] Loss_D: 0.3763 Loss_G: -0.7995, Wasserstein_dist: 0.8739, real_ins_contras_loss: 1.2110,fake_ins_contras_loss : 4.8833
[900/3001] Loss_D: 0.3447 Loss_G: -0.7049, Wasserstein_dist: 0.9139, real_ins_contras_loss: 1.2141,fake_ins_contras_loss : 4.7294
[950/3001] Loss_D: 0.1792 Loss_G: -0.6481, Wasserstein_dist: 1.0463, real_ins_contras_loss: 1.1825,fake_ins_contras_loss : 4.5052
model 0 is saved
cls traing
Training classifier loss= 0.8720
seen=0.4815
[1000/3001] Loss_D: 0.1786 Loss_G: -0.5846, Wasserstein_dist: 1.0253, real_ins_contras_loss: 1.1681,fake_ins_contras_loss : 4.4018
[1050/3001] Loss_D: 0.1530 Loss_G: -0.6493, Wasserstein_dist: 1.1127, real_ins_contras_loss: 1.2214,fake_ins_contras_loss : 4.2260
[1100/3001] Loss_D: 0.1537 Loss_G: -0.4655, Wasserstein_dist: 1.1374, real_ins_contras_loss: 1.2519,fake_ins_contras_loss : 4.1687
[1150/3001] Loss_D: 0.0980 Loss_G: -0.4017, Wasserstein_dist: 1.1716, real_ins_contras_loss: 1.2184,fake_ins_contras_loss : 4.0409
model 0 is saved
cls traing
Training classifier loss= 0.7134
seen=0.5062
[1200/3001] Loss_D: 0.0728 Loss_G: -0.3970, Wasserstein_dist: 1.1997, real_ins_contras_loss: 1.2197,fake_ins_contras_loss : 3.9352
[1250/3001] Loss_D: 0.0366 Loss_G: -0.3494, Wasserstein_dist: 1.2334, real_ins_contras_loss: 1.2113,fake_ins_contras_loss : 3.7996
[1300/3001] Loss_D: 0.0815 Loss_G: -0.2053, Wasserstein_dist: 1.2667, real_ins_contras_loss: 1.2590,fake_ins_contras_loss : 3.7718
[1350/3001] Loss_D: 0.0161 Loss_G: -0.3272, Wasserstein_dist: 1.3032, real_ins_contras_loss: 1.2380,fake_ins_contras_loss : 3.6353
model 0 is saved
cls traing
Training classifier loss= 0.6253
seen=0.5169
[1400/3001] Loss_D: 0.0576 Loss_G: -0.3589, Wasserstein_dist: 1.2670, real_ins_contras_loss: 1.2642,fake_ins_contras_loss : 3.5274
[1450/3001] Loss_D: -0.0157 Loss_G: -0.2105, Wasserstein_dist: 1.2868, real_ins_contras_loss: 1.2069,fake_ins_contras_loss : 3.4092
[1500/3001] Loss_D: 0.0635 Loss_G: -0.1875, Wasserstein_dist: 1.2508, real_ins_contras_loss: 1.2440,fake_ins_contras_loss : 3.3345
[1550/3001] Loss_D: -0.0285 Loss_G: -0.2244, Wasserstein_dist: 1.3156, real_ins_contras_loss: 1.2122,fake_ins_contras_loss : 3.3330
model 0 is saved
cls traing
Training classifier loss= 0.4599
seen=0.5308
[1600/3001] Loss_D: -0.0510 Loss_G: -0.0903, Wasserstein_dist: 1.3521, real_ins_contras_loss: 1.2165,fake_ins_contras_loss : 3.3035
[1650/3001] Loss_D: -0.0819 Loss_G: 0.0371, Wasserstein_dist: 1.3366, real_ins_contras_loss: 1.1743,fake_ins_contras_loss : 3.1624
[1700/3001] Loss_D: -0.0368 Loss_G: -0.0732, Wasserstein_dist: 1.3413, real_ins_contras_loss: 1.2210,fake_ins_contras_loss : 3.1055
[1750/3001] Loss_D: -0.0444 Loss_G: -0.0602, Wasserstein_dist: 1.3286, real_ins_contras_loss: 1.2176,fake_ins_contras_loss : 3.1205
model 0 is saved
cls traing
Training classifier loss= 0.4505
seen=0.5492
[1800/3001] Loss_D: -0.1264 Loss_G: 0.0682, Wasserstein_dist: 1.4003, real_ins_contras_loss: 1.1889,fake_ins_contras_loss : 3.0411
[1850/3001] Loss_D: -0.1244 Loss_G: 0.1384, Wasserstein_dist: 1.3985, real_ins_contras_loss: 1.2005,fake_ins_contras_loss : 2.8949
[1900/3001] Loss_D: 0.0247 Loss_G: 0.1758, Wasserstein_dist: 1.2765, real_ins_contras_loss: 1.2229,fake_ins_contras_loss : 2.9166
[1950/3001] Loss_D: -0.0212 Loss_G: 0.2889, Wasserstein_dist: 1.3424, real_ins_contras_loss: 1.2375,fake_ins_contras_loss : 2.8715
model 0 is saved
cls traing
Training classifier loss= 0.3448
seen=0.5400
[2000/3001] Loss_D: -0.1256 Loss_G: 0.3184, Wasserstein_dist: 1.4405, real_ins_contras_loss: 1.2235,fake_ins_contras_loss : 2.8414
[2050/3001] Loss_D: -0.0682 Loss_G: 0.3444, Wasserstein_dist: 1.4238, real_ins_contras_loss: 1.2398,fake_ins_contras_loss : 2.8138
[2100/3001] Loss_D: -0.2163 Loss_G: 0.3635, Wasserstein_dist: 1.5135, real_ins_contras_loss: 1.1869,fake_ins_contras_loss : 2.7209
[2150/3001] Loss_D: -0.1060 Loss_G: 0.4842, Wasserstein_dist: 1.4697, real_ins_contras_loss: 1.2584,fake_ins_contras_loss : 2.7610
model 0 is saved
cls traing
Training classifier loss= 0.3177
seen=0.5431
[2200/3001] Loss_D: -0.1461 Loss_G: 0.5445, Wasserstein_dist: 1.4582, real_ins_contras_loss: 1.2070,fake_ins_contras_loss : 2.7523
[2250/3001] Loss_D: -0.1896 Loss_G: 0.5976, Wasserstein_dist: 1.5166, real_ins_contras_loss: 1.2069,fake_ins_contras_loss : 2.6226
[2300/3001] Loss_D: -0.1431 Loss_G: 0.5524, Wasserstein_dist: 1.5030, real_ins_contras_loss: 1.2497,fake_ins_contras_loss : 2.5847
[2350/3001] Loss_D: -0.1874 Loss_G: 0.5594, Wasserstein_dist: 1.4970, real_ins_contras_loss: 1.2026,fake_ins_contras_loss : 2.5425
model 0 is saved
cls traing
Training classifier loss= 0.2884
seen=0.5523
[2400/3001] Loss_D: -0.1637 Loss_G: 0.6568, Wasserstein_dist: 1.4979, real_ins_contras_loss: 1.2162,fake_ins_contras_loss : 2.5546
[2450/3001] Loss_D: -0.2141 Loss_G: 0.8165, Wasserstein_dist: 1.5206, real_ins_contras_loss: 1.1956,fake_ins_contras_loss : 2.4652
[2500/3001] Loss_D: -0.2187 Loss_G: 0.7650, Wasserstein_dist: 1.5359, real_ins_contras_loss: 1.1954,fake_ins_contras_loss : 2.5088
[2550/3001] Loss_D: -0.2071 Loss_G: 0.9832, Wasserstein_dist: 1.5502, real_ins_contras_loss: 1.2127,fake_ins_contras_loss : 2.4895
model 0 is saved
cls traing
Training classifier loss= 0.2697
seen=0.5615
[2600/3001] Loss_D: -0.2373 Loss_G: 0.6317, Wasserstein_dist: 1.5435, real_ins_contras_loss: 1.1918,fake_ins_contras_loss : 2.4188
[2650/3001] Loss_D: -0.1892 Loss_G: 0.8838, Wasserstein_dist: 1.5332, real_ins_contras_loss: 1.2260,fake_ins_contras_loss : 2.4366
[2700/3001] Loss_D: -0.2460 Loss_G: 1.1139, Wasserstein_dist: 1.5783, real_ins_contras_loss: 1.2074,fake_ins_contras_loss : 2.3988
[2750/3001] Loss_D: -0.1667 Loss_G: 0.9262, Wasserstein_dist: 1.5428, real_ins_contras_loss: 1.2545,fake_ins_contras_loss : 2.3602
model 0 is saved
cls traing
Training classifier loss= 0.2940
seen=0.5615
[2800/3001] Loss_D: -0.2089 Loss_G: 0.9886, Wasserstein_dist: 1.5665, real_ins_contras_loss: 1.2376,fake_ins_contras_loss : 2.3964
[2850/3001] Loss_D: -0.2092 Loss_G: 1.0602, Wasserstein_dist: 1.6161, real_ins_contras_loss: 1.2662,fake_ins_contras_loss : 2.3318
[2900/3001] Loss_D: -0.2681 Loss_G: 0.9123, Wasserstein_dist: 1.5680, real_ins_contras_loss: 1.1803,fake_ins_contras_loss : 2.3275
[2950/3001] Loss_D: -0.2732 Loss_G: 1.2792, Wasserstein_dist: 1.6067, real_ins_contras_loss: 1.1889,fake_ins_contras_loss : 2.3149
model 0 is saved
cls traing
Training classifier loss= 0.2365
seen=0.5692
[3000/3001] Loss_D: -0.2441 Loss_G: 1.1648, Wasserstein_dist: 1.5775, real_ins_contras_loss: 1.2069,fake_ins_contras_loss : 2.2753
average_acc is 0.49010989010989
----------------------------------------
loading task 1th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258]
current seen_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43]
curr_unseen_label is [358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315]
pre_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276]
finish pretraining cgan model ···
incremental training task 1···
loading task 1 data and the pre model
model 0 is loaded
kl_loss G is 0.00893687829375267
kl_loss G is 0.005806799046695232
kl_loss G is 0.0036640851758420467
kl_loss G is 0.0026893517933785915
[0/3001] Loss_D: 9.6999 Loss_G: 1.1746, Wasserstein_dist: 0.1122, real_ins_contras_loss: 4.4428,fake_ins_contras_loss : 8.3261, CL_contra_loss : 0.4969
model 1 is saved
cls traing
unseen=0.5079, seen=0.2942, h=0.3726
[100/3001] Loss_D: 0.4757 Loss_G: 1.9436, Wasserstein_dist: 1.3769, real_ins_contras_loss: 1.3002,fake_ins_contras_loss : 8.3071, CL_contra_loss : 0.0462
[200/3001] Loss_D: 0.1285 Loss_G: 2.1930, Wasserstein_dist: 1.6107, real_ins_contras_loss: 1.2611,fake_ins_contras_loss : 8.2299, CL_contra_loss : 0.0357
model 1 is saved
cls traing
unseen=0.5291, seen=0.3367, h=0.4115
[300/3001] Loss_D: -0.1079 Loss_G: 2.2537, Wasserstein_dist: 1.7954, real_ins_contras_loss: 1.2592,fake_ins_contras_loss : 8.3626, CL_contra_loss : 0.0319
[400/3001] Loss_D: -0.1643 Loss_G: 2.3651, Wasserstein_dist: 1.8314, real_ins_contras_loss: 1.2362,fake_ins_contras_loss : 8.3372, CL_contra_loss : 0.0317
model 1 is saved
cls traing
unseen=0.5291, seen=0.3336, h=0.4092
kl_loss G is 0.005584758706390858
kl_loss G is 0.0056331646628677845
kl_loss G is 0.0056337034329771996
kl_loss G is 0.0056016952730715275
[500/3001] Loss_D: -0.2085 Loss_G: 2.3185, Wasserstein_dist: 1.9128, real_ins_contras_loss: 1.2675,fake_ins_contras_loss : 8.3983, CL_contra_loss : 0.0291
[600/3001] Loss_D: -0.3471 Loss_G: 2.2099, Wasserstein_dist: 1.9940, real_ins_contras_loss: 1.1999,fake_ins_contras_loss : 8.2572, CL_contra_loss : 0.0288
model 1 is saved
cls traing
unseen=0.4815, seen=0.3691, h=0.4179
[700/3001] Loss_D: -0.4027 Loss_G: 2.3145, Wasserstein_dist: 2.0513, real_ins_contras_loss: 1.2287,fake_ins_contras_loss : 8.3368, CL_contra_loss : 0.0282
[800/3001] Loss_D: -0.4829 Loss_G: 2.2832, Wasserstein_dist: 2.1072, real_ins_contras_loss: 1.2175,fake_ins_contras_loss : 8.3661, CL_contra_loss : 0.0275
model 1 is saved
cls traing
unseen=0.4815, seen=0.3761, h=0.4223
[900/3001] Loss_D: -0.4430 Loss_G: 2.4241, Wasserstein_dist: 2.0928, real_ins_contras_loss: 1.2371,fake_ins_contras_loss : 8.2803, CL_contra_loss : 0.0274
kl_loss G is 0.00620447751134634
kl_loss G is 0.006150864530354738
kl_loss G is 0.006139540113508701
kl_loss G is 0.00620428379625082
[1000/3001] Loss_D: -0.4826 Loss_G: 2.4190, Wasserstein_dist: 2.1091, real_ins_contras_loss: 1.2376,fake_ins_contras_loss : 8.3416, CL_contra_loss : 0.0289
model 1 is saved
cls traing
unseen=0.4630, seen=0.3931, h=0.4252
[1100/3001] Loss_D: -0.5076 Loss_G: 2.4505, Wasserstein_dist: 2.1270, real_ins_contras_loss: 1.2335,fake_ins_contras_loss : 8.3731, CL_contra_loss : 0.0250
[1200/3001] Loss_D: -0.5430 Loss_G: 2.5840, Wasserstein_dist: 2.1554, real_ins_contras_loss: 1.2423,fake_ins_contras_loss : 8.2948, CL_contra_loss : 0.0236
model 1 is saved
cls traing
unseen=0.4868, seen=0.3923, h=0.4344
[1300/3001] Loss_D: -0.5370 Loss_G: 2.6223, Wasserstein_dist: 2.1478, real_ins_contras_loss: 1.2463,fake_ins_contras_loss : 8.3360, CL_contra_loss : 0.0247
[1400/3001] Loss_D: -0.5416 Loss_G: 2.6913, Wasserstein_dist: 2.1770, real_ins_contras_loss: 1.2784,fake_ins_contras_loss : 8.2880, CL_contra_loss : 0.0202
model 1 is saved
cls traing
unseen=0.4815, seen=0.4093, h=0.4424
kl_loss G is 0.006499536335468292
kl_loss G is 0.006358347833156586
kl_loss G is 0.006259220652282238
kl_loss G is 0.006383524276316166
[1500/3001] Loss_D: -0.5793 Loss_G: 2.7867, Wasserstein_dist: 2.1637, real_ins_contras_loss: 1.2230,fake_ins_contras_loss : 8.2427, CL_contra_loss : 0.0222
[1600/3001] Loss_D: -0.6479 Loss_G: 2.8177, Wasserstein_dist: 2.2294, real_ins_contras_loss: 1.2140,fake_ins_contras_loss : 8.2981, CL_contra_loss : 0.0215
model 1 is saved
cls traing
unseen=0.5106, seen=0.3861, h=0.4397
[1700/3001] Loss_D: -0.5798 Loss_G: 2.7742, Wasserstein_dist: 2.1965, real_ins_contras_loss: 1.2697,fake_ins_contras_loss : 8.3403, CL_contra_loss : 0.0216
[1800/3001] Loss_D: -0.6730 Loss_G: 2.8657, Wasserstein_dist: 2.2311, real_ins_contras_loss: 1.2110,fake_ins_contras_loss : 8.2376, CL_contra_loss : 0.0207
model 1 is saved
cls traing
unseen=0.4788, seen=0.4039, h=0.4382
[1900/3001] Loss_D: -0.6355 Loss_G: 2.9933, Wasserstein_dist: 2.1643, real_ins_contras_loss: 1.2028,fake_ins_contras_loss : 8.2246, CL_contra_loss : 0.0193
kl_loss G is 0.006675021722912788
kl_loss G is 0.006488841492682695
kl_loss G is 0.006670375354588032
kl_loss G is 0.006656414829194546
[2000/3001] Loss_D: -0.6050 Loss_G: 3.0453, Wasserstein_dist: 2.1852, real_ins_contras_loss: 1.2369,fake_ins_contras_loss : 8.2438, CL_contra_loss : 0.0209
model 1 is saved
cls traing
unseen=0.5106, seen=0.3861, h=0.4397
[2100/3001] Loss_D: -0.6830 Loss_G: 3.0013, Wasserstein_dist: 2.2241, real_ins_contras_loss: 1.2077,fake_ins_contras_loss : 8.1868, CL_contra_loss : 0.0171
[2200/3001] Loss_D: -0.6590 Loss_G: 3.1224, Wasserstein_dist: 2.2406, real_ins_contras_loss: 1.2686,fake_ins_contras_loss : 8.3532, CL_contra_loss : 0.0187
model 1 is saved
cls traing
unseen=0.4974, seen=0.3946, h=0.4401
[2300/3001] Loss_D: -0.7375 Loss_G: 3.1940, Wasserstein_dist: 2.2352, real_ins_contras_loss: 1.1859,fake_ins_contras_loss : 8.2794, CL_contra_loss : 0.0177
[2400/3001] Loss_D: -0.6384 Loss_G: 3.2067, Wasserstein_dist: 2.1992, real_ins_contras_loss: 1.2341,fake_ins_contras_loss : 8.3287, CL_contra_loss : 0.0161
model 1 is saved
cls traing
unseen=0.4921, seen=0.3969, h=0.4394
kl_loss G is 0.006596207618713379
kl_loss G is 0.006598370149731636
kl_loss G is 0.006570040713995695
kl_loss G is 0.006679530721157789
[2500/3001] Loss_D: -0.7070 Loss_G: 3.2981, Wasserstein_dist: 2.2329, real_ins_contras_loss: 1.2175,fake_ins_contras_loss : 8.2758, CL_contra_loss : 0.0168
[2600/3001] Loss_D: -0.6926 Loss_G: 3.2574, Wasserstein_dist: 2.2189, real_ins_contras_loss: 1.2282,fake_ins_contras_loss : 8.2570, CL_contra_loss : 0.0188
model 1 is saved
cls traing
unseen=0.4815, seen=0.3977, h=0.4356
[2700/3001] Loss_D: -0.6339 Loss_G: 3.3150, Wasserstein_dist: 2.2040, real_ins_contras_loss: 1.2264,fake_ins_contras_loss : 8.2644, CL_contra_loss : 0.0178
[2800/3001] Loss_D: -0.6356 Loss_G: 3.2986, Wasserstein_dist: 2.1896, real_ins_contras_loss: 1.2540,fake_ins_contras_loss : 8.2675, CL_contra_loss : 0.0150
model 1 is saved
cls traing
unseen=0.4762, seen=0.4093, h=0.4402
[2900/3001] Loss_D: -0.7271 Loss_G: 3.5523, Wasserstein_dist: 2.2162, real_ins_contras_loss: 1.2083,fake_ins_contras_loss : 8.2395, CL_contra_loss : 0.0172
kl_loss G is 0.006664702203124762
kl_loss G is 0.006502415984869003
kl_loss G is 0.006683026906102896
kl_loss G is 0.006744484417140484
[3000/3001] Loss_D: -0.6786 Loss_G: 3.5697, Wasserstein_dist: 2.1764, real_ins_contras_loss: 1.2032,fake_ins_contras_loss : 8.2398, CL_contra_loss : 0.0178
model 1 is saved
cls traing
unseen=0.4577, seen=0.4162, h=0.4360
average_acc is 0.4314449490484635
 hightest s is 0.4092664092664092, u is 0.48148148148148145, H is 0.4424466206453684,
----------------------------------------
loading task 2th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405]
current seen_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100, 490, 21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268, 447, 431, 703, 572, 667, 50, 504, 352, 590, 359, 365, 500, 6, 274, 280, 270, 258, 713, 548, 648, 211, 514, 4, 52, 607, 233, 60, 165, 253, 316, 709, 135, 698, 227, 86, 355, 435, 675, 541, 202, 434, 140, 540, 66, 213, 588, 550, 229, 235, 295, 395, 201, 42, 542, 116, 306, 444, 101, 232, 391, 685, 93, 680, 510, 454, 154, 264, 544, 109, 285, 642, 236, 427, 291, 181, 207, 65, 44, 594, 222, 22, 203, 399, 20, 107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292, 33, 526, 507, 90]
curr_unseen_label is [358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315, 95, 72, 216, 10, 711, 152, 471, 509, 658, 631, 130, 420, 57, 124, 158, 679, 493, 342]
pre_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315, 537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100, 490, 21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268, 447, 431, 703, 572, 667, 50, 504, 352, 590, 359, 365, 500, 6, 274, 280, 270, 258, 713, 548, 648, 211, 514, 4, 52, 607, 233, 60, 165, 253, 316, 709, 135, 698, 227, 86, 355, 435, 675, 541, 202, 434, 140, 540, 66, 213, 588, 550, 229, 235, 295, 395, 201, 42, 542, 116, 306, 444, 101, 232, 391, 685, 93, 680, 510, 454, 154, 264, 544, 109, 285, 642, 236, 427, 291, 181, 207, 65, 44, 594, 222, 22, 203, 399, 20, 107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292, 33, 526, 507, 90, 95, 72, 216, 10, 711, 152, 471, 509, 658, 631, 130, 420, 57, 124, 158, 679, 493, 342]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423]
incremental training task 2···
loading task 2 data and the pre model
model 1 is loaded
kl_loss G is 0.0
kl_loss G is 0.0028988232370465994
kl_loss G is 0.0016500127967447042
kl_loss G is 0.0015038587152957916
[0/3001] Loss_D: 4.6579 Loss_G: 3.6745, Wasserstein_dist: 0.7838, real_ins_contras_loss: 4.6611,fake_ins_contras_loss : 8.0315, CL_contra_loss : 0.0684
model 2 is saved
cls traing
unseen=0.4550, seen=0.2856, h=0.3509
[100/3001] Loss_D: -0.0346 Loss_G: 2.8263, Wasserstein_dist: 1.6681, real_ins_contras_loss: 1.3146,fake_ins_contras_loss : 7.8757, CL_contra_loss : 0.0278
[200/3001] Loss_D: -0.3605 Loss_G: 2.6472, Wasserstein_dist: 1.8409, real_ins_contras_loss: 1.2465,fake_ins_contras_loss : 7.9311, CL_contra_loss : 0.0142
model 2 is saved
cls traing
unseen=0.4656, seen=0.3072, h=0.3702
[300/3001] Loss_D: -0.3955 Loss_G: 2.6726, Wasserstein_dist: 1.9016, real_ins_contras_loss: 1.2697,fake_ins_contras_loss : 7.9645, CL_contra_loss : 0.0170
[400/3001] Loss_D: -0.5163 Loss_G: 2.7393, Wasserstein_dist: 1.9974, real_ins_contras_loss: 1.2153,fake_ins_contras_loss : 7.9345, CL_contra_loss : 0.0157
model 2 is saved
cls traing
unseen=0.4418, seen=0.3253, h=0.3747
kl_loss G is 0.004917607642710209
kl_loss G is 0.00495634600520134
kl_loss G is 0.0049567921087145805
kl_loss G is 0.004920442588627338
[500/3001] Loss_D: -0.5677 Loss_G: 2.8844, Wasserstein_dist: 2.0318, real_ins_contras_loss: 1.2068,fake_ins_contras_loss : 7.9916, CL_contra_loss : 0.0161
[600/3001] Loss_D: -0.5583 Loss_G: 2.9944, Wasserstein_dist: 2.0441, real_ins_contras_loss: 1.2314,fake_ins_contras_loss : 7.9552, CL_contra_loss : 0.0147
model 2 is saved
cls traing
unseen=0.4471, seen=0.3247, h=0.3762
[700/3001] Loss_D: -0.5844 Loss_G: 3.1636, Wasserstein_dist: 2.0785, real_ins_contras_loss: 1.2391,fake_ins_contras_loss : 8.0590, CL_contra_loss : 0.0137
[800/3001] Loss_D: -0.6852 Loss_G: 3.3066, Wasserstein_dist: 2.1377, real_ins_contras_loss: 1.1988,fake_ins_contras_loss : 7.9756, CL_contra_loss : 0.0146
model 2 is saved
cls traing
unseen=0.4206, seen=0.3284, h=0.3688
[900/3001] Loss_D: -0.6752 Loss_G: 3.3726, Wasserstein_dist: 2.1279, real_ins_contras_loss: 1.1977,fake_ins_contras_loss : 7.9809, CL_contra_loss : 0.0172
kl_loss G is 0.005710419733077288
kl_loss G is 0.005609962157905102
kl_loss G is 0.005658549256622791
kl_loss G is 0.005551612935960293
[1000/3001] Loss_D: -0.5878 Loss_G: 3.5889, Wasserstein_dist: 2.1018, real_ins_contras_loss: 1.2639,fake_ins_contras_loss : 8.1062, CL_contra_loss : 0.0152
model 2 is saved
cls traing
unseen=0.4325, seen=0.3392, h=0.3802
[1100/3001] Loss_D: -0.6270 Loss_G: 3.5501, Wasserstein_dist: 2.1108, real_ins_contras_loss: 1.2143,fake_ins_contras_loss : 8.0041, CL_contra_loss : 0.0168
[1200/3001] Loss_D: -0.5889 Loss_G: 3.8174, Wasserstein_dist: 2.1047, real_ins_contras_loss: 1.2456,fake_ins_contras_loss : 8.0995, CL_contra_loss : 0.0129
model 2 is saved
cls traing
unseen=0.4114, seen=0.3454, h=0.3755
[1300/3001] Loss_D: -0.6688 Loss_G: 3.8247, Wasserstein_dist: 2.1177, real_ins_contras_loss: 1.1918,fake_ins_contras_loss : 8.0507, CL_contra_loss : 0.0129
[1400/3001] Loss_D: -0.6425 Loss_G: 3.8948, Wasserstein_dist: 2.1396, real_ins_contras_loss: 1.2401,fake_ins_contras_loss : 8.0242, CL_contra_loss : 0.0143
model 2 is saved
cls traing
unseen=0.4577, seen=0.3278, h=0.3820
kl_loss G is 0.005711027421057224
kl_loss G is 0.005781723186373711
kl_loss G is 0.005714965518563986
kl_loss G is 0.005730847362428904
[1500/3001] Loss_D: -0.6662 Loss_G: 4.1092, Wasserstein_dist: 2.1775, real_ins_contras_loss: 1.2346,fake_ins_contras_loss : 8.1011, CL_contra_loss : 0.0148
[1600/3001] Loss_D: -0.6269 Loss_G: 4.0387, Wasserstein_dist: 2.1258, real_ins_contras_loss: 1.2565,fake_ins_contras_loss : 8.0135, CL_contra_loss : 0.0115
model 2 is saved
cls traing
unseen=0.4259, seen=0.3443, h=0.3808
[1700/3001] Loss_D: -0.6310 Loss_G: 4.0717, Wasserstein_dist: 2.1757, real_ins_contras_loss: 1.2597,fake_ins_contras_loss : 8.0159, CL_contra_loss : 0.0139
[1800/3001] Loss_D: -0.7210 Loss_G: 4.1239, Wasserstein_dist: 2.1900, real_ins_contras_loss: 1.2258,fake_ins_contras_loss : 7.9755, CL_contra_loss : 0.0117
model 2 is saved
cls traing
unseen=0.4365, seen=0.3418, h=0.3834
[1900/3001] Loss_D: -0.6930 Loss_G: 4.1683, Wasserstein_dist: 2.1637, real_ins_contras_loss: 1.2256,fake_ins_contras_loss : 7.9853, CL_contra_loss : 0.0121
kl_loss G is 0.0059874095022678375
kl_loss G is 0.005925402510911226
kl_loss G is 0.005949100479483604
kl_loss G is 0.005974010564386845
[2000/3001] Loss_D: -0.7031 Loss_G: 4.3980, Wasserstein_dist: 2.2123, real_ins_contras_loss: 1.2638,fake_ins_contras_loss : 7.9586, CL_contra_loss : 0.0109
model 2 is saved
cls traing
unseen=0.4497, seen=0.3387, h=0.3864
[2100/3001] Loss_D: -0.6843 Loss_G: 4.3951, Wasserstein_dist: 2.1339, real_ins_contras_loss: 1.2185,fake_ins_contras_loss : 7.9180, CL_contra_loss : 0.0128
[2200/3001] Loss_D: -0.7327 Loss_G: 4.2450, Wasserstein_dist: 2.1771, real_ins_contras_loss: 1.1973,fake_ins_contras_loss : 8.0059, CL_contra_loss : 0.0093
model 2 is saved
cls traing
unseen=0.4233, seen=0.3479, h=0.3819
[2300/3001] Loss_D: -0.6474 Loss_G: 4.3368, Wasserstein_dist: 2.1305, real_ins_contras_loss: 1.2662,fake_ins_contras_loss : 8.1010, CL_contra_loss : 0.0095
[2400/3001] Loss_D: -0.7358 Loss_G: 4.4955, Wasserstein_dist: 2.1701, real_ins_contras_loss: 1.2100,fake_ins_contras_loss : 8.0259, CL_contra_loss : 0.0122
model 2 is saved
cls traing
unseen=0.4497, seen=0.3423, h=0.3887
kl_loss G is 0.0060324459336698055
kl_loss G is 0.005925407633185387
kl_loss G is 0.005995458923280239
kl_loss G is 0.0059660254046320915
[2500/3001] Loss_D: -0.6435 Loss_G: 4.4569, Wasserstein_dist: 2.1717, real_ins_contras_loss: 1.2636,fake_ins_contras_loss : 8.0167, CL_contra_loss : 0.0118
[2600/3001] Loss_D: -0.7555 Loss_G: 4.3791, Wasserstein_dist: 2.1951, real_ins_contras_loss: 1.2103,fake_ins_contras_loss : 7.9923, CL_contra_loss : 0.0095
model 2 is saved
cls traing
unseen=0.4378, seen=0.3423, h=0.3842
[2700/3001] Loss_D: -0.7346 Loss_G: 4.4740, Wasserstein_dist: 2.1844, real_ins_contras_loss: 1.2307,fake_ins_contras_loss : 7.9281, CL_contra_loss : 0.0080
[2800/3001] Loss_D: -0.6635 Loss_G: 4.6411, Wasserstein_dist: 2.1376, real_ins_contras_loss: 1.2534,fake_ins_contras_loss : 8.0360, CL_contra_loss : 0.0095
model 2 is saved
cls traing
unseen=0.4352, seen=0.3469, h=0.3861
[2900/3001] Loss_D: -0.7218 Loss_G: 4.6604, Wasserstein_dist: 2.2018, real_ins_contras_loss: 1.2173,fake_ins_contras_loss : 8.1088, CL_contra_loss : 0.0097
kl_loss G is 0.005910900887101889
kl_loss G is 0.00592919671908021
kl_loss G is 0.006103529129177332
kl_loss G is 0.006108368746936321
[3000/3001] Loss_D: -0.6350 Loss_G: 4.5405, Wasserstein_dist: 2.1524, real_ins_contras_loss: 1.2651,fake_ins_contras_loss : 7.9633, CL_contra_loss : 0.0109
model 2 is saved
cls traing
unseen=0.4444, seen=0.3366, h=0.3831
average_acc is 0.3801413704841066
 hightest s is 0.3422680412371136, u is 0.4497354497354497, H is 0.38871058830011446,
----------------------------------------
loading task 3th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552]
current seen_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100, 490, 21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268, 447, 431, 703, 572, 667, 50, 504, 352, 590, 359, 365, 500, 6, 274, 280, 270, 258, 713, 548, 648, 211, 514, 4, 52, 607, 233, 60, 165, 253, 316, 709, 135, 698, 227, 86, 355, 435, 675, 541, 202, 434, 140, 540, 66, 213, 588, 550, 229, 235, 295, 395, 201, 42, 542, 116, 306, 444, 101, 232, 391, 685, 93, 680, 510, 454, 154, 264, 544, 109, 285, 642, 236, 427, 291, 181, 207, 65, 44, 594, 222, 22, 203, 399, 20, 107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292, 33, 526, 507, 90, 499, 14, 686, 79, 539, 404, 348, 212, 457, 553, 410, 408, 139, 633, 94, 191, 287, 651, 18, 123, 345, 149, 321, 46, 519, 36, 664, 92, 636, 567, 591, 269, 589, 67, 76, 230, 414, 282, 655, 702, 533, 647, 568, 9, 418, 170, 364, 480, 351, 474, 41, 279, 91, 188, 251, 275, 522, 547, 296, 411, 119, 389, 523, 356, 45, 363, 332, 290, 505, 277, 466, 146, 663, 225, 126, 361, 419, 17, 666, 179, 386, 29, 88, 255, 570, 132, 531, 536, 327, 646, 248, 111, 48, 250, 13, 310, 382, 330, 473, 177, 429, 15, 157, 384, 71, 39, 630, 556, 543, 450, 700, 661, 273, 309, 397, 634, 77, 304, 81, 623, 125, 452, 691, 218, 579, 494, 625, 436, 70]
curr_unseen_label is [358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315, 95, 72, 216, 10, 711, 152, 471, 509, 658, 631, 130, 420, 57, 124, 158, 679, 493, 342, 138, 286, 650, 336, 259, 221, 262, 425, 560, 99, 23, 508, 112, 635, 580, 24, 710, 656]
pre_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315, 537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100, 490, 21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268, 447, 431, 703, 572, 667, 50, 504, 352, 590, 359, 365, 500, 6, 274, 280, 270, 258, 713, 548, 648, 211, 514, 4, 52, 607, 233, 60, 165, 253, 316, 709, 135, 698, 227, 86, 355, 435, 675, 541, 202, 434, 140, 540, 66, 213, 588, 550, 229, 235, 295, 395, 201, 42, 542, 116, 306, 444, 101, 232, 391, 685, 93, 680, 510, 454, 154, 264, 544, 109, 285, 642, 236, 427, 291, 181, 207, 65, 44, 594, 222, 22, 203, 399, 20, 107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292, 33, 526, 507, 90, 95, 72, 216, 10, 711, 152, 471, 509, 658, 631, 130, 420, 57, 124, 158, 679, 493, 342, 499, 14, 686, 79, 539, 404, 348, 212, 457, 553, 410, 408, 139, 633, 94, 191, 287, 651, 18, 123, 345, 149, 321, 46, 519, 36, 664, 92, 636, 567, 591, 269, 589, 67, 76, 230, 414, 282, 655, 702, 533, 647, 568, 9, 418, 170, 364, 480, 351, 474, 41, 279, 91, 188, 251, 275, 522, 547, 296, 411, 119, 389, 523, 356, 45, 363, 332, 290, 505, 277, 466, 146, 663, 225, 126, 361, 419, 17, 666, 179, 386, 29, 88, 255, 570, 132, 531, 536, 327, 646, 248, 111, 48, 250, 13, 310, 382, 330, 473, 177, 429, 15, 157, 384, 71, 39, 630, 556, 543, 450, 700, 661, 273, 309, 397, 634, 77, 304, 81, 623, 125, 452, 691, 218, 579, 494, 625, 436, 70, 138, 286, 650, 336, 259, 221, 262, 425, 560, 99, 23, 508, 112, 635, 580, 24, 710, 656]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570]
incremental training task 3···
loading task 3 data and the pre model
model 2 is loaded
kl_loss G is 0.0
kl_loss G is 0.0029671990778297186
kl_loss G is 0.001641619368456304
kl_loss G is 0.0015046033076941967
[0/3001] Loss_D: 4.5250 Loss_G: 4.7127, Wasserstein_dist: 0.5817, real_ins_contras_loss: 4.5188,fake_ins_contras_loss : 8.0076, CL_contra_loss : 0.0504
model 3 is saved
cls traing
unseen=0.3898, seen=0.2809, h=0.3265
[100/3001] Loss_D: -0.1042 Loss_G: 3.8372, Wasserstein_dist: 1.5390, real_ins_contras_loss: 1.2389,fake_ins_contras_loss : 7.7859, CL_contra_loss : 0.0137
[200/3001] Loss_D: -0.2881 Loss_G: 3.7678, Wasserstein_dist: 1.6811, real_ins_contras_loss: 1.2088,fake_ins_contras_loss : 7.8669, CL_contra_loss : 0.0117
model 3 is saved
cls traing
unseen=0.3801, seen=0.2870, h=0.3271
[300/3001] Loss_D: -0.4259 Loss_G: 3.6960, Wasserstein_dist: 1.8102, real_ins_contras_loss: 1.2137,fake_ins_contras_loss : 7.8623, CL_contra_loss : 0.0086
[400/3001] Loss_D: -0.3858 Loss_G: 3.8047, Wasserstein_dist: 1.8259, real_ins_contras_loss: 1.2459,fake_ins_contras_loss : 7.9426, CL_contra_loss : 0.0082
model 3 is saved
cls traing
unseen=0.3924, seen=0.2925, h=0.3351
kl_loss G is 0.0042039621621370316
kl_loss G is 0.0042080990970134735
kl_loss G is 0.004182134754955769
kl_loss G is 0.004223888739943504
[500/3001] Loss_D: -0.4197 Loss_G: 3.9003, Wasserstein_dist: 1.8720, real_ins_contras_loss: 1.2602,fake_ins_contras_loss : 7.9551, CL_contra_loss : 0.0083
[600/3001] Loss_D: -0.4442 Loss_G: 3.9766, Wasserstein_dist: 1.8795, real_ins_contras_loss: 1.2442,fake_ins_contras_loss : 8.0143, CL_contra_loss : 0.0097
model 3 is saved
cls traing
unseen=0.4012, seen=0.2843, h=0.3328
[700/3001] Loss_D: -0.4954 Loss_G: 4.1261, Wasserstein_dist: 1.9167, real_ins_contras_loss: 1.2481,fake_ins_contras_loss : 7.9185, CL_contra_loss : 0.0074
[800/3001] Loss_D: -0.5388 Loss_G: 4.2447, Wasserstein_dist: 1.9712, real_ins_contras_loss: 1.2450,fake_ins_contras_loss : 7.9987, CL_contra_loss : 0.0090
model 3 is saved
cls traing
unseen=0.4295, seen=0.2774, h=0.3370
[900/3001] Loss_D: -0.5998 Loss_G: 4.3426, Wasserstein_dist: 2.0109, real_ins_contras_loss: 1.2086,fake_ins_contras_loss : 7.8829, CL_contra_loss : 0.0087
kl_loss G is 0.004754456225782633
kl_loss G is 0.004707396496087313
kl_loss G is 0.004764712415635586
kl_loss G is 0.004738565068691969
[1000/3001] Loss_D: -0.5936 Loss_G: 4.3249, Wasserstein_dist: 2.0064, real_ins_contras_loss: 1.2247,fake_ins_contras_loss : 7.9097, CL_contra_loss : 0.0080
model 3 is saved
cls traing
unseen=0.4012, seen=0.2843, h=0.3328
[1100/3001] Loss_D: -0.5851 Loss_G: 4.5316, Wasserstein_dist: 1.9783, real_ins_contras_loss: 1.2220,fake_ins_contras_loss : 7.9093, CL_contra_loss : 0.0070
[1200/3001] Loss_D: -0.6003 Loss_G: 4.3804, Wasserstein_dist: 2.0390, real_ins_contras_loss: 1.2316,fake_ins_contras_loss : 7.9660, CL_contra_loss : 0.0090
model 3 is saved
cls traing
unseen=0.4065, seen=0.2843, h=0.3346
[1300/3001] Loss_D: -0.6474 Loss_G: 4.6136, Wasserstein_dist: 2.0439, real_ins_contras_loss: 1.2101,fake_ins_contras_loss : 7.9355, CL_contra_loss : 0.0073
[1400/3001] Loss_D: -0.5788 Loss_G: 4.6297, Wasserstein_dist: 2.0281, real_ins_contras_loss: 1.2589,fake_ins_contras_loss : 7.9574, CL_contra_loss : 0.0073
model 3 is saved
cls traing
unseen=0.4145, seen=0.2828, h=0.3362
kl_loss G is 0.0048547666519880295
kl_loss G is 0.004885025322437286
kl_loss G is 0.004965859465301037
kl_loss G is 0.004838629625737667
[1500/3001] Loss_D: -0.7113 Loss_G: 4.6618, Wasserstein_dist: 2.0965, real_ins_contras_loss: 1.1994,fake_ins_contras_loss : 7.9632, CL_contra_loss : 0.0058
[1600/3001] Loss_D: -0.6093 Loss_G: 4.7956, Wasserstein_dist: 2.0502, real_ins_contras_loss: 1.2368,fake_ins_contras_loss : 7.8976, CL_contra_loss : 0.0077
model 3 is saved
cls traing
unseen=0.3854, seen=0.2994, h=0.3370
[1700/3001] Loss_D: -0.5465 Loss_G: 4.7470, Wasserstein_dist: 2.0739, real_ins_contras_loss: 1.2888,fake_ins_contras_loss : 7.9378, CL_contra_loss : 0.0089
[1800/3001] Loss_D: -0.6601 Loss_G: 4.7098, Wasserstein_dist: 2.0567, real_ins_contras_loss: 1.2025,fake_ins_contras_loss : 7.9321, CL_contra_loss : 0.0059
model 3 is saved
cls traing
unseen=0.4242, seen=0.2859, h=0.3416
[1900/3001] Loss_D: -0.6795 Loss_G: 4.7017, Wasserstein_dist: 2.0694, real_ins_contras_loss: 1.2057,fake_ins_contras_loss : 7.9811, CL_contra_loss : 0.0072
kl_loss G is 0.004835501313209534
kl_loss G is 0.004897258244454861
kl_loss G is 0.004900195170193911
kl_loss G is 0.004878198262304068
[2000/3001] Loss_D: -0.6874 Loss_G: 4.7507, Wasserstein_dist: 2.0833, real_ins_contras_loss: 1.2060,fake_ins_contras_loss : 7.9272, CL_contra_loss : 0.0052
model 3 is saved
cls traing
unseen=0.4268, seen=0.2793, h=0.3376
[2100/3001] Loss_D: -0.7353 Loss_G: 4.5930, Wasserstein_dist: 2.1325, real_ins_contras_loss: 1.2164,fake_ins_contras_loss : 7.9492, CL_contra_loss : 0.0049
[2200/3001] Loss_D: -0.6998 Loss_G: 4.8107, Wasserstein_dist: 2.0926, real_ins_contras_loss: 1.2174,fake_ins_contras_loss : 7.9996, CL_contra_loss : 0.0050
model 3 is saved
cls traing
unseen=0.4321, seen=0.2770, h=0.3376
[2300/3001] Loss_D: -0.6726 Loss_G: 4.7992, Wasserstein_dist: 2.0892, real_ins_contras_loss: 1.2384,fake_ins_contras_loss : 8.0065, CL_contra_loss : 0.0061
[2400/3001] Loss_D: -0.6752 Loss_G: 4.9052, Wasserstein_dist: 2.0989, real_ins_contras_loss: 1.2589,fake_ins_contras_loss : 8.0352, CL_contra_loss : 0.0047
model 3 is saved
cls traing
unseen=0.4012, seen=0.2894, h=0.3362
kl_loss G is 0.004981405101716518
kl_loss G is 0.004944338463246822
kl_loss G is 0.005024540703743696
kl_loss G is 0.005036137066781521
[2500/3001] Loss_D: -0.6901 Loss_G: 4.9914, Wasserstein_dist: 2.1175, real_ins_contras_loss: 1.2310,fake_ins_contras_loss : 7.9413, CL_contra_loss : 0.0066
[2600/3001] Loss_D: -0.6286 Loss_G: 5.0071, Wasserstein_dist: 2.1042, real_ins_contras_loss: 1.2794,fake_ins_contras_loss : 8.0297, CL_contra_loss : 0.0077
model 3 is saved
cls traing
unseen=0.4056, seen=0.2971, h=0.3430
[2700/3001] Loss_D: -0.7140 Loss_G: 5.0313, Wasserstein_dist: 2.1206, real_ins_contras_loss: 1.2250,fake_ins_contras_loss : 7.9475, CL_contra_loss : 0.0048
[2800/3001] Loss_D: -0.6636 Loss_G: 4.8363, Wasserstein_dist: 2.1180, real_ins_contras_loss: 1.2662,fake_ins_contras_loss : 8.0851, CL_contra_loss : 0.0058
model 3 is saved
cls traing
unseen=0.4250, seen=0.2878, h=0.3432
[2900/3001] Loss_D: -0.7086 Loss_G: 5.0304, Wasserstein_dist: 2.1232, real_ins_contras_loss: 1.2311,fake_ins_contras_loss : 8.0098, CL_contra_loss : 0.0052
kl_loss G is 0.00492874626070261
kl_loss G is 0.005023233592510223
kl_loss G is 0.004955545999109745
kl_loss G is 0.004976038355380297
[3000/3001] Loss_D: -0.6751 Loss_G: 4.8365, Wasserstein_dist: 2.1485, real_ins_contras_loss: 1.2808,fake_ins_contras_loss : 8.0720, CL_contra_loss : 0.0068
model 3 is saved
cls traing
unseen=0.4074, seen=0.2855, h=0.3357
average_acc is 0.33651027109011494
 hightest s is 0.28781431334622837, u is 0.42504409171075835, H is 0.34322039981509017,
----------------------------------------
loading task 4th data
seen_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698]
current seen_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100, 490, 21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268, 447, 431, 703, 572, 667, 50, 504, 352, 590, 359, 365, 500, 6, 274, 280, 270, 258, 713, 548, 648, 211, 514, 4, 52, 607, 233, 60, 165, 253, 316, 709, 135, 698, 227, 86, 355, 435, 675, 541, 202, 434, 140, 540, 66, 213, 588, 550, 229, 235, 295, 395, 201, 42, 542, 116, 306, 444, 101, 232, 391, 685, 93, 680, 510, 454, 154, 264, 544, 109, 285, 642, 236, 427, 291, 181, 207, 65, 44, 594, 222, 22, 203, 399, 20, 107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292, 33, 526, 507, 90, 499, 14, 686, 79, 539, 404, 348, 212, 457, 553, 410, 408, 139, 633, 94, 191, 287, 651, 18, 123, 345, 149, 321, 46, 519, 36, 664, 92, 636, 567, 591, 269, 589, 67, 76, 230, 414, 282, 655, 702, 533, 647, 568, 9, 418, 170, 364, 480, 351, 474, 41, 279, 91, 188, 251, 275, 522, 547, 296, 411, 119, 389, 523, 356, 45, 363, 332, 290, 505, 277, 466, 146, 663, 225, 126, 361, 419, 17, 666, 179, 386, 29, 88, 255, 570, 132, 531, 536, 327, 646, 248, 111, 48, 250, 13, 310, 382, 330, 473, 177, 429, 15, 157, 384, 71, 39, 630, 556, 543, 450, 700, 661, 273, 309, 397, 634, 77, 304, 81, 623, 125, 452, 691, 218, 579, 494, 625, 436, 70, 412, 486, 308, 215, 428, 256, 338, 176, 616, 186, 302, 263, 16, 598, 496, 68, 687, 87, 335, 339, 223, 210, 224, 383, 456, 303, 596, 592, 459, 421, 620, 297, 609, 2, 704, 433, 604, 128, 644, 322, 495, 599, 341, 619, 573, 716, 401, 12, 611, 178, 369, 40, 678, 462, 417, 199, 603, 696, 467, 398, 701, 162, 688, 377, 677, 142, 606, 552, 415, 347, 652, 260, 692, 131, 451, 82, 684, 305, 375, 569, 144, 615, 481, 173, 134, 366, 672, 430, 487, 35, 0, 621, 156, 373, 492, 483, 272, 325, 707, 388, 657, 243, 565, 362, 228, 1, 185, 271, 247, 346, 697, 106, 278, 148, 206, 47, 521, 600, 344, 562, 637, 638, 104, 283, 324, 58, 349, 617]
curr_unseen_label is [358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315, 95, 72, 216, 10, 711, 152, 471, 509, 658, 631, 130, 420, 57, 124, 158, 679, 493, 342, 138, 286, 650, 336, 259, 221, 262, 425, 560, 99, 23, 508, 112, 635, 580, 24, 710, 656, 53, 196, 328, 645, 712, 440, 448, 482, 381, 245, 254, 246, 681, 74, 558, 423, 184, 695]
pre_label is [458, 422, 682, 78, 194, 385, 11, 200, 597, 267, 317, 665, 659, 461, 525, 208, 122, 31, 171, 706, 367, 61, 469, 614, 438, 195, 394, 301, 133, 618, 198, 520, 424, 141, 97, 532, 586, 217, 390, 694, 193, 151, 416, 441, 143, 5, 205, 51, 30, 98, 182, 121, 705, 318, 455, 576, 442, 183, 564, 518, 582, 137, 577, 689, 293, 643, 524, 673, 320, 627, 662, 649, 463, 289, 478, 25, 105, 234, 551, 402, 147, 118, 406, 299, 574, 538, 464, 266, 683, 595, 7, 549, 439, 632, 187, 585, 465, 554, 333, 169, 608, 668, 583, 497, 69, 511, 226, 284, 240, 516, 8, 563, 561, 59, 374, 27, 127, 380, 19, 699, 407, 319, 350, 244, 528, 534, 468, 371, 405, 180, 84, 488, 108, 571, 209, 446, 357, 566, 610, 640, 545, 281, 669, 477, 546, 312, 26, 368, 426, 670, 294, 166, 393, 340, 337, 49, 485, 671, 219, 28, 175, 331, 475, 323, 313, 37, 387, 360, 164, 443, 83, 460, 503, 693, 249, 129, 557, 470, 581, 120, 575, 378, 601, 587, 63, 513, 476, 515, 276, 115, 155, 660, 189, 409, 80, 690, 396, 62, 654, 626, 204, 89, 479, 555, 136, 559, 343, 392, 437, 484, 498, 372, 334, 527, 501, 641, 163, 376, 329, 300, 110, 506, 56, 159, 241, 400, 64, 150, 432, 174, 55, 54, 161, 629, 252, 197, 214, 238, 220, 34, 168, 117, 714, 96, 413, 453, 239, 639, 472, 113, 326, 445, 535, 73, 172, 605, 512, 653, 43, 358, 3, 145, 379, 715, 237, 674, 103, 529, 298, 622, 517, 75, 38, 353, 32, 85, 315, 537, 584, 190, 102, 612, 370, 624, 578, 160, 491, 167, 261, 502, 100, 490, 21, 593, 311, 257, 489, 403, 314, 613, 628, 242, 265, 288, 268, 447, 431, 703, 572, 667, 50, 504, 352, 590, 359, 365, 500, 6, 274, 280, 270, 258, 713, 548, 648, 211, 514, 4, 52, 607, 233, 60, 165, 253, 316, 709, 135, 698, 227, 86, 355, 435, 675, 541, 202, 434, 140, 540, 66, 213, 588, 550, 229, 235, 295, 395, 201, 42, 542, 116, 306, 444, 101, 232, 391, 685, 93, 680, 510, 454, 154, 264, 544, 109, 285, 642, 236, 427, 291, 181, 207, 65, 44, 594, 222, 22, 203, 399, 20, 107, 530, 449, 676, 231, 114, 192, 708, 602, 307, 354, 153, 292, 33, 526, 507, 90, 95, 72, 216, 10, 711, 152, 471, 509, 658, 631, 130, 420, 57, 124, 158, 679, 493, 342, 499, 14, 686, 79, 539, 404, 348, 212, 457, 553, 410, 408, 139, 633, 94, 191, 287, 651, 18, 123, 345, 149, 321, 46, 519, 36, 664, 92, 636, 567, 591, 269, 589, 67, 76, 230, 414, 282, 655, 702, 533, 647, 568, 9, 418, 170, 364, 480, 351, 474, 41, 279, 91, 188, 251, 275, 522, 547, 296, 411, 119, 389, 523, 356, 45, 363, 332, 290, 505, 277, 466, 146, 663, 225, 126, 361, 419, 17, 666, 179, 386, 29, 88, 255, 570, 132, 531, 536, 327, 646, 248, 111, 48, 250, 13, 310, 382, 330, 473, 177, 429, 15, 157, 384, 71, 39, 630, 556, 543, 450, 700, 661, 273, 309, 397, 634, 77, 304, 81, 623, 125, 452, 691, 218, 579, 494, 625, 436, 70, 138, 286, 650, 336, 259, 221, 262, 425, 560, 99, 23, 508, 112, 635, 580, 24, 710, 656, 412, 486, 308, 215, 428, 256, 338, 176, 616, 186, 302, 263, 16, 598, 496, 68, 687, 87, 335, 339, 223, 210, 224, 383, 456, 303, 596, 592, 459, 421, 620, 297, 609, 2, 704, 433, 604, 128, 644, 322, 495, 599, 341, 619, 573, 716, 401, 12, 611, 178, 369, 40, 678, 462, 417, 199, 603, 696, 467, 398, 701, 162, 688, 377, 677, 142, 606, 552, 415, 347, 652, 260, 692, 131, 451, 82, 684, 305, 375, 569, 144, 615, 481, 173, 134, 366, 672, 430, 487, 35, 0, 621, 156, 373, 492, 483, 272, 325, 707, 388, 657, 243, 565, 362, 228, 1, 185, 271, 247, 346, 697, 106, 278, 148, 206, 47, 521, 600, 344, 562, 637, 638, 104, 283, 324, 58, 349, 617, 53, 196, 328, 645, 712, 440, 448, 482, 381, 245, 254, 246, 681, 74, 558, 423, 184, 695]
new_label is [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716]
incremental training task 4···
loading task 4 data and the pre model
model 3 is loaded
kl_loss G is 0.0
kl_loss G is 0.0029320267494767904
kl_loss G is 0.0016758214915171266
kl_loss G is 0.001497958437539637
[0/3001] Loss_D: 4.3986 Loss_G: 5.1012, Wasserstein_dist: 0.5222, real_ins_contras_loss: 4.5021,fake_ins_contras_loss : 8.1109, CL_contra_loss : 0.0357
model 4 is saved
cls traing
unseen=0.3525, seen=0.2369, h=0.2834
[100/3001] Loss_D: -0.0882 Loss_G: 3.9855, Wasserstein_dist: 1.5377, real_ins_contras_loss: 1.2799,fake_ins_contras_loss : 7.7805, CL_contra_loss : 0.0092
[200/3001] Loss_D: -0.3347 Loss_G: 3.9949, Wasserstein_dist: 1.7477, real_ins_contras_loss: 1.2601,fake_ins_contras_loss : 7.8298, CL_contra_loss : 0.0065
model 4 is saved
cls traing
unseen=0.3406, seen=0.2515, h=0.2893
[300/3001] Loss_D: -0.3692 Loss_G: 4.2208, Wasserstein_dist: 1.7827, real_ins_contras_loss: 1.2478,fake_ins_contras_loss : 7.8668, CL_contra_loss : 0.0063
[400/3001] Loss_D: -0.5060 Loss_G: 4.3309, Wasserstein_dist: 1.8930, real_ins_contras_loss: 1.2305,fake_ins_contras_loss : 7.8891, CL_contra_loss : 0.0049
model 4 is saved
cls traing
unseen=0.3320, seen=0.2546, h=0.2882
kl_loss G is 0.0038445894606411457
kl_loss G is 0.00400881702080369
kl_loss G is 0.003933362662792206
kl_loss G is 0.0038611406926065683
[500/3001] Loss_D: -0.4996 Loss_G: 4.2945, Wasserstein_dist: 1.9398, real_ins_contras_loss: 1.2849,fake_ins_contras_loss : 7.9062, CL_contra_loss : 0.0048
[600/3001] Loss_D: -0.5458 Loss_G: 4.5657, Wasserstein_dist: 1.9803, real_ins_contras_loss: 1.2645,fake_ins_contras_loss : 7.8880, CL_contra_loss : 0.0050
model 4 is saved
cls traing
unseen=0.3571, seen=0.2453, h=0.2908
[700/3001] Loss_D: -0.5900 Loss_G: 4.7178, Wasserstein_dist: 1.9903, real_ins_contras_loss: 1.2342,fake_ins_contras_loss : 7.9007, CL_contra_loss : 0.0048
[800/3001] Loss_D: -0.5284 Loss_G: 4.8297, Wasserstein_dist: 1.9653, real_ins_contras_loss: 1.2803,fake_ins_contras_loss : 7.8990, CL_contra_loss : 0.0049
model 4 is saved
cls traing
unseen=0.3624, seen=0.2431, h=0.2910
[900/3001] Loss_D: -0.6467 Loss_G: 4.8875, Wasserstein_dist: 2.0458, real_ins_contras_loss: 1.2403,fake_ins_contras_loss : 7.9450, CL_contra_loss : 0.0044
kl_loss G is 0.004304151050746441
kl_loss G is 0.004440586548298597
kl_loss G is 0.0043650902807712555
kl_loss G is 0.004387109540402889
[1000/3001] Loss_D: -0.6306 Loss_G: 4.9194, Wasserstein_dist: 2.0431, real_ins_contras_loss: 1.2429,fake_ins_contras_loss : 7.8784, CL_contra_loss : 0.0044
model 4 is saved
cls traing
unseen=0.3380, seen=0.2577, h=0.2924
[1100/3001] Loss_D: -0.6641 Loss_G: 4.9916, Wasserstein_dist: 2.0541, real_ins_contras_loss: 1.2200,fake_ins_contras_loss : 7.8697, CL_contra_loss : 0.0043
[1200/3001] Loss_D: -0.6594 Loss_G: 5.0142, Wasserstein_dist: 2.0993, real_ins_contras_loss: 1.2655,fake_ins_contras_loss : 7.8767, CL_contra_loss : 0.0059
model 4 is saved
cls traing
unseen=0.3538, seen=0.2564, h=0.2974
[1300/3001] Loss_D: -0.6230 Loss_G: 5.1762, Wasserstein_dist: 2.0569, real_ins_contras_loss: 1.2586,fake_ins_contras_loss : 7.8169, CL_contra_loss : 0.0054
[1400/3001] Loss_D: -0.6688 Loss_G: 5.2921, Wasserstein_dist: 2.0724, real_ins_contras_loss: 1.2279,fake_ins_contras_loss : 7.8651, CL_contra_loss : 0.0039
model 4 is saved
cls traing
unseen=0.3585, seen=0.2447, h=0.2908
kl_loss G is 0.0046081505715847015
kl_loss G is 0.004542915150523186
kl_loss G is 0.0046044583432376385
kl_loss G is 0.0046285707503557205
[1500/3001] Loss_D: -0.6854 Loss_G: 5.1770, Wasserstein_dist: 2.1127, real_ins_contras_loss: 1.2522,fake_ins_contras_loss : 7.8951, CL_contra_loss : 0.0044
[1600/3001] Loss_D: -0.6523 Loss_G: 5.4476, Wasserstein_dist: 2.0721, real_ins_contras_loss: 1.2403,fake_ins_contras_loss : 7.8976, CL_contra_loss : 0.0041
model 4 is saved
cls traing
unseen=0.3439, seen=0.2617, h=0.2972
[1700/3001] Loss_D: -0.6486 Loss_G: 5.2313, Wasserstein_dist: 2.0725, real_ins_contras_loss: 1.2540,fake_ins_contras_loss : 7.8317, CL_contra_loss : 0.0042
[1800/3001] Loss_D: -0.7409 Loss_G: 5.4743, Wasserstein_dist: 2.1539, real_ins_contras_loss: 1.2261,fake_ins_contras_loss : 7.9126, CL_contra_loss : 0.0040
model 4 is saved
cls traing
unseen=0.3426, seen=0.2592, h=0.2951
[1900/3001] Loss_D: -0.6944 Loss_G: 5.3352, Wasserstein_dist: 2.0961, real_ins_contras_loss: 1.2363,fake_ins_contras_loss : 7.8290, CL_contra_loss : 0.0038
kl_loss G is 0.004615085665136576
kl_loss G is 0.0047168736346066
kl_loss G is 0.0047689140774309635
kl_loss G is 0.004742861725389957
[2000/3001] Loss_D: -0.6792 Loss_G: 5.4551, Wasserstein_dist: 2.1289, real_ins_contras_loss: 1.2724,fake_ins_contras_loss : 7.9694, CL_contra_loss : 0.0039
model 4 is saved
cls traing
unseen=0.3532, seen=0.2515, h=0.2938
[2100/3001] Loss_D: -0.7010 Loss_G: 5.4169, Wasserstein_dist: 2.1291, real_ins_contras_loss: 1.2452,fake_ins_contras_loss : 7.9128, CL_contra_loss : 0.0050
[2200/3001] Loss_D: -0.6635 Loss_G: 5.3752, Wasserstein_dist: 2.0971, real_ins_contras_loss: 1.2626,fake_ins_contras_loss : 7.8896, CL_contra_loss : 0.0041
model 4 is saved
cls traing
unseen=0.3565, seen=0.2549, h=0.2972
[2300/3001] Loss_D: -0.7220 Loss_G: 5.4207, Wasserstein_dist: 2.1526, real_ins_contras_loss: 1.2455,fake_ins_contras_loss : 7.8848, CL_contra_loss : 0.0052
[2400/3001] Loss_D: -0.7650 Loss_G: 5.4899, Wasserstein_dist: 2.1577, real_ins_contras_loss: 1.2258,fake_ins_contras_loss : 7.9413, CL_contra_loss : 0.0037
model 4 is saved
cls traing
unseen=0.3194, seen=0.2679, h=0.2914
kl_loss G is 0.0048215375281870365
kl_loss G is 0.0047937240451574326
kl_loss G is 0.004943089559674263
kl_loss G is 0.004821533337235451
[2500/3001] Loss_D: -0.6980 Loss_G: 5.6191, Wasserstein_dist: 2.1240, real_ins_contras_loss: 1.2393,fake_ins_contras_loss : 7.9470, CL_contra_loss : 0.0056
[2600/3001] Loss_D: -0.7264 Loss_G: 5.5356, Wasserstein_dist: 2.1010, real_ins_contras_loss: 1.2060,fake_ins_contras_loss : 7.9017, CL_contra_loss : 0.0051
model 4 is saved
cls traing
unseen=0.3479, seen=0.2536, h=0.2934
[2700/3001] Loss_D: -0.7436 Loss_G: 5.5794, Wasserstein_dist: 2.1786, real_ins_contras_loss: 1.2532,fake_ins_contras_loss : 7.8954, CL_contra_loss : 0.0040
[2800/3001] Loss_D: -0.7896 Loss_G: 5.4499, Wasserstein_dist: 2.1665, real_ins_contras_loss: 1.2048,fake_ins_contras_loss : 7.9340, CL_contra_loss : 0.0038
model 4 is saved
cls traing
unseen=0.3684, seen=0.2422, h=0.2922
[2900/3001] Loss_D: -0.7420 Loss_G: 5.5034, Wasserstein_dist: 2.1574, real_ins_contras_loss: 1.2352,fake_ins_contras_loss : 7.9858, CL_contra_loss : 0.0041
kl_loss G is 0.004848272539675236
kl_loss G is 0.004861768800765276
kl_loss G is 0.004757281392812729
kl_loss G is 0.004828877747058868
[3000/3001] Loss_D: -0.7093 Loss_G: 5.5309, Wasserstein_dist: 2.1274, real_ins_contras_loss: 1.2378,fake_ins_contras_loss : 7.9775, CL_contra_loss : 0.0035
model 4 is saved
cls traing
unseen=0.3247, seen=0.2645, h=0.2915
average_acc is 0.2927913743663496
 hightest s is 0.25643410852713183, u is 0.3538359788359788, H is 0.297362153828266,
----------------------------------------
loading task 5th data
Traceback (most recent call last):
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 372, in <module>
    train_one_task(premodel = pregan)
  File "E:/终身学习/代码/第二篇论文代码备份/zero-shot-lifelong-gan - v3/train_pre.py", line 348, in train_one_task
    data.current_class_index()
  File "E:\终身学习\代码\第二篇论文代码备份\zero-shot-lifelong-gan - v3\utils\data_pretrain.py", line 270, in current_class_index
    current_data = self.splited_seen_class[self.current_taskid].unsqueeze(dim=1)
IndexError: list index out of range

Process finished with exit code 1
